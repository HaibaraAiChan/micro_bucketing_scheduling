main start at this time 1698450250.9863582
-----------------------------------------before load data 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

karate data
{}
{}
Graph(num_nodes=7, num_edges=14,
      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(4,), dtype=torch.float32)}
      edata_schemes={})
#nodes: 7
#edges: 14
#classes: 2
success----------------------------------------
4
2
1
# Nodes: 7
# Edges: 14
# Train: 4
# Val: 2
# Test: 1
# Classes: 2

----------------------------------------start of run function 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([2, 0, 1, 3, 5, 4])
get_in_degree_bucketing dst global nid  tensor([2, 0, 1, 3])
get_in_degree_bucketing corresponding in degs tensor([2, 1, 3, 3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  1
local bkt nids  tensor([0])
len(bkt)  2
local bkt nids  tensor([2, 3])
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
partitioner **** batches_nid_list  [tensor([2]), tensor([3])]
partitioner g_bucket_nids_list  [tensor([1]), tensor([0])]
partitioner local_batches_nid_list  [tensor([2]), tensor([3])]
self.weights_list  [0.5, 0.5]
partitioner final local_batches_nid_list  [tensor([2, 1]), tensor([3, 0])]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.001493215560913086
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0006589889526367188
local_to_global: src global  tensor([2, 0, 1, 3, 5, 4])
local_to_global: local nid  tensor([2, 1])
local_to_global: local nid after sort  tensor([1, 2])
local_to_global: local nid  tensor([3, 0])
local_to_global: local nid after sort  tensor([0, 3])
local_to_global: local_batched_seeds_list  [tensor([2, 1]), tensor([3, 0])]
local_to_global: global_batched_seeds_list  [tensor([0, 1]), tensor([2, 3])]
partition total batch output list spend :  0.003019094467163086
self.buckets_partition() spend  sec:  0.0021631717681884766
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  2
bucketing dataloader: global_batched_output_nid_list  [tensor([0, 1]), tensor([2, 3])]
connection check : local_output_nid  [1, 2]
connection check : local_output_nid  [0, 3]

block_gen_time in "generate_blocks_for_one_layer_block"  0.00529789924621582

bucketing dataloader: global src_list  [tensor([0, 1, 2, 5]), tensor([2, 3, 1, 4])]
----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  2
connection check : local_output_nid  [1, 2, 0, 4]
connection check : local_output_nid  [0, 3, 2, 5]

block_gen_time in "generate_blocks_for_one_layer_block"  0.003163576126098633

bucketing dataloader: src_list  [tensor([0, 1, 2, 5, 3]), tensor([2, 3, 1, 4, 0, 5])]
block collection to dataloader spend  7.62939453125e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.462890625 GB
    Memory Allocated: 0.03133106231689453  GigaBytes
Max Memory Allocated: 0.03133106231689453  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.031641483306884766  GigaBytes
Max Memory Allocated: 0.03167533874511719  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.031642913818359375  GigaBytes
Max Memory Allocated: 0.03167533874511719  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06266450881958008  GigaBytes
Max Memory Allocated: 0.09408807754516602  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06302785873413086  GigaBytes
Max Memory Allocated: 0.09408807754516602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06302881240844727  GigaBytes
Max Memory Allocated: 0.09408807754516602  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
----------------------------------------after optimizer
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.12532520294189453  GigaBytes
Max Memory Allocated: 0.17220020294189453  GigaBytes

epoch  0
----------------------------------------------------------pseudo_mini_loss sum 0.7422652840614319
pure train time :  0.44470858573913574
train time :  0.9943733215332031
end to end time :  1.0157811641693115
connection check time:  0.008767366409301758
block generation time  0.008461475372314453
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([1, 3, 2, 0, 5, 4])
get_in_degree_bucketing dst global nid  tensor([1, 3, 2, 0])
get_in_degree_bucketing corresponding in degs tensor([3, 3, 2, 1])
len(bkt)  1
local bkt nids  tensor([3])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([0, 1])
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
partitioner **** batches_nid_list  [tensor([0]), tensor([1])]
partitioner g_bucket_nids_list  [tensor([3]), tensor([2])]
partitioner local_batches_nid_list  [tensor([0]), tensor([1])]
self.weights_list  [0.5, 0.5]
partitioner final local_batches_nid_list  [tensor([0, 3]), tensor([1, 2])]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0016238689422607422
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0010945796966552734
local_to_global: src global  tensor([1, 3, 2, 0, 5, 4])
local_to_global: local nid  tensor([0, 3])
local_to_global: local nid after sort  tensor([0, 3])
local_to_global: local nid  tensor([1, 2])
local_to_global: local nid after sort  tensor([1, 2])
local_to_global: local_batched_seeds_list  [tensor([0, 3]), tensor([1, 2])]
local_to_global: global_batched_seeds_list  [tensor([1, 0]), tensor([3, 2])]
partition total batch output list spend :  0.004405498504638672
self.buckets_partition() spend  sec:  0.002735614776611328
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  2
bucketing dataloader: global_batched_output_nid_list  [tensor([1, 0]), tensor([3, 2])]
connection check : local_output_nid  [0, 3]
connection check : local_output_nid  [1, 2]

block_gen_time in "generate_blocks_for_one_layer_block"  0.004666805267333984

bucketing dataloader: global src_list  [tensor([1, 0, 2, 5]), tensor([3, 2, 1, 4])]
----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  2
connection check : local_output_nid  [0, 3, 2, 4]
connection check : local_output_nid  [1, 2, 0, 5]

block_gen_time in "generate_blocks_for_one_layer_block"  0.003957271575927734

bucketing dataloader: src_list  [tensor([1, 0, 2, 5, 3]), tensor([3, 2, 1, 4, 0, 5])]
block collection to dataloader spend  4.5299530029296875e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.17220020294189453  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.12563037872314453  GigaBytes
Max Memory Allocated: 0.17220020294189453  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.1256275177001953  GigaBytes
Max Memory Allocated: 0.17220020294189453  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.18805742263793945  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12568426132202148  GigaBytes
Max Memory Allocated: 0.18805742263793945  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1256852149963379  GigaBytes
Max Memory Allocated: 0.18805742263793945  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532520294189453  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

epoch  1
----------------------------------------------------------pseudo_mini_loss sum 0.6633391380310059
pure train time :  0.044089317321777344
train time :  0.04567885398864746
end to end time :  0.06912922859191895
connection check time:  0.0070340633392333984
block generation time  0.008624076843261719
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([1, 2, 3, 0, 5, 4])
get_in_degree_bucketing dst global nid  tensor([1, 2, 3, 0])
get_in_degree_bucketing corresponding in degs tensor([3, 2, 3, 1])
len(bkt)  1
local bkt nids  tensor([3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  2
local bkt nids  tensor([0, 2])
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
partitioner **** batches_nid_list  [tensor([0]), tensor([2])]
partitioner g_bucket_nids_list  [tensor([3]), tensor([1])]
partitioner local_batches_nid_list  [tensor([0]), tensor([2])]
self.weights_list  [0.5, 0.5]
partitioner final local_batches_nid_list  [tensor([0, 3]), tensor([2, 1])]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0023958683013916016
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0011606216430664062
local_to_global: src global  tensor([1, 2, 3, 0, 5, 4])
local_to_global: local nid  tensor([0, 3])
local_to_global: local nid after sort  tensor([0, 3])
local_to_global: local nid  tensor([2, 1])
local_to_global: local nid after sort  tensor([1, 2])
local_to_global: local_batched_seeds_list  [tensor([0, 3]), tensor([2, 1])]
local_to_global: global_batched_seeds_list  [tensor([1, 0]), tensor([2, 3])]
partition total batch output list spend :  0.0052263736724853516
self.buckets_partition() spend  sec:  0.0035729408264160156
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  2
bucketing dataloader: global_batched_output_nid_list  [tensor([1, 0]), tensor([2, 3])]
connection check : local_output_nid  [0, 3]
connection check : local_output_nid  [1, 2]

block_gen_time in "generate_blocks_for_one_layer_block"  0.005089759826660156

bucketing dataloader: global src_list  [tensor([1, 0, 2, 5]), tensor([2, 3, 1, 4])]
----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  2
connection check : local_output_nid  [0, 3, 1, 4]
connection check : local_output_nid  [1, 2, 0, 5]

block_gen_time in "generate_blocks_for_one_layer_block"  0.004059314727783203

bucketing dataloader: src_list  [tensor([1, 0, 2, 5, 3]), tensor([2, 3, 1, 4, 5, 0])]
block collection to dataloader spend  5.7220458984375e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12563037872314453  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1256275177001953  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.4817, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12568426132202148  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1256852149963379  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8336, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532520294189453  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

epoch  2
----------------------------------------------------------pseudo_mini_loss sum 0.6576522588729858
pure train time :  0.04039502143859863
train time :  0.04192996025085449
end to end time :  0.06650614738464355
connection check time:  0.006876468658447266
block generation time  0.00914907455444336
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([0, 3, 2, 1, 4, 5])
get_in_degree_bucketing dst global nid  tensor([0, 3, 2, 1])
get_in_degree_bucketing corresponding in degs tensor([1, 3, 2, 3])
len(bkt)  1
local bkt nids  tensor([0])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([1, 3])
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
partitioner **** batches_nid_list  [tensor([1]), tensor([3])]
partitioner g_bucket_nids_list  [tensor([0]), tensor([2])]
partitioner local_batches_nid_list  [tensor([1]), tensor([3])]
self.weights_list  [0.5, 0.5]
partitioner final local_batches_nid_list  [tensor([1, 0]), tensor([3, 2])]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0011000633239746094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0007522106170654297
local_to_global: src global  tensor([0, 3, 2, 1, 4, 5])
local_to_global: local nid  tensor([1, 0])
local_to_global: local nid after sort  tensor([0, 1])
local_to_global: local nid  tensor([3, 2])
local_to_global: local nid after sort  tensor([2, 3])
local_to_global: local_batched_seeds_list  [tensor([1, 0]), tensor([3, 2])]
local_to_global: global_batched_seeds_list  [tensor([0, 3]), tensor([2, 1])]
partition total batch output list spend :  0.0029973983764648438
self.buckets_partition() spend  sec:  0.0018630027770996094
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  2
bucketing dataloader: global_batched_output_nid_list  [tensor([0, 3]), tensor([2, 1])]
connection check : local_output_nid  [0, 1]
connection check : local_output_nid  [2, 3]

block_gen_time in "generate_blocks_for_one_layer_block"  0.0031511783599853516

bucketing dataloader: global src_list  [tensor([0, 3, 1, 2, 4]), tensor([2, 1, 3, 0, 5])]
----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  2
connection check : local_output_nid  [0, 1, 3, 2, 4]
connection check : local_output_nid  [2, 3, 1, 0, 5]

block_gen_time in "generate_blocks_for_one_layer_block"  0.0031239986419677734

bucketing dataloader: src_list  [tensor([0, 3, 1, 2, 4]), tensor([2, 1, 3, 0, 5])]
block collection to dataloader spend  5.0067901611328125e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1256389617919922  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12563610076904297  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12569665908813477  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12569761276245117  GigaBytes
Max Memory Allocated: 0.18811511993408203  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532520294189453  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

epoch  3
----------------------------------------------------------pseudo_mini_loss sum 0.5355501174926758
pure train time :  0.04390120506286621
train time :  0.04540443420410156
end to end time :  0.06307840347290039
connection check time:  0.006067037582397461
block generation time  0.006275177001953125
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([3, 0, 2, 1, 4, 5])
get_in_degree_bucketing dst global nid  tensor([3, 0, 2, 1])
get_in_degree_bucketing corresponding in degs tensor([3, 1, 2, 3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([0, 3])
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
partitioner **** batches_nid_list  [tensor([0]), tensor([3])]
partitioner g_bucket_nids_list  [tensor([1]), tensor([2])]
partitioner local_batches_nid_list  [tensor([0]), tensor([3])]
self.weights_list  [0.5, 0.5]
partitioner final local_batches_nid_list  [tensor([0, 1]), tensor([3, 2])]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.002201557159423828
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0011761188507080078
local_to_global: src global  tensor([3, 0, 2, 1, 4, 5])
local_to_global: local nid  tensor([0, 1])
local_to_global: local nid after sort  tensor([0, 1])
local_to_global: local nid  tensor([3, 2])
local_to_global: local nid after sort  tensor([2, 3])
local_to_global: local_batched_seeds_list  [tensor([0, 1]), tensor([3, 2])]
local_to_global: global_batched_seeds_list  [tensor([3, 0]), tensor([2, 1])]
partition total batch output list spend :  0.005178689956665039
self.buckets_partition() spend  sec:  0.003397703170776367
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  2
bucketing dataloader: global_batched_output_nid_list  [tensor([3, 0]), tensor([2, 1])]
connection check : local_output_nid  [0, 1]
connection check : local_output_nid  [2, 3]

block_gen_time in "generate_blocks_for_one_layer_block"  0.005677461624145508

bucketing dataloader: global src_list  [tensor([3, 0, 1, 2, 4]), tensor([2, 1, 3, 0, 5])]
----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  2
connection check : local_output_nid  [0, 1, 3, 2, 4]
connection check : local_output_nid  [2, 3, 0, 1, 5]

block_gen_time in "generate_blocks_for_one_layer_block"  0.004027128219604492

bucketing dataloader: src_list  [tensor([3, 0, 1, 2, 4]), tensor([2, 1, 3, 0, 5, 4])]
block collection to dataloader spend  4.5299530029296875e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1256389617919922  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12563610076904297  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8779, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12569665908813477  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12569761276245117  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.3378, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532520294189453  GigaBytes
Max Memory Allocated: 0.1881237030029297  GigaBytes

epoch  4
----------------------------------------------------------pseudo_mini_loss sum 0.6078296899795532
pure train time :  0.044542789459228516
train time :  0.045966148376464844
end to end time :  0.0705268383026123
connection check time:  0.006507158279418945
block generation time  0.00970458984375
end to end time  0.07861590385437012
