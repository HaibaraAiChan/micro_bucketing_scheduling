main start at this time 1698472710.1337636
-----------------------------------------before load data 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

karate data
{}
{}
Graph(num_nodes=7, num_edges=14,
      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(4,), dtype=torch.float32)}
      edata_schemes={})
#nodes: 7
#edges: 14
#classes: 2
success----------------------------------------
4
2
1
# Nodes: 7
# Edges: 14
# Train: 4
# Val: 2
# Test: 1
# Classes: 2

----------------------------------------start of run function 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([2, 0, 1, 3, 5, 4])
get_in_degree_bucketing dst global nid  tensor([2, 0, 1, 3])
get_in_degree_bucketing corresponding in degs tensor([2, 1, 3, 3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  1
local bkt nids  tensor([0])
len(bkt)  2
local bkt nids  tensor([2, 3])
bucket partitioner: bkt_dst_nodes_list_local  [tensor([1]), tensor([0]), tensor([2, 3])]
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.002039194107055664
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0006430149078369141
local_to_global: src global  tensor([2, 0, 1, 3, 5, 4])
output nodes length match
global output equals  True
partition total batch output list spend :  0.0036296844482421875
self.buckets_partition() spend  sec:  0.0027031898498535156
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  3
bucketing dataloader: global_batched_output_nid_list  [tensor([2, 0]), tensor([1]), tensor([3])]
check_connections_block*********************************
connection check : local_output_nid  2
connection check : local_output_nid  1
connection check : local_output_nid  1

block_gen_time in "generate_blocks_for_one_layer_block"  0.006957530975341797

----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  3
check_connections_block*********************************
connection check : local_output_nid  4
connection check : local_output_nid  4
connection check : local_output_nid  4

block_gen_time in "generate_blocks_for_one_layer_block"  0.004376888275146484

block collection to dataloader spend  5.9604644775390625e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.462890625 GB
    Memory Allocated: 0.03133106231689453  GigaBytes
Max Memory Allocated: 0.03133106231689453  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.03158426284790039  GigaBytes
Max Memory Allocated: 0.03161430358886719  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.031585693359375  GigaBytes
Max Memory Allocated: 0.03161430358886719  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06266450881958008  GigaBytes
Max Memory Allocated: 0.09401941299438477  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06290149688720703  GigaBytes
Max Memory Allocated: 0.09401941299438477  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.623046875 GB
    Memory Allocated: 0.06290245056152344  GigaBytes
Max Memory Allocated: 0.09401941299438477  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.654296875 GB
    Memory Allocated: 0.0626683235168457  GigaBytes
Max Memory Allocated: 0.12532854080200195  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.654296875 GB
    Memory Allocated: 0.06290149688720703  GigaBytes
Max Memory Allocated: 0.12532854080200195  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.654296875 GB
    Memory Allocated: 0.06290245056152344  GigaBytes
Max Memory Allocated: 0.12532854080200195  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
----------------------------------------after optimizer
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.12532901763916016  GigaBytes
Max Memory Allocated: 0.17220401763916016  GigaBytes

epoch  0
----------------------------------------------------------pseudo_mini_loss sum 0.743654191493988
pure train time :  0.46645498275756836
train time :  0.9771497249603271
end to end time :  1.0116901397705078
connection check time:  0.01837635040283203
block generation time  0.011334419250488281
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([1, 3, 2, 0, 5, 4])
get_in_degree_bucketing dst global nid  tensor([1, 3, 2, 0])
get_in_degree_bucketing corresponding in degs tensor([3, 3, 2, 1])
len(bkt)  1
local bkt nids  tensor([3])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([0, 1])
bucket partitioner: bkt_dst_nodes_list_local  [tensor([3]), tensor([2]), tensor([0, 1])]
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0017542839050292969
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00021266937255859375
local_to_global: src global  tensor([1, 3, 2, 0, 5, 4])
output nodes length match
global output equals  True
partition total batch output list spend :  0.0029671192169189453
self.buckets_partition() spend  sec:  0.0019843578338623047
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  3
bucketing dataloader: global_batched_output_nid_list  [tensor([2, 0]), tensor([3]), tensor([1])]
check_connections_block*********************************
connection check : local_output_nid  2
connection check : local_output_nid  1
connection check : local_output_nid  1

block_gen_time in "generate_blocks_for_one_layer_block"  0.005163908004760742

----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  3
check_connections_block*********************************
connection check : local_output_nid  4
connection check : local_output_nid  4
connection check : local_output_nid  4

block_gen_time in "generate_blocks_for_one_layer_block"  0.0050640106201171875

block collection to dataloader spend  5.7220458984375e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.17220401763916016  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.12557315826416016  GigaBytes
Max Memory Allocated: 0.17220401763916016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.701171875 GB
    Memory Allocated: 0.12557029724121094  GigaBytes
Max Memory Allocated: 0.17220401763916016  GigaBytes

unweigthed pseudo_mini_loss  tensor(1.6061, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555408477783203  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555503845214844  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532472610473633  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12556171417236328  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1255626678466797  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532901763916016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

epoch  1
----------------------------------------------------------pseudo_mini_loss sum 0.8149848580360413
pure train time :  0.059178829193115234
train time :  0.062272071838378906
end to end time :  0.09054374694824219
connection check time:  0.011893987655639648
block generation time  0.01022791862487793
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([1, 2, 3, 0, 5, 4])
get_in_degree_bucketing dst global nid  tensor([1, 2, 3, 0])
get_in_degree_bucketing corresponding in degs tensor([3, 2, 3, 1])
len(bkt)  1
local bkt nids  tensor([3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  2
local bkt nids  tensor([0, 2])
bucket partitioner: bkt_dst_nodes_list_local  [tensor([3]), tensor([1]), tensor([0, 2])]
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0027513504028320312
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00023412704467773438
local_to_global: src global  tensor([1, 2, 3, 0, 5, 4])
output nodes length match
global output equals  True
partition total batch output list spend :  0.004070758819580078
self.buckets_partition() spend  sec:  0.0030057430267333984
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  3
bucketing dataloader: global_batched_output_nid_list  [tensor([2, 0]), tensor([3]), tensor([1])]
check_connections_block*********************************
connection check : local_output_nid  2
connection check : local_output_nid  1
connection check : local_output_nid  1

block_gen_time in "generate_blocks_for_one_layer_block"  0.006662130355834961

----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  3
check_connections_block*********************************
connection check : local_output_nid  4
connection check : local_output_nid  4
connection check : local_output_nid  4

block_gen_time in "generate_blocks_for_one_layer_block"  0.005208730697631836

block collection to dataloader spend  5.4836273193359375e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557315826416016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557029724121094  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(1.2710, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555408477783203  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555503845214844  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532472610473633  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12556171417236328  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1255626678466797  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532901763916016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

epoch  2
----------------------------------------------------------pseudo_mini_loss sum 0.7233595848083496
pure train time :  0.06066727638244629
train time :  0.06432723999023438
end to end time :  0.11048769950866699
connection check time:  0.027138233184814453
block generation time  0.011870861053466797
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([0, 3, 2, 1, 4, 5])
get_in_degree_bucketing dst global nid  tensor([0, 3, 2, 1])
get_in_degree_bucketing corresponding in degs tensor([1, 3, 2, 3])
len(bkt)  1
local bkt nids  tensor([0])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([1, 3])
bucket partitioner: bkt_dst_nodes_list_local  [tensor([0]), tensor([2]), tensor([1, 3])]
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0023398399353027344
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00027823448181152344
local_to_global: src global  tensor([0, 3, 2, 1, 4, 5])
output nodes length match
global output equals  True
partition total batch output list spend :  0.00395512580871582
self.buckets_partition() spend  sec:  0.0026404857635498047
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  3
bucketing dataloader: global_batched_output_nid_list  [tensor([0, 2]), tensor([3]), tensor([1])]
check_connections_block*********************************
connection check : local_output_nid  2
connection check : local_output_nid  1
connection check : local_output_nid  1

block_gen_time in "generate_blocks_for_one_layer_block"  0.005715608596801758

----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  3
check_connections_block*********************************
connection check : local_output_nid  4
connection check : local_output_nid  4
connection check : local_output_nid  4

block_gen_time in "generate_blocks_for_one_layer_block"  0.0050106048583984375

block collection to dataloader spend  6.198883056640625e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557315826416016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557029724121094  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.9124, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555408477783203  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555503845214844  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532472610473633  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12556171417236328  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1255626678466797  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532901763916016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

epoch  3
----------------------------------------------------------pseudo_mini_loss sum 0.5253439545631409
pure train time :  0.06744503974914551
train time :  0.07115483283996582
end to end time :  0.10326766967773438
connection check time:  0.014069080352783203
block generation time  0.010726213455200195
main fucntion generate_dataloader_bucket_block=======
get_in_degree_bucketing src global nid  tensor([3, 0, 2, 1, 4, 5])
get_in_degree_bucketing dst global nid  tensor([3, 0, 2, 1])
get_in_degree_bucketing corresponding in degs tensor([3, 1, 2, 3])
len(bkt)  1
local bkt nids  tensor([1])
len(bkt)  1
local bkt nids  tensor([2])
len(bkt)  2
local bkt nids  tensor([0, 3])
bucket partitioner: bkt_dst_nodes_list_local  [tensor([1]), tensor([2]), tensor([0, 3])]
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0014846324920654297
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00018262863159179688
local_to_global: src global  tensor([3, 0, 2, 1, 4, 5])
output nodes length match
global output equals  True
partition total batch output list spend :  0.002473592758178711
self.buckets_partition() spend  sec:  0.0016810894012451172
bucketing dataloader: layer  0
bucketing dataloader: the number of batches:  3
bucketing dataloader: global_batched_output_nid_list  [tensor([0, 2]), tensor([3]), tensor([1])]
check_connections_block*********************************
connection check : local_output_nid  2
connection check : local_output_nid  1
connection check : local_output_nid  1

block_gen_time in "generate_blocks_for_one_layer_block"  0.0049898624420166016

----------------------------------------
bucketing dataloader: layer  1
bucketing dataloader: num of batch  3
check_connections_block*********************************
connection check : local_output_nid  4
connection check : local_output_nid  4
connection check : local_output_nid  4

block_gen_time in "generate_blocks_for_one_layer_block"  0.0050008296966552734

block collection to dataloader spend  5.4836273193359375e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253204345703125  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557315826416016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12557029724121094  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.5
step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1253209114074707  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555408477783203  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12555503845214844  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(1.9850, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532472610473633  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12556171417236328  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.1255626678466797  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

unweigthed pseudo_mini_loss  tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)
unweigthed weights_list[step]  0.25
----------------------------------------after optimizer
 Nvidia-smi: 1.716796875 GB
    Memory Allocated: 0.12532901763916016  GigaBytes
Max Memory Allocated: 0.1879887580871582  GigaBytes

epoch  4
----------------------------------------------------------pseudo_mini_loss sum 0.7265047430992126
pure train time :  0.04503607749938965
train time :  0.04718804359436035
end to end time :  0.06956100463867188
connection check time:  0.007413387298583984
block generation time  0.009990692138671875
end to end time  0.0760340690612793
