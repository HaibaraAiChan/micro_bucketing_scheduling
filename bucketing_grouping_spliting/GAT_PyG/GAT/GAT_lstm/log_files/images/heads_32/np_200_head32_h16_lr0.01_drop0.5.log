main start at this time 1696111504.3877501
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.002673625946044922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5033950805664062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.6484375 GB
    Memory Allocated: 0.120635986328125  GigaBytes
Max Memory Allocated: 0.12720680236816406  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.6484375 GB
    Memory Allocated: 0.12064075469970703  GigaBytes
Max Memory Allocated: 0.12720680236816406  GigaBytes

pure train time  0.5211193561553955
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.697265625 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.12977170944213867  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9525909423828125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9526 | Train 0.1429 | Val 0.0780 | Test 0.0740
loading full batch data spends  0.00218963623046875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.75390625 GB
    Memory Allocated: 0.15223217010498047  GigaBytes
Max Memory Allocated: 0.15886592864990234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.75390625 GB
    Memory Allocated: 0.1522369384765625  GigaBytes
Max Memory Allocated: 0.15886592864990234  GigaBytes

pure train time  0.1033773422241211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.75390625 GB
    Memory Allocated: 0.04997968673706055  GigaBytes
Max Memory Allocated: 0.16089582443237305  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9489325284957886
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9489 | Train 0.1714 | Val 0.0980 | Test 0.0832
loading full batch data spends  0.001978158950805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15226316452026367  GigaBytes
Max Memory Allocated: 0.16089582443237305  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522679328918457  GigaBytes
Max Memory Allocated: 0.16089582443237305  GigaBytes

pure train time  0.11205816268920898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16140317916870117  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.946462631225586
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9465 | Train 0.1786 | Val 0.1260 | Test 0.1161
loading full batch data spends  0.00199127197265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523432731628418  GigaBytes
Max Memory Allocated: 0.16140317916870117  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15234804153442383  GigaBytes
Max Memory Allocated: 0.16140317916870117  GigaBytes

pure train time  0.1179194450378418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9471291303634644
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9471 | Train 0.1714 | Val 0.1380 | Test 0.1228
loading full batch data spends  0.001995086669921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15286016464233398  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15286493301391602  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

pure train time  0.11022353172302246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050017356872558594  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9480865001678467
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9481 | Train 0.1857 | Val 0.1340 | Test 0.1132
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15260887145996094  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261363983154297  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

pure train time  0.11925697326660156
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.944246768951416
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9442 | Train 0.1643 | Val 0.0900 | Test 0.0880
loading full batch data spends  0.0019791126251220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261507034301758  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526198387145996  GigaBytes
Max Memory Allocated: 0.1621112823486328  GigaBytes

pure train time  0.12448549270629883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05010271072387695  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9443211555480957
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9443 | Train 0.1643 | Val 0.0820 | Test 0.0895
loading full batch data spends  0.0020132064819335938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15276813507080078  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527729034423828  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

pure train time  0.12291693687438965
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9441951513290405
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9442 | Train 0.1786 | Val 0.0840 | Test 0.0909
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15310144424438477  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1531062126159668  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

pure train time  0.12494659423828125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050057411193847656  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.946009874343872
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9460 | Train 0.1929 | Val 0.1660 | Test 0.1431
loading full batch data spends  0.0020227432250976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15262937545776367  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526341438293457  GigaBytes
Max Memory Allocated: 0.16231727600097656  GigaBytes

pure train time  0.1253981590270996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9420260190963745
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9420 | Train 0.1714 | Val 0.1760 | Test 0.1572
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1521306037902832  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15213537216186523  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12600231170654297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.049958229064941406  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9347771406173706
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9348 | Train 0.1643 | Val 0.1880 | Test 0.1572
loading full batch data spends  0.002008676528930664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15240812301635742  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15241289138793945  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12138843536376953
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9395420551300049
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9395 | Train 0.1643 | Val 0.1860 | Test 0.1615
loading full batch data spends  0.001972675323486328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15235233306884766  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523571014404297  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12299346923828125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9457170963287354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9457 | Train 0.1714 | Val 0.1840 | Test 0.1639
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15218067169189453  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15218544006347656  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12259054183959961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9440017938613892
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9440 | Train 0.2143 | Val 0.1500 | Test 0.1243
loading full batch data spends  0.0019741058349609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15311002731323242  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15311479568481445  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12345623970031738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050035953521728516  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.957057237625122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9571 | Train 0.1929 | Val 0.1320 | Test 0.1228
loading full batch data spends  0.0019974708557128906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15244388580322266  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524486541748047  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12170553207397461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9443804025650024
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9444 | Train 0.1714 | Val 0.1040 | Test 0.1040
loading full batch data spends  0.0019867420196533203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15237092971801758  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523756980895996  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.10920310020446777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0499424934387207  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9379898309707642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9380 | Train 0.1929 | Val 0.0900 | Test 0.0899
loading full batch data spends  0.0020210742950439453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15239763259887695  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15240240097045898  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12037110328674316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9244937896728516
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9245 | Train 0.1929 | Val 0.1260 | Test 0.1083
loading full batch data spends  0.001993417739868164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524791717529297  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15248394012451172  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12584733963012695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0499424934387207  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9343279600143433
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9343 | Train 0.2143 | Val 0.1900 | Test 0.1552
loading full batch data spends  0.0020210742950439453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15293121337890625  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15293598175048828  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12337589263916016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005979537963867  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9424399137496948
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9424 | Train 0.2000 | Val 0.1460 | Test 0.1209
loading full batch data spends  0.001934051513671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15254640579223633  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15255117416381836  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.10875582695007324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.932189702987671
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9322 | Train 0.1929 | Val 0.1040 | Test 0.0846
loading full batch data spends  0.0020406246185302734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1518716812133789  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15187644958496094  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12224793434143066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997968673706055  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9326503276824951
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9327 | Train 0.2000 | Val 0.0840 | Test 0.0812
loading full batch data spends  0.0019631385803222656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1532278060913086  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15323257446289062  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12401390075683594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050014495849609375  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.935489535331726
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9355 | Train 0.2000 | Val 0.0900 | Test 0.0832
loading full batch data spends  0.002042055130004883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15239763259887695  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15240240097045898  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12076807022094727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.930808424949646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9308 | Train 0.2071 | Val 0.1040 | Test 0.0924
loading full batch data spends  0.0019571781158447266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15193653106689453  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15194129943847656  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12549328804016113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04993152618408203  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9407280683517456
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9407 | Train 0.2143 | Val 0.1100 | Test 0.0895
loading full batch data spends  0.0020062923431396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15281105041503906  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528158187866211  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.123931884765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998493194580078  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9434401988983154
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9434 | Train 0.2143 | Val 0.1100 | Test 0.0982
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15279245376586914  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15279722213745117  GigaBytes
Max Memory Allocated: 0.16237592697143555  GigaBytes

pure train time  0.12134051322937012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.927336573600769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9273 | Train 0.2214 | Val 0.1040 | Test 0.0957
loading full batch data spends  0.0022106170654296875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15236854553222656  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523733139038086  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.11977338790893555
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997968673706055  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9220033884048462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.9220 | Train 0.2429 | Val 0.1080 | Test 0.1025
loading full batch data spends  0.0019500255584716797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.078315734863281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15274381637573242  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15274858474731445  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12175726890563965
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9406311511993408
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9406 | Train 0.2286 | Val 0.1180 | Test 0.1054
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15266799926757812  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15267276763916016  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.1170969009399414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050017356872558594  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9168498516082764
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9168 | Train 0.2143 | Val 0.1160 | Test 0.1040
loading full batch data spends  0.001955270767211914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15266799926757812  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15267276763916016  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.11966323852539062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9064058065414429
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9064 | Train 0.2143 | Val 0.0980 | Test 0.1025
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15273332595825195  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15273809432983398  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.11896848678588867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.923032522201538
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.9230 | Train 0.2000 | Val 0.0880 | Test 0.0841
loading full batch data spends  0.0022535324096679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.5299530029296875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15231847763061523  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232324600219727  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.11475753784179688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9483178853988647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.9483 | Train 0.2000 | Val 0.0940 | Test 0.0885
loading full batch data spends  0.0020525455474853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15264558792114258  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526503562927246  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.11735200881958008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9516209363937378
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.9516 | Train 0.2214 | Val 0.1120 | Test 0.0870
loading full batch data spends  0.0019741058349609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15303277969360352  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15303754806518555  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12276601791381836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050014495849609375  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9463634490966797
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.9464 | Train 0.2286 | Val 0.0900 | Test 0.0895
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15281009674072266  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528148651123047  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12582874298095703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9343512058258057
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.9344 | Train 0.2071 | Val 0.1160 | Test 0.1098
loading full batch data spends  0.001981973648071289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15260696411132812  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261173248291016  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12403059005737305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998493194580078  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9221502542495728
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9222 | Train 0.2214 | Val 0.1260 | Test 0.1136
loading full batch data spends  0.0019919872283935547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525707244873047  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15257549285888672  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12356758117675781
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9240072965621948
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.9240 | Train 0.2071 | Val 0.1280 | Test 0.1199
loading full batch data spends  0.001951456069946289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.981590270996094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15091276168823242  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15091753005981445  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.1225883960723877
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04993724822998047  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9217036962509155
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.9217 | Train 0.2000 | Val 0.1320 | Test 0.1243
loading full batch data spends  0.002087116241455078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15226125717163086  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522660255432129  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12757277488708496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9357770681381226
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.9358 | Train 0.2143 | Val 0.1280 | Test 0.1233
loading full batch data spends  0.002031564712524414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15273237228393555  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15273714065551758  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.12311553955078125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998493194580078  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9280177354812622
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.9280 | Train 0.2357 | Val 0.1240 | Test 0.1257
loading full batch data spends  0.002013683319091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227031707763672  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227508544921875  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.14292669296264648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9023256301879883
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.9023 | Train 0.2286 | Val 0.1260 | Test 0.1277
loading full batch data spends  0.002017498016357422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.7670135498046875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296316146850586  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529679298400879  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.114837646484375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9202736616134644
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.9203 | Train 0.2357 | Val 0.1300 | Test 0.1262
loading full batch data spends  0.0019495487213134766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15284395217895508  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528487205505371  GigaBytes
Max Memory Allocated: 0.16250848770141602  GigaBytes

pure train time  0.10527253150939941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9249078035354614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.9249 | Train 0.2286 | Val 0.1040 | Test 0.1127
loading full batch data spends  0.0019817352294921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15228700637817383  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15229177474975586  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.10424280166625977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.902701735496521
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.9027 | Train 0.2500 | Val 0.1020 | Test 0.1069
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524052619934082  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15241003036499023  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.1113743782043457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999589920043945  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9163683652877808
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.9164 | Train 0.2714 | Val 0.0900 | Test 0.1025
loading full batch data spends  0.0019795894622802734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15204095840454102  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15204572677612305  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.11607885360717773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.916661262512207
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.9167 | Train 0.2643 | Val 0.0820 | Test 0.1020
loading full batch data spends  0.0020265579223632812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525430679321289  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15254783630371094  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.11175036430358887
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0499262809753418  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.918002963066101
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.9180 | Train 0.2429 | Val 0.0760 | Test 0.0986
loading full batch data spends  0.001986265182495117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15251588821411133  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15252065658569336  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.10618877410888672
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.906640887260437
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.9066 | Train 0.2714 | Val 0.0800 | Test 0.1020
loading full batch data spends  0.002088308334350586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15285587310791016  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528606414794922  GigaBytes
Max Memory Allocated: 0.16260242462158203  GigaBytes

pure train time  0.10502219200134277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9058443307876587
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.9058 | Train 0.2429 | Val 0.0940 | Test 0.1006
loading full batch data spends  0.001961231231689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529083251953125  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15291309356689453  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10810089111328125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.885746955871582
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8857 | Train 0.2143 | Val 0.1200 | Test 0.1059
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296411514282227  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529688835144043  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11465144157409668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003833770751953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9188485145568848
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.9188 | Train 0.2571 | Val 0.1180 | Test 0.1103
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525115966796875  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15251636505126953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.12151026725769043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9276602268218994
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.9277 | Train 0.2357 | Val 0.1180 | Test 0.1122
loading full batch data spends  0.002031564712524414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15245342254638672  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15245819091796875  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10971426963806152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.914609432220459
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.9146 | Train 0.2286 | Val 0.1240 | Test 0.1175
loading full batch data spends  0.0019643306732177734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1530628204345703  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15306758880615234  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10161447525024414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050014495849609375  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.89675772190094
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8968 | Train 0.2500 | Val 0.1180 | Test 0.1204
loading full batch data spends  0.002016305923461914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526932716369629  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15269804000854492  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1049807071685791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998493194580078  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9031167030334473
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.9031 | Train 0.2429 | Val 0.1200 | Test 0.1219
loading full batch data spends  0.001981019973754883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1533341407775879  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15333890914916992  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11123323440551758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050014495849609375  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9186772108078003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.9187 | Train 0.2500 | Val 0.1040 | Test 0.1078
loading full batch data spends  0.002195596694946289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15290498733520508  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529097557067871  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10830402374267578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9091672897338867
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.9092 | Train 0.2714 | Val 0.1080 | Test 0.1054
loading full batch data spends  0.0019757747650146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15290594100952148  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15291070938110352  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11214613914489746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05006265640258789  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9146639108657837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.9147 | Train 0.2571 | Val 0.1060 | Test 0.1049
loading full batch data spends  0.0020389556884765625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15285539627075195  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15286016464233398  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1047203540802002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003833770751953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.897491455078125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8975 | Train 0.2500 | Val 0.0880 | Test 0.0986
loading full batch data spends  0.0019783973693847656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15249347686767578  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524982452392578  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11480832099914551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.049963951110839844  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8822053670883179
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8822 | Train 0.2286 | Val 0.0900 | Test 0.0957
loading full batch data spends  0.002014636993408203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524496078491211  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15245437622070312  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1131443977355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003833770751953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.899839997291565
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8998 | Train 0.2357 | Val 0.0860 | Test 0.1011
loading full batch data spends  0.001986265182495117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15264272689819336  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526474952697754  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1149439811706543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04996919631958008  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8893487453460693
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8893 | Train 0.2714 | Val 0.0940 | Test 0.0986
loading full batch data spends  0.0020265579223632812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524825096130371  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15248727798461914  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.12116742134094238
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003833770751953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8854262828826904
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8854 | Train 0.2643 | Val 0.0880 | Test 0.0986
loading full batch data spends  0.0019927024841308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15318775177001953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15319252014160156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10843205451965332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050046443939208984  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8815239667892456
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8815 | Train 0.2429 | Val 0.0900 | Test 0.1025
loading full batch data spends  0.0020093917846679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15249872207641602  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15250349044799805  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11425399780273438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003833770751953  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8857051134109497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8857 | Train 0.2429 | Val 0.0880 | Test 0.1035
loading full batch data spends  0.001971721649169922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227270126342773  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227746963500977  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11285972595214844
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8858457803726196
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8858 | Train 0.2357 | Val 0.0880 | Test 0.1015
loading full batch data spends  0.002007722854614258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15308475494384766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1530895233154297  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10716390609741211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05004119873046875  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8886215686798096
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8886 | Train 0.2500 | Val 0.0960 | Test 0.1054
loading full batch data spends  0.0019598007202148438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15247344970703125  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15247821807861328  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11076188087463379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04996919631958008  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8879071474075317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8879 | Train 0.2429 | Val 0.0940 | Test 0.1059
loading full batch data spends  0.002000570297241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15244054794311523  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15244531631469727  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10791897773742676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050096988677978516  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8866217136383057
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8866 | Train 0.2429 | Val 0.0880 | Test 0.1074
loading full batch data spends  0.001960277557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15214252471923828  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1521472930908203  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1022942066192627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997968673706055  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.868833065032959
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8688 | Train 0.2500 | Val 0.0880 | Test 0.1093
loading full batch data spends  0.0019757747650146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529073715209961  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15291213989257812  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.09987831115722656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04995584487915039  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.886542797088623
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8865 | Train 0.2500 | Val 0.0880 | Test 0.1044
loading full batch data spends  0.001966714859008789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15235328674316406  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523580551147461  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10514688491821289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8780561685562134
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8781 | Train 0.2571 | Val 0.0960 | Test 0.1107
loading full batch data spends  0.002019643783569336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528310775756836  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15283584594726562  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10645914077758789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8898491859436035
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8898 | Train 0.2786 | Val 0.0940 | Test 0.1141
loading full batch data spends  0.001978635787963867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15278196334838867  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527867317199707  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11202740669250488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8843320608139038
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8843 | Train 0.2714 | Val 0.0920 | Test 0.1088
loading full batch data spends  0.00200653076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15231752395629883  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232229232788086  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11247634887695312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8659343719482422
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8659 | Train 0.2857 | Val 0.1060 | Test 0.1122
loading full batch data spends  0.0019571781158447266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525869369506836  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259170532226562  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11525154113769531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8830795288085938
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8831 | Train 0.2643 | Val 0.0920 | Test 0.1030
loading full batch data spends  0.002045869827270508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15291547775268555  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15292024612426758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11003684997558594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8692569732666016
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8693 | Train 0.2857 | Val 0.1080 | Test 0.1025
loading full batch data spends  0.0019571781158447266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15258216857910156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525869369506836  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1101527214050293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8771111965179443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8771 | Train 0.2643 | Val 0.1080 | Test 0.1093
loading full batch data spends  0.002197742462158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15319204330444336  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1531968116760254  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10711383819580078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003070831298828  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8478020429611206
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8478 | Train 0.2357 | Val 0.0820 | Test 0.0890
loading full batch data spends  0.0019941329956054688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261554718017578  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526203155517578  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.106658935546875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8765863180160522
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8766 | Train 0.2214 | Val 0.0880 | Test 0.0924
loading full batch data spends  0.002053499221801758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15312814712524414  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15313291549682617  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10513877868652344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003070831298828  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.008758783340454
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 2.0088 | Train 0.2143 | Val 0.1040 | Test 0.0924
loading full batch data spends  0.0019745826721191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15236282348632812  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15236759185791016  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11339426040649414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9274442195892334
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.9274 | Train 0.2714 | Val 0.1040 | Test 0.1214
loading full batch data spends  0.002077817916870117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15236186981201172  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15236663818359375  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10859036445617676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05007600784301758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.903714656829834
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.9037 | Train 0.2429 | Val 0.0860 | Test 0.1074
loading full batch data spends  0.002175569534301758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524658203125  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15247058868408203  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11637425422668457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.858304738998413
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8583 | Train 0.2500 | Val 0.1040 | Test 0.1161
loading full batch data spends  0.0023419857025146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.124641418457031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15276479721069336  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527695655822754  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10873794555664062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008125305175781  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8659988641738892
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8660 | Train 0.2571 | Val 0.0880 | Test 0.1132
loading full batch data spends  0.0019707679748535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522984504699707  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15230321884155273  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10534381866455078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.888733148574829
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8887 | Train 0.2286 | Val 0.0800 | Test 0.1112
loading full batch data spends  0.002015352249145508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15274524688720703  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15275001525878906  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10710024833679199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8537222146987915
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8537 | Train 0.2500 | Val 0.0840 | Test 0.0822
loading full batch data spends  0.001973867416381836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1520533561706543  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15205812454223633  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11024022102355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9267957210540771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.9268 | Train 0.1714 | Val 0.1440 | Test 0.1170
loading full batch data spends  0.0020143985748291016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232610702514648  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15233087539672852  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11504793167114258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9113597869873047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.9114 | Train 0.2143 | Val 0.1180 | Test 0.1228
loading full batch data spends  0.001978158950805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15276765823364258  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527724266052246  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11284232139587402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9059034585952759
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.9059 | Train 0.2286 | Val 0.1280 | Test 0.1238
loading full batch data spends  0.0020380020141601562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15295696258544922  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296173095703125  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11381149291992188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8961344957351685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.8961 | Train 0.2214 | Val 0.1200 | Test 0.1223
loading full batch data spends  0.001984119415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523880958557129  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15239286422729492  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1144418716430664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9109538793563843
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.9110 | Train 0.2071 | Val 0.0980 | Test 0.1161
loading full batch data spends  0.002027273178100586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15292024612426758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529250144958496  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10831451416015625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9015483856201172
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.9015 | Train 0.2500 | Val 0.1060 | Test 0.1035
loading full batch data spends  0.0021390914916992188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523876190185547  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15239238739013672  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10433626174926758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.911258578300476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.9113 | Train 0.2571 | Val 0.1080 | Test 0.1069
loading full batch data spends  0.002084016799926758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15337467193603516  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1533794403076172  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10939955711364746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050046443939208984  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.902100920677185
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.9021 | Train 0.2357 | Val 0.1180 | Test 0.1156
loading full batch data spends  0.001961231231689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15266799926757812  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15267276763916016  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1168053150177002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999589920043945  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9035778045654297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.9036 | Train 0.2357 | Val 0.1300 | Test 0.1214
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15251445770263672  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15251922607421875  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10369229316711426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.882438063621521
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.8824 | Train 0.2429 | Val 0.1260 | Test 0.1141
loading full batch data spends  0.001987934112548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15230941772460938  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523141860961914  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10721206665039062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04993724822998047  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.880928874015808
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.8809 | Train 0.2286 | Val 0.1180 | Test 0.1025
loading full batch data spends  0.002014636993408203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523141860961914  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15231895446777344  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1186833381652832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.897578239440918
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8976 | Train 0.2786 | Val 0.1160 | Test 0.1074
loading full batch data spends  0.0019736289978027344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15289735794067383  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15290212631225586  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11393022537231445
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997968673706055  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8963749408721924
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.8964 | Train 0.2714 | Val 0.1140 | Test 0.1049
loading full batch data spends  0.0021991729736328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15252161026000977  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525263786315918  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10113763809204102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.883754849433899
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.8838 | Train 0.2643 | Val 0.1180 | Test 0.1011
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15293598175048828  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529407501220703  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11253905296325684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8819067478179932
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.8819 | Train 0.2571 | Val 0.0940 | Test 0.1025
loading full batch data spends  0.002003908157348633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527233123779297  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15272808074951172  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1075136661529541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8817501068115234
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.8818 | Train 0.2571 | Val 0.0940 | Test 0.0895
loading full batch data spends  0.001962423324584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15309858322143555  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15310335159301758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10848069190979004
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0499720573425293  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8956435918807983
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.8956 | Train 0.2571 | Val 0.1100 | Test 0.1025
loading full batch data spends  0.0020101070404052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261507034301758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526198387145996  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10657095909118652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.857452392578125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.8575 | Train 0.2786 | Val 0.1240 | Test 0.1069
loading full batch data spends  0.0019767284393310547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232372283935547  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523284912109375  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.1138615608215332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050017356872558594  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8653191328048706
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.8653 | Train 0.2929 | Val 0.1260 | Test 0.1161
loading full batch data spends  0.0020182132720947266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1538863182067871  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15389108657836914  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.11000990867614746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8713181018829346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.8713 | Train 0.2786 | Val 0.1200 | Test 0.1170
loading full batch data spends  0.0019674301147460938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528182029724121  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15282297134399414  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10594964027404785
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8667497634887695
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.8667 | Train 0.2786 | Val 0.1200 | Test 0.1209
loading full batch data spends  0.0020151138305664062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15225744247436523  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15226221084594727  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10767126083374023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.869040846824646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.8690 | Train 0.2786 | Val 0.0920 | Test 0.1069
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15261507034301758  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526198387145996  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10833501815795898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8862886428833008
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.8863 | Train 0.2429 | Val 0.1080 | Test 0.1020
loading full batch data spends  0.002022981643676758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15295934677124023  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296411514282227  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10851597785949707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8887184858322144
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.8887 | Train 0.2929 | Val 0.1020 | Test 0.1156
loading full batch data spends  0.0020062923431396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15396785736083984  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15397262573242188  GigaBytes
Max Memory Allocated: 0.16265392303466797  GigaBytes

pure train time  0.10724973678588867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.051030635833740234  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8557262420654297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.8557 | Train 0.2643 | Val 0.1240 | Test 0.1243
loading full batch data spends  0.002016305923461914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15252113342285156  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525259017944336  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.10628223419189453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8851728439331055
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.8852 | Train 0.2571 | Val 0.1300 | Test 0.1209
loading full batch data spends  0.001962423324584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15263891220092773  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15264368057250977  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.1080324649810791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8769291639328003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.8769 | Train 0.2857 | Val 0.1140 | Test 0.1223
loading full batch data spends  0.002010345458984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.2438507080078125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15352535247802734  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15353012084960938  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.10709047317504883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8672653436660767
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.8673 | Train 0.2429 | Val 0.1020 | Test 0.1098
loading full batch data spends  0.002071380615234375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.029273986816406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259456634521484  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259933471679688  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.10395240783691406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999589920043945  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8900161981582642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.8900 | Train 0.2714 | Val 0.1060 | Test 0.1170
loading full batch data spends  0.002362966537475586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.076957702636719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1539449691772461  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15394973754882812  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.11122894287109375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8547757863998413
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.8548 | Train 0.2857 | Val 0.1020 | Test 0.1180
loading full batch data spends  0.002027273178100586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15230894088745117  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523137092590332  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.1145179271697998
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8574016094207764
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.8574 | Train 0.2857 | Val 0.1160 | Test 0.1214
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15256738662719727  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525721549987793  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.11302304267883301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8470135927200317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.8470 | Train 0.2786 | Val 0.0960 | Test 0.1194
loading full batch data spends  0.0019867420196533203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15275096893310547  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527557373046875  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.10466980934143066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.866052269935608
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.8661 | Train 0.2500 | Val 0.0940 | Test 0.1223
loading full batch data spends  0.0019507408142089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524038314819336  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15240859985351562  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.11841988563537598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8454807996749878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.8455 | Train 0.2571 | Val 0.0920 | Test 0.1141
loading full batch data spends  0.0019736289978027344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15197229385375977  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1519770622253418  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.11450767517089844
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04988908767700195  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8502968549728394
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.8503 | Train 0.2643 | Val 0.0820 | Test 0.0986
loading full batch data spends  0.002009868621826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15410947799682617  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1541142463684082  GigaBytes
Max Memory Allocated: 0.1626582145690918  GigaBytes

pure train time  0.10564017295837402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050089359283447266  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8853875398635864
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8854 | Train 0.2571 | Val 0.1040 | Test 0.1141
loading full batch data spends  0.0019795894622802734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15320158004760742  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15320634841918945  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11214733123779297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.051030635833740234  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8312890529632568
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.8313 | Train 0.2357 | Val 0.1000 | Test 0.1107
loading full batch data spends  0.002004861831665039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524977684020996  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15250253677368164  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10981917381286621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8603633642196655
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.8604 | Train 0.2357 | Val 0.1200 | Test 0.1306
loading full batch data spends  0.001977682113647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259265899658203  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259742736816406  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.1165933609008789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8822435140609741
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.8822 | Train 0.2214 | Val 0.1200 | Test 0.1291
loading full batch data spends  0.0020422935485839844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15270042419433594  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15270519256591797  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10264158248901367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8547720909118652
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.8548 | Train 0.2429 | Val 0.1180 | Test 0.1223
loading full batch data spends  0.0019800662994384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15283775329589844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15284252166748047  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10370087623596191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.876533031463623
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.8765 | Train 0.2357 | Val 0.1020 | Test 0.1127
loading full batch data spends  0.002001523971557617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15301990509033203  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15302467346191406  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10566306114196777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8259806632995605
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.8260 | Train 0.3000 | Val 0.1120 | Test 0.1262
loading full batch data spends  0.0019063949584960938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15315628051757812  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15316104888916016  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10321712493896484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998779296875  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8447439670562744
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.8447 | Train 0.2643 | Val 0.1060 | Test 0.1194
loading full batch data spends  0.0020148754119873047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15220117568969727  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522059440612793  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11329221725463867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8728493452072144
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.8728 | Train 0.2786 | Val 0.1020 | Test 0.1238
loading full batch data spends  0.001966714859008789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522970199584961  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15230178833007812  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11003684997558594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999589920043945  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8784416913986206
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.8784 | Train 0.2643 | Val 0.0960 | Test 0.1103
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227890014648438  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522836685180664  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10253524780273438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008649826049805  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.841886281967163
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.8419 | Train 0.2500 | Val 0.0920 | Test 0.1059
loading full batch data spends  0.0023183822631835938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.9577484130859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527862548828125  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15279102325439453  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10695385932922363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.829225778579712
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.8292 | Train 0.2571 | Val 0.0980 | Test 0.1209
loading full batch data spends  0.002035379409790039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15331077575683594  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15331554412841797  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11365056037902832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050089359283447266  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.860891342163086
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.8609 | Train 0.2429 | Val 0.1000 | Test 0.1025
loading full batch data spends  0.0019648075103759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522674560546875  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15227222442626953  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11418676376342773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04992103576660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8492279052734375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.8492 | Train 0.2571 | Val 0.1020 | Test 0.1078
loading full batch data spends  0.0020482540130615234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15282821655273438  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528329849243164  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11235308647155762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050096988677978516  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.851893424987793
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.8519 | Train 0.2214 | Val 0.1100 | Test 0.1228
loading full batch data spends  0.0019876956939697266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1531081199645996  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15311288833618164  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11188149452209473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003070831298828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8325073719024658
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.8325 | Train 0.2214 | Val 0.1180 | Test 0.0982
loading full batch data spends  0.002187967300415039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526045799255371  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15260934829711914  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10592198371887207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8635929822921753
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.8636 | Train 0.2857 | Val 0.1060 | Test 0.1151
loading full batch data spends  0.002148151397705078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529679298400879  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15297269821166992  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10951495170593262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05007314682006836  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8299996852874756
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.8300 | Train 0.2714 | Val 0.1040 | Test 0.1107
loading full batch data spends  0.0020148754119873047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524510383605957  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15245580673217773  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11416459083557129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8232325315475464
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.8232 | Train 0.2786 | Val 0.1140 | Test 0.1228
loading full batch data spends  0.002145051956176758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1531381607055664  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15314292907714844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10459470748901367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008411407470703  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.831547737121582
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.8315 | Train 0.2429 | Val 0.1020 | Test 0.1204
loading full batch data spends  0.0020291805267333984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15247774124145508  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524825096130371  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10872626304626465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8283343315124512
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.8283 | Train 0.2500 | Val 0.1160 | Test 0.1180
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15268564224243164  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15269041061401367  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10609912872314453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04995298385620117  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8364710807800293
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.8365 | Train 0.2786 | Val 0.1100 | Test 0.1257
loading full batch data spends  0.0020439624786376953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1525592803955078  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15256404876708984  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10706067085266113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.866186261177063
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.8662 | Train 0.2714 | Val 0.1200 | Test 0.1306
loading full batch data spends  0.0019750595092773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1523275375366211  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15233230590820312  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11403012275695801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.81862473487854
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.8186 | Train 0.3000 | Val 0.1120 | Test 0.1233
loading full batch data spends  0.0020995140075683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15240812301635742  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15241289138793945  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11001896858215332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008125305175781  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8402838706970215
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.8403 | Train 0.2714 | Val 0.1200 | Test 0.1209
loading full batch data spends  0.001994609832763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15291547775268555  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15292024612426758  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11136102676391602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050035953521728516  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.83151113986969
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.8315 | Train 0.2786 | Val 0.1160 | Test 0.1194
loading full batch data spends  0.0022492408752441406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.030632019042969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15256166458129883  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15256643295288086  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10720062255859375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04997444152832031  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8216161727905273
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.8216 | Train 0.2929 | Val 0.1080 | Test 0.1214
loading full batch data spends  0.002065420150756836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1520977020263672  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15210247039794922  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10688304901123047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04992103576660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8162418603897095
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.8162 | Train 0.2714 | Val 0.1060 | Test 0.1199
loading full batch data spends  0.0020775794982910156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15207290649414062  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15207767486572266  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11343908309936523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8330312967300415
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.8330 | Train 0.2500 | Val 0.1060 | Test 0.1209
loading full batch data spends  0.0020034313201904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15294837951660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529531478881836  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10990238189697266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050017356872558594  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8366154432296753
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.8366 | Train 0.2643 | Val 0.1080 | Test 0.1199
loading full batch data spends  0.0020122528076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526508331298828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15265560150146484  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11339473724365234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8283950090408325
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.8284 | Train 0.2714 | Val 0.1160 | Test 0.1228
loading full batch data spends  0.0020072460174560547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15299034118652344  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15299510955810547  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10218977928161621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002021789550781  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8340785503387451
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.8341 | Train 0.2786 | Val 0.1100 | Test 0.1190
loading full batch data spends  0.002027750015258789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15183687210083008  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1518416404724121  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11167597770690918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8323700428009033
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.8324 | Train 0.2714 | Val 0.1280 | Test 0.1368
loading full batch data spends  0.0019919872283935547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15282917022705078  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528339385986328  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11636972427368164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050043582916259766  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8242076635360718
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.8242 | Train 0.2929 | Val 0.1220 | Test 0.1209
loading full batch data spends  0.002023458480834961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15263986587524414  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15264463424682617  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10281705856323242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04998493194580078  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8332898616790771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.8333 | Train 0.2786 | Val 0.1120 | Test 0.1238
loading full batch data spends  0.0019826889038085938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15304899215698242  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15305376052856445  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10367417335510254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003070831298828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8309515714645386
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.8310 | Train 0.2714 | Val 0.1120 | Test 0.1190
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259361267089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259838104248047  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11350274085998535
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999065399169922  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8170496225357056
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.8170 | Train 0.2643 | Val 0.1200 | Test 0.1262
loading full batch data spends  0.001985311508178711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529383659362793  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15294313430786133  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10556483268737793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.837918758392334
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.8379 | Train 0.2643 | Val 0.1120 | Test 0.1122
loading full batch data spends  0.0020227432250976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15268325805664062  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15268802642822266  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10347509384155273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050065040588378906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8202861547470093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.8203 | Train 0.2000 | Val 0.1040 | Test 0.0982
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524186134338379  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15242338180541992  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11226868629455566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8558928966522217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.8559 | Train 0.2286 | Val 0.1120 | Test 0.1122
loading full batch data spends  0.002006053924560547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15241575241088867  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524205207824707  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10569238662719727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8446826934814453
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.8447 | Train 0.2500 | Val 0.1160 | Test 0.1141
loading full batch data spends  0.001985788345336914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1522054672241211  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15221023559570312  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10479068756103516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002784729003906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.830206036567688
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.8302 | Train 0.2214 | Val 0.1180 | Test 0.1112
loading full batch data spends  0.0020227432250976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15289878845214844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15290355682373047  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10491228103637695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8628445863723755
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.8628 | Train 0.2143 | Val 0.1280 | Test 0.1107
loading full batch data spends  0.0019757747650146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1530618667602539  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15306663513183594  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11342978477478027
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05003070831298828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8851392269134521
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.8851 | Train 0.2500 | Val 0.1420 | Test 0.1146
loading full batch data spends  0.0020411014556884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15304851531982422  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15305328369140625  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10674023628234863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050057411193847656  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8831208944320679
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.8831 | Train 0.2429 | Val 0.1280 | Test 0.1185
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15277719497680664  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15278196334838867  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.1110379695892334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8385432958602905
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.8385 | Train 0.2286 | Val 0.1080 | Test 0.1074
loading full batch data spends  0.00200653076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15251541137695312  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15252017974853516  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11263418197631836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8355494737625122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.8355 | Train 0.2500 | Val 0.1080 | Test 0.1074
loading full batch data spends  0.001978158950805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524815559387207  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15248632431030273  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11114025115966797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05002260208129883  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.804426908493042
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.8044 | Train 0.2429 | Val 0.1220 | Test 0.1136
loading full batch data spends  0.0023047924041748047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.9577484130859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15235137939453125  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15235614776611328  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11127471923828125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500330924987793  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8156367540359497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.8156 | Train 0.2357 | Val 0.1100 | Test 0.1204
loading full batch data spends  0.0020003318786621094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15275335311889648  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15275812149047852  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10234403610229492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8436558246612549
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.8437 | Train 0.2643 | Val 0.1240 | Test 0.1107
loading full batch data spends  0.002015829086303711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1524181365966797  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15242290496826172  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10997653007507324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8350213766098022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.8350 | Train 0.2571 | Val 0.1120 | Test 0.1103
loading full batch data spends  0.001955747604370117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15201282501220703  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15201759338378906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10562586784362793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000638961791992  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7973514795303345
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.7974 | Train 0.2571 | Val 0.1040 | Test 0.1170
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296173095703125  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15296649932861328  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11179995536804199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05010795593261719  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7863037586212158
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.7863 | Train 0.2357 | Val 0.1020 | Test 0.1107
loading full batch data spends  0.001965761184692383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15281009674072266  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1528148651123047  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10695028305053711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050065040588378906  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8048235177993774
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.8048 | Train 0.2714 | Val 0.1020 | Test 0.1165
loading full batch data spends  0.0020182132720947266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1530447006225586  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15304946899414062  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.1122734546661377
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8046425580978394
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.8046 | Train 0.2786 | Val 0.1040 | Test 0.1127
loading full batch data spends  0.0019576549530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527714729309082  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15277624130249023  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.1074063777923584
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05007028579711914  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8018102645874023
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.8018 | Train 0.2857 | Val 0.0980 | Test 0.1161
loading full batch data spends  0.0019505023956298828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526508331298828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15265560150146484  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10458850860595703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8013309240341187
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.8013 | Train 0.2571 | Val 0.0980 | Test 0.1011
loading full batch data spends  0.001994609832763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15280771255493164  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15281248092651367  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11302566528320312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050017356872558594  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8069908618927002
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.8070 | Train 0.2571 | Val 0.0980 | Test 0.1040
loading full batch data spends  0.002009868621826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1520395278930664  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15204429626464844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.1007242202758789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8331643342971802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.8332 | Train 0.2571 | Val 0.1060 | Test 0.1146
loading full batch data spends  0.0020198822021484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.8623809814453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15278196334838867  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527867317199707  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11175179481506348
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04995298385620117  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8013039827346802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.8013 | Train 0.2429 | Val 0.0980 | Test 0.1035
loading full batch data spends  0.0019648075103759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15298032760620117  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529850959777832  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11012125015258789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050057411193847656  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8343042135238647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.8343 | Train 0.2143 | Val 0.0980 | Test 0.1064
loading full batch data spends  0.0024323463439941406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.173683166503906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15294599533081055  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15295076370239258  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10494613647460938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05008125305175781  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7996231317520142
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.7996 | Train 0.2857 | Val 0.1060 | Test 0.1141
loading full batch data spends  0.002067089080810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1533493995666504  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15335416793823242  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.11411356925964355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050057411193847656  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8094055652618408
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.8094 | Train 0.2357 | Val 0.1200 | Test 0.1069
loading full batch data spends  0.002024412155151367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232086181640625  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15232563018798828  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10606980323791504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.049963951110839844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8226362466812134
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.8226 | Train 0.2500 | Val 0.1120 | Test 0.1146
loading full batch data spends  0.0020074844360351562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259504318237305  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259981155395508  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10327839851379395
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8320231437683105
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.8320 | Train 0.2857 | Val 0.1020 | Test 0.1194
loading full batch data spends  0.0019834041595458984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15389633178710938  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1539011001586914  GigaBytes
Max Memory Allocated: 0.16276979446411133  GigaBytes

pure train time  0.10493969917297363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.051062583923339844  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.815205454826355
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.8152 | Train 0.2500 | Val 0.1140 | Test 0.1185
loading full batch data spends  0.002016305923461914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1526494026184082  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15265417098999023  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10744166374206543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.839137077331543
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.8391 | Train 0.2714 | Val 0.1120 | Test 0.1223
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15242576599121094  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15243053436279297  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10814309120178223
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05000114440917969  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8449015617370605
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.8449 | Train 0.2429 | Val 0.1280 | Test 0.1136
loading full batch data spends  0.002026796340942383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15255975723266602  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15256452560424805  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10772514343261719
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05005455017089844  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8200182914733887
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.8200 | Train 0.2714 | Val 0.1060 | Test 0.1185
loading full batch data spends  0.001978158950805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15299510955810547  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1529998779296875  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.11398744583129883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.04999876022338867  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8236987590789795
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.8237 | Train 0.2786 | Val 0.1140 | Test 0.1233
loading full batch data spends  0.0020072460174560547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15259599685668945  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15260076522827148  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.11063385009765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05007600784301758  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.79729163646698
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.7973 | Train 0.2643 | Val 0.1260 | Test 0.1281
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15289974212646484  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15290451049804688  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10364508628845215
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050065040588378906  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8521329164505005
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.8521 | Train 0.2571 | Val 0.1240 | Test 0.1320
loading full batch data spends  0.002008199691772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15285348892211914  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15285825729370117  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10860300064086914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050011634826660156  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.826951503753662
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.8270 | Train 0.2714 | Val 0.1260 | Test 0.1301
loading full batch data spends  0.0019953250885009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15210437774658203  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15210914611816406  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10365533828735352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050065040588378906  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8034173250198364
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.8034 | Train 0.2786 | Val 0.1280 | Test 0.1330
loading full batch data spends  0.002069234848022461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15371990203857422  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15372467041015625  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.10300993919372559
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.05006265640258789  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8458322286605835
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.8458 | Train 0.2857 | Val 0.1240 | Test 0.1349
loading full batch data spends  0.00197601318359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15196561813354492  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15197038650512695  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.12061500549316406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.050065040588378906  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.795300841331482
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.7953 | Train 0.2786 | Val 0.1340 | Test 0.1291
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.15270709991455078  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.1527118682861328  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

pure train time  0.11304903030395508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.7734375 GB
    Memory Allocated: 0.0500493049621582  GigaBytes
Max Memory Allocated: 0.1629047393798828  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.825337290763855
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 32, 16])
h.size() torch.Size([2708, 512])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.8253 | Train 0.2857 | Val 0.1260 | Test 0.1296
Total (block generation + training)time/epoch 0.12292555451393128
pure train time/epoch 0.11173688392249906

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368]
