main start at this time 1696109153.248714
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.0030677318572998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.56640625 GB
    Memory Allocated: 0.03950691223144531  GigaBytes
Max Memory Allocated: 0.04075431823730469  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.56640625 GB
    Memory Allocated: 0.039511680603027344  GigaBytes
Max Memory Allocated: 0.04075431823730469  GigaBytes

pure train time  0.5193581581115723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.57421875 GB
    Memory Allocated: 0.01199960708618164  GigaBytes
Max Memory Allocated: 0.04075431823730469  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9523334503173828
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9523 | Train 0.1500 | Val 0.2600 | Test 0.2809
loading full batch data spends  0.002023458480834961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353046417236328  GigaBytes
Max Memory Allocated: 0.05611085891723633  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353523254394531  GigaBytes
Max Memory Allocated: 0.05611085891723633  GigaBytes

pure train time  0.10653233528137207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012132644653320312  GigaBytes
Max Memory Allocated: 0.05611085891723633  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9494010210037231
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9494 | Train 0.1286 | Val 0.1000 | Test 0.1233
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04312753677368164  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04313230514526367  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.11046743392944336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012026309967041016  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.947053074836731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9471 | Train 0.1500 | Val 0.1180 | Test 0.1088
loading full batch data spends  0.0021483898162841797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04323244094848633  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04323720932006836  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.10953235626220703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9451402425765991
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9451 | Train 0.2071 | Val 0.1180 | Test 0.0953
loading full batch data spends  0.0019550323486328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324054718017578  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324531555175781  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.10783219337463379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01203155517578125  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9435195922851562
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9435 | Train 0.1857 | Val 0.1160 | Test 0.0991
loading full batch data spends  0.002156496047973633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337501525878906  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043379783630371094  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.12077713012695312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012127399444580078  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9427295923233032
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9427 | Train 0.1857 | Val 0.1160 | Test 0.0977
loading full batch data spends  0.0019423961639404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348945617675781  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043494224548339844  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.12005162239074707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01213836669921875  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.947171926498413
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9472 | Train 0.1929 | Val 0.1140 | Test 0.0957
loading full batch data spends  0.0019876956939697266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338359832763672  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338836669921875  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

pure train time  0.11897683143615723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012127399444580078  GigaBytes
Max Memory Allocated: 0.056243896484375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9401016235351562
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9401 | Train 0.1857 | Val 0.1160 | Test 0.0933
loading full batch data spends  0.0021240711212158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338836669921875  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339313507080078  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.11866331100463867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01213836669921875  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.937491774559021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9375 | Train 0.1786 | Val 0.1140 | Test 0.0924
loading full batch data spends  0.0021507740020751953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333925247192383  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334402084350586  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.11897754669189453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9437534809112549
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9438 | Train 0.1714 | Val 0.1020 | Test 0.0890
loading full batch data spends  0.0021200180053710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043360233306884766  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433650016784668  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.11857914924621582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01213836669921875  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9357397556304932
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9357 | Train 0.1714 | Val 0.1020 | Test 0.0880
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043428897857666016  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343366622924805  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.12087798118591309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.935400366783142
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9354 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0021042823791503906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349327087402344  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349803924560547  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.11988568305969238
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01213836669921875  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9352225065231323
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9352 | Train 0.1714 | Val 0.1140 | Test 0.0914
loading full batch data spends  0.001973867416381836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353523254394531  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043540000915527344  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

pure train time  0.11914944648742676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01213836669921875  GigaBytes
Max Memory Allocated: 0.05628490447998047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.925504446029663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9255 | Train 0.1929 | Val 0.1160 | Test 0.1015
loading full batch data spends  0.00194549560546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455137252807617  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445561408996582  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

pure train time  0.11909127235412598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9253356456756592
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9253 | Train 0.1714 | Val 0.1140 | Test 0.1219
loading full batch data spends  0.002168416976928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345846176147461  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346323013305664  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

pure train time  0.12093853950500488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.05629587173461914  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.927068829536438
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9271 | Train 0.1500 | Val 0.1220 | Test 0.1238
loading full batch data spends  0.0019369125366210938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043497562408447266  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435023307800293  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

pure train time  0.12219452857971191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9384379386901855
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9384 | Train 0.1500 | Val 0.1180 | Test 0.1223
loading full batch data spends  0.001990079879760742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339742660522461  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340219497680664  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

pure train time  0.11617588996887207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9200152158737183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9200 | Train 0.1714 | Val 0.1200 | Test 0.1219
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044399261474609375  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044404029846191406  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

pure train time  0.11758589744567871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.925633430480957
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9256 | Train 0.1714 | Val 0.1220 | Test 0.1223
loading full batch data spends  0.0019829273223876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043381690979003906  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338645935058594  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

pure train time  0.11855888366699219
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.056322574615478516  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9092326164245605
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9092 | Train 0.1714 | Val 0.1220 | Test 0.1223
loading full batch data spends  0.0021071434020996094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044409751892089844  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044414520263671875  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.12209868431091309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9330638647079468
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9331 | Train 0.1929 | Val 0.1300 | Test 0.1185
loading full batch data spends  0.002184629440307617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043485164642333984  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043489933013916016  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.11435508728027344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01203155517578125  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9102586507797241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9103 | Train 0.2071 | Val 0.1320 | Test 0.1170
loading full batch data spends  0.0020987987518310547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343414306640625  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343891143798828  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.10950064659118652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9028483629226685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9028 | Train 0.2286 | Val 0.1320 | Test 0.1175
loading full batch data spends  0.002160310745239258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340696334838867  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434117317199707  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.12144064903259277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012042045593261719  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.900025725364685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9000 | Train 0.2286 | Val 0.1340 | Test 0.1141
loading full batch data spends  0.0021123886108398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347562789916992  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348039627075195  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.1187143325805664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.898350715637207
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.8984 | Train 0.2214 | Val 0.1280 | Test 0.1185
loading full batch data spends  0.002102375030517578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445838928222656  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044463157653808594  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

pure train time  0.10680890083312988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.05632781982421875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8951396942138672
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.8951 | Train 0.2286 | Val 0.1300 | Test 0.1156
loading full batch data spends  0.0021424293518066406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450511932373047  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445098876953125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1208648681640625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8970928192138672
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.8971 | Train 0.2214 | Val 0.1280 | Test 0.1146
loading full batch data spends  0.0021927356719970703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04329633712768555  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04330110549926758  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11749863624572754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8972619771957397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.8973 | Train 0.2571 | Val 0.1400 | Test 0.1281
loading full batch data spends  0.002102375030517578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044536590576171875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044541358947753906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12050533294677734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8887171745300293
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.8887 | Train 0.2286 | Val 0.1720 | Test 0.1262
loading full batch data spends  0.0022139549255371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.933906555175781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445246696472168  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452943801879883  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1235964298248291
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9024584293365479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9025 | Train 0.2571 | Val 0.1520 | Test 0.1262
loading full batch data spends  0.002118825912475586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043494224548339844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043498992919921875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12189269065856934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9066498279571533
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9066 | Train 0.2643 | Val 0.1440 | Test 0.1262
loading full batch data spends  0.002482891082763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.076957702636719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044521331787109375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044526100158691406  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11715078353881836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8904821872711182
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.8905 | Train 0.2857 | Val 0.1200 | Test 0.1190
loading full batch data spends  0.0019328594207763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043426513671875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343128204345703  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10457229614257812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8956241607666016
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.8956 | Train 0.2571 | Val 0.1460 | Test 0.1223
loading full batch data spends  0.0020036697387695312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043322086334228516  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332685470581055  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10996460914611816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012026309967041016  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8971081972122192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.8971 | Train 0.2000 | Val 0.1480 | Test 0.1238
loading full batch data spends  0.001950979232788086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435795783996582  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043584346771240234  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10655498504638672
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.87933349609375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8793 | Train 0.2286 | Val 0.1480 | Test 0.1248
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043401241302490234  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043406009674072266  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10680627822875977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.873011589050293
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8730 | Train 0.2429 | Val 0.1280 | Test 0.1204
loading full batch data spends  0.0019335746765136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451799392700195  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044522762298583984  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10666370391845703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9148199558258057
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9148 | Train 0.2500 | Val 0.1240 | Test 0.1228
loading full batch data spends  0.001987934112548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451560974121094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452037811279297  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10988426208496094
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8974672555923462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8975 | Train 0.2357 | Val 0.1200 | Test 0.1190
loading full batch data spends  0.0019986629486083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044602394104003906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04460716247558594  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10627222061157227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8678009510040283
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8678 | Train 0.2357 | Val 0.1220 | Test 0.1199
loading full batch data spends  0.002003908157348633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435333251953125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353809356689453  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10389256477355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8874688148498535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8875 | Train 0.2500 | Val 0.1180 | Test 0.1132
loading full batch data spends  0.0019259452819824219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339599609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340076446533203  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1035914421081543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.894440770149231
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.8944 | Train 0.2571 | Val 0.1120 | Test 0.1093
loading full batch data spends  0.0019905567169189453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044696807861328125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044701576232910156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1016242504119873
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8704768419265747
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8705 | Train 0.2357 | Val 0.1040 | Test 0.1098
loading full batch data spends  0.0015025138854980469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043483734130859375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043488502502441406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10498356819152832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8953009843826294
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.8953 | Train 0.2500 | Val 0.1200 | Test 0.1175
loading full batch data spends  0.0014851093292236328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452800750732422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453277587890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10544919967651367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9015668630599976
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.9016 | Train 0.2429 | Val 0.1400 | Test 0.1209
loading full batch data spends  0.0014872550964355469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445399284362793  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454469680786133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1059410572052002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.869200348854065
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8692 | Train 0.2286 | Val 0.1380 | Test 0.1252
loading full batch data spends  0.0015444755554199219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044542789459228516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454755783081055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10519266128540039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8843486309051514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.8843 | Train 0.2429 | Val 0.1320 | Test 0.1262
loading full batch data spends  0.001424551010131836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336118698120117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09893536567687988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8978036642074585
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.8978 | Train 0.2571 | Val 0.1200 | Test 0.1194
loading full batch data spends  0.0016560554504394531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04437446594238281  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044379234313964844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09863734245300293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9015445709228516
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.9015 | Train 0.2500 | Val 0.1400 | Test 0.1248
loading full batch data spends  0.0016202926635742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453849792480469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454326629638672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09834694862365723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8728663921356201
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8729 | Train 0.2643 | Val 0.1420 | Test 0.1151
loading full batch data spends  0.0014832019805908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043256282806396484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043261051177978516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0966804027557373
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012026309967041016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8668317794799805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8668 | Train 0.2571 | Val 0.1460 | Test 0.1238
loading full batch data spends  0.0014233589172363281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043453216552734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043457984924316406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09884762763977051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8916515111923218
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8917 | Train 0.2500 | Val 0.1500 | Test 0.1281
loading full batch data spends  0.0015032291412353516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043421268463134766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434260368347168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09581923484802246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012036800384521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9080055952072144
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.9080 | Train 0.2429 | Val 0.1480 | Test 0.1335
loading full batch data spends  0.0015895366668701172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044467926025390625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044472694396972656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10596299171447754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.883760929107666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8838 | Train 0.2357 | Val 0.1460 | Test 0.1272
loading full batch data spends  0.0016527175903320312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342842102050781  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043433189392089844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1001737117767334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8830581903457642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8831 | Train 0.2429 | Val 0.1500 | Test 0.1296
loading full batch data spends  0.0014269351959228516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341888427734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342365264892578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09906864166259766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8843187093734741
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8843 | Train 0.2429 | Val 0.1520 | Test 0.1262
loading full batch data spends  0.0014767646789550781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449939727783203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450416564941406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09975194931030273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8943296670913696
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8943 | Train 0.2429 | Val 0.1520 | Test 0.1257
loading full batch data spends  0.0014300346374511719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452180862426758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452657699584961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1036064624786377
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.877748727798462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8777 | Train 0.2643 | Val 0.1420 | Test 0.1228
loading full batch data spends  0.0015003681182861328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04330110549926758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04330587387084961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09861636161804199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8955141305923462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8955 | Train 0.2714 | Val 0.1400 | Test 0.1204
loading full batch data spends  0.0014340877532958984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043529510498046875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043534278869628906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09956693649291992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8783835172653198
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8784 | Train 0.3000 | Val 0.1180 | Test 0.1156
loading full batch data spends  0.0015940666198730469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.029273986816406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043612003326416016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361677169799805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10542678833007812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012066364288330078  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8767669200897217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8768 | Train 0.2714 | Val 0.1060 | Test 0.1165
loading full batch data spends  0.0016672611236572266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445028305053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445505142211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10615777969360352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8737337589263916
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8737 | Train 0.2857 | Val 0.1080 | Test 0.1141
loading full batch data spends  0.0015039443969726562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349517822265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349994659423828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10422492027282715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896704077720642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8967 | Train 0.2786 | Val 0.1000 | Test 0.1103
loading full batch data spends  0.0014548301696777344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0443425178527832  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044347286224365234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10747957229614258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.876254677772522
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8763 | Train 0.2643 | Val 0.1080 | Test 0.1078
loading full batch data spends  0.001615762710571289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349851608276367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435032844543457  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1093597412109375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8559269905090332
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8559 | Train 0.2929 | Val 0.1020 | Test 0.1117
loading full batch data spends  0.001573324203491211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457712173461914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04458189010620117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10190963745117188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8568025827407837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8568 | Train 0.2929 | Val 0.1120 | Test 0.1103
loading full batch data spends  0.0015370845794677734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043460845947265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043465614318847656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10216903686523438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8733110427856445
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8733 | Train 0.2786 | Val 0.1220 | Test 0.1151
loading full batch data spends  0.0014345645904541016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443216323852539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443693161010742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09936046600341797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8603343963623047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8603 | Train 0.2643 | Val 0.1260 | Test 0.1156
loading full batch data spends  0.0014808177947998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352235794067383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352712631225586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10443925857543945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8911669254302979
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8912 | Train 0.2714 | Val 0.1340 | Test 0.1204
loading full batch data spends  0.0015058517456054688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346466064453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346942901611328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10193634033203125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8542687892913818
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8543 | Train 0.2714 | Val 0.1400 | Test 0.1257
loading full batch data spends  0.0015463829040527344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04437732696533203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438209533691406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09976029396057129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8890517950057983
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8891 | Train 0.2714 | Val 0.1400 | Test 0.1252
loading full batch data spends  0.0014557838439941406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341840744018555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342317581176758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0958397388458252
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8674837350845337
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8675 | Train 0.2643 | Val 0.1340 | Test 0.1252
loading full batch data spends  0.0015368461608886719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044589996337890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044594764709472656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09869980812072754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.869503378868103
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8695 | Train 0.2714 | Val 0.1300 | Test 0.1248
loading full batch data spends  0.0014858245849609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044551849365234375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044556617736816406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0975039005279541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8448723554611206
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8449 | Train 0.2857 | Val 0.1300 | Test 0.1296
loading full batch data spends  0.0015451908111572266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044381141662597656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438591003417969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10545611381530762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8575371503829956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8575 | Train 0.2643 | Val 0.1200 | Test 0.1204
loading full batch data spends  0.0015981197357177734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444265365600586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444742202758789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0980994701385498
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.851624846458435
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8516 | Train 0.2786 | Val 0.1120 | Test 0.1156
loading full batch data spends  0.0014886856079101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043669700622558594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043674468994140625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09552502632141113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012093067169189453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8720035552978516
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8720 | Train 0.2571 | Val 0.1080 | Test 0.1117
loading full batch data spends  0.0015065670013427734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352712631225586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353189468383789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09991598129272461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8283028602600098
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8283 | Train 0.2786 | Val 0.1060 | Test 0.1088
loading full batch data spends  0.0015025138854980469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044558048248291016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04456281661987305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09381771087646484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8643070459365845
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8643 | Train 0.2786 | Val 0.1000 | Test 0.1122
loading full batch data spends  0.001588582992553711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04322528839111328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04323005676269531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10164904594421387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.850797176361084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8508 | Train 0.2929 | Val 0.1120 | Test 0.1204
loading full batch data spends  0.001615762710571289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463052749633789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463529586791992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10052776336669922
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.853446364402771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8534 | Train 0.2643 | Val 0.1100 | Test 0.1098
loading full batch data spends  0.001474618911743164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459381103515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459857940673828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10666966438293457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.861757516860962
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8618 | Train 0.2571 | Val 0.1120 | Test 0.1112
loading full batch data spends  0.0014612674713134766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331827163696289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332304000854492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10082864761352539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8374499082565308
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8374 | Train 0.2500 | Val 0.1060 | Test 0.1175
loading full batch data spends  0.001500844955444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444456100463867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444493293762207  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10705399513244629
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8451817035675049
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8452 | Train 0.2429 | Val 0.1160 | Test 0.1199
loading full batch data spends  0.0014996528625488281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336977005004883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337453842163086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09872603416442871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012047290802001953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8207210302352905
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8207 | Train 0.2429 | Val 0.1220 | Test 0.1219
loading full batch data spends  0.00148773193359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454612731933594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455089569091797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09953165054321289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8616769313812256
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8617 | Train 0.2500 | Val 0.1140 | Test 0.1223
loading full batch data spends  0.001500844955444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04458951950073242  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459428787231445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0994114875793457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8237876892089844
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8238 | Train 0.2714 | Val 0.1200 | Test 0.1248
loading full batch data spends  0.001447916030883789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435032844543457  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043508052825927734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09860396385192871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.841741919517517
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8417 | Train 0.2857 | Val 0.1240 | Test 0.1161
loading full batch data spends  0.0014870166778564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352903366088867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435338020324707  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09718537330627441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012034416198730469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8261168003082275
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8261 | Train 0.2714 | Val 0.1220 | Test 0.1165
loading full batch data spends  0.0014452934265136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044508934020996094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044513702392578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09923911094665527
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.864133358001709
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8641 | Train 0.2714 | Val 0.1280 | Test 0.1223
loading full batch data spends  0.0016467571258544922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044391632080078125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044396400451660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1040647029876709
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8787386417388916
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8787 | Train 0.2786 | Val 0.1340 | Test 0.1267
loading full batch data spends  0.0014529228210449219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04323625564575195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043241024017333984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09934878349304199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8479315042495728
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.8479 | Train 0.2857 | Val 0.1420 | Test 0.1277
loading full batch data spends  0.0014941692352294922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043463706970214844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043468475341796875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10407161712646484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8308223485946655
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.8308 | Train 0.2786 | Val 0.1220 | Test 0.1291
loading full batch data spends  0.0015053749084472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345369338989258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345846176147461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10614776611328125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8780369758605957
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8780 | Train 0.2571 | Val 0.1240 | Test 0.1267
loading full batch data spends  0.0014722347259521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453229904174805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453706741333008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09798860549926758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8487985134124756
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8488 | Train 0.2786 | Val 0.1260 | Test 0.1257
loading full batch data spends  0.0014941692352294922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043422698974609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043427467346191406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09812021255493164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8447341918945312
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.8447 | Train 0.2714 | Val 0.1360 | Test 0.1252
loading full batch data spends  0.0015010833740234375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349803924560547  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435028076171875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1049962043762207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8424080610275269
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8424 | Train 0.2571 | Val 0.1240 | Test 0.1204
loading full batch data spends  0.0014634132385253906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443025588989258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443502426147461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09950780868530273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8662598133087158
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.8663 | Train 0.2571 | Val 0.1380 | Test 0.1223
loading full batch data spends  0.0014808177947998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442167282104492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442644119262695  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10132384300231934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8296782970428467
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.8297 | Train 0.2643 | Val 0.1140 | Test 0.1175
loading full batch data spends  0.0014576911926269531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345083236694336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345560073852539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10495710372924805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8098036050796509
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.8098 | Train 0.2143 | Val 0.1340 | Test 0.1277
loading full batch data spends  0.001482248306274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044501304626464844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044506072998046875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09993553161621094
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.871375560760498
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8714 | Train 0.2571 | Val 0.1300 | Test 0.1252
loading full batch data spends  0.0015904903411865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340410232543945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043408870697021484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10078954696655273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8295656442642212
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.8296 | Train 0.2786 | Val 0.1180 | Test 0.1320
loading full batch data spends  0.0016148090362548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337644577026367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433812141418457  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10574626922607422
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.825072169303894
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.8251 | Train 0.2857 | Val 0.1260 | Test 0.1407
loading full batch data spends  0.0017879009246826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.57763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343748092651367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434422492980957  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09596514701843262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.828913688659668
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.8289 | Train 0.2786 | Val 0.1260 | Test 0.1325
loading full batch data spends  0.0015094280242919922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445246696472168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452943801879883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09920692443847656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8079147338867188
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.8079 | Train 0.2857 | Val 0.1200 | Test 0.1354
loading full batch data spends  0.0014531612396240234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334306716918945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043347835540771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10375642776489258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8372244834899902
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.8372 | Train 0.3000 | Val 0.1100 | Test 0.1397
loading full batch data spends  0.001474142074584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334259033203125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334735870361328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09975981712341309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012020587921142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8360990285873413
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.8361 | Train 0.2571 | Val 0.0960 | Test 0.1248
loading full batch data spends  0.0016770362854003906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447746276855469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448223114013672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10205483436584473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.831640601158142
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.8316 | Train 0.2714 | Val 0.1120 | Test 0.1209
loading full batch data spends  0.0014886856079101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346656799316406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043471336364746094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11443424224853516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8300918340682983
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.8301 | Train 0.2571 | Val 0.1140 | Test 0.1219
loading full batch data spends  0.0014729499816894531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044522762298583984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044527530670166016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09959983825683594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8276318311691284
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.8276 | Train 0.2643 | Val 0.1140 | Test 0.1257
loading full batch data spends  0.0014796257019042969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044487953186035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449272155761719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09886312484741211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8145469427108765
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.8145 | Train 0.2571 | Val 0.1260 | Test 0.1325
loading full batch data spends  0.0014843940734863281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347848892211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348325729370117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10082483291625977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8464924097061157
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.8465 | Train 0.3000 | Val 0.1260 | Test 0.1359
loading full batch data spends  0.0014803409576416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044492244720458984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044497013092041016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09950375556945801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8376896381378174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.8377 | Train 0.3143 | Val 0.1200 | Test 0.1335
loading full batch data spends  0.0014624595642089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044404029846191406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04440879821777344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10550975799560547
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8096518516540527
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.8097 | Train 0.2929 | Val 0.1200 | Test 0.1204
loading full batch data spends  0.0016427040100097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044483184814453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044487953186035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10470008850097656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8560935258865356
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.8561 | Train 0.3143 | Val 0.1240 | Test 0.1199
loading full batch data spends  0.001508474349975586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451179504394531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044516563415527344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10432124137878418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8593471050262451
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.8593 | Train 0.2857 | Val 0.1220 | Test 0.1190
loading full batch data spends  0.0015277862548828125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044420719146728516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442548751831055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09903955459594727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.838436484336853
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.8384 | Train 0.2571 | Val 0.1280 | Test 0.1209
loading full batch data spends  0.001432657241821289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442930221557617  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444340705871582  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09645628929138184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8262958526611328
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.8263 | Train 0.2786 | Val 0.1320 | Test 0.1214
loading full batch data spends  0.001470327377319336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044425010681152344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044429779052734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10018754005432129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8093349933624268
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.8093 | Train 0.2786 | Val 0.1240 | Test 0.1170
loading full batch data spends  0.0014851093292236328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443502426147461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443979263305664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10027599334716797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8059767484664917
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.8060 | Train 0.2786 | Val 0.1280 | Test 0.1223
loading full batch data spends  0.001489400863647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349946975708008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350423812866211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09553647041320801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.839084267616272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.8391 | Train 0.2714 | Val 0.1180 | Test 0.1141
loading full batch data spends  0.0015201568603515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043474674224853516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347944259643555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10789299011230469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.81229829788208
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.8123 | Train 0.2714 | Val 0.1180 | Test 0.1175
loading full batch data spends  0.0015823841094970703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435175895690918  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352235794067383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09998559951782227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.833492636680603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.8335 | Train 0.3000 | Val 0.1200 | Test 0.1219
loading full batch data spends  0.0014595985412597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334831237792969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335308074951172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10118794441223145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8425666093826294
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.8426 | Train 0.2786 | Val 0.1240 | Test 0.1257
loading full batch data spends  0.0015494823455810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451751708984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452228546142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09960365295410156
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8430054187774658
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8430 | Train 0.2786 | Val 0.1320 | Test 0.1219
loading full batch data spends  0.0015001296997070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452228546142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452705383300781  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10349082946777344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.791969895362854
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.7920 | Train 0.3214 | Val 0.1260 | Test 0.1146
loading full batch data spends  0.0016880035400390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346036911010742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346513748168945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09961223602294922
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8159399032592773
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.8159 | Train 0.2929 | Val 0.1260 | Test 0.1248
loading full batch data spends  0.0014541149139404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447317123413086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447793960571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10560297966003418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.817281723022461
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.8173 | Train 0.3143 | Val 0.1320 | Test 0.1320
loading full batch data spends  0.00151824951171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333305358886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333782196044922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10690641403198242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012042045593261719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8396201133728027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.8396 | Train 0.3143 | Val 0.1300 | Test 0.1306
loading full batch data spends  0.0015034675598144531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04326343536376953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04326820373535156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10099363327026367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8343669176101685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.8344 | Train 0.3286 | Val 0.1360 | Test 0.1272
loading full batch data spends  0.0014739036560058594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445791244506836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446268081665039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10140204429626465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8232024908065796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.8232 | Train 0.3214 | Val 0.1220 | Test 0.1277
loading full batch data spends  0.0014319419860839844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044465065002441406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446983337402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09963345527648926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8066694736480713
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.8067 | Train 0.3143 | Val 0.1320 | Test 0.1248
loading full batch data spends  0.0015320777893066406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451704025268555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452180862426758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09890413284301758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8016923666000366
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.8017 | Train 0.2643 | Val 0.1240 | Test 0.1185
loading full batch data spends  0.0014312267303466797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433807373046875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338550567626953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10163259506225586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8101661205291748
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.8102 | Train 0.2643 | Val 0.1100 | Test 0.1209
loading full batch data spends  0.0014848709106445312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044406890869140625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044411659240722656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09488654136657715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8213547468185425
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.8214 | Train 0.3000 | Val 0.1140 | Test 0.1228
loading full batch data spends  0.0014586448669433594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450511932373047  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445098876953125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09888124465942383
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.826310157775879
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.8263 | Train 0.3071 | Val 0.1260 | Test 0.1141
loading full batch data spends  0.0016410350799560547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454326629638672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454803466796875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09889769554138184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7946161031723022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7946 | Train 0.3071 | Val 0.1200 | Test 0.1136
loading full batch data spends  0.0014846324920654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351234436035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043517112731933594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10274505615234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8234524726867676
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.8235 | Train 0.3071 | Val 0.1280 | Test 0.1214
loading full batch data spends  0.001651763916015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043355464935302734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043360233306884766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09938168525695801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7848881483078003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7849 | Train 0.3143 | Val 0.1440 | Test 0.1368
loading full batch data spends  0.0014796257019042969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457902908325195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044583797454833984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09911513328552246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.795915961265564
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.7959 | Train 0.3643 | Val 0.1600 | Test 0.1426
loading full batch data spends  0.0014734268188476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450416564941406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044508934020996094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09500265121459961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.801074743270874
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.8011 | Train 0.3429 | Val 0.1520 | Test 0.1451
loading full batch data spends  0.0014472007751464844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043520450592041016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352521896362305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09891986846923828
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8093693256378174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.8094 | Train 0.3214 | Val 0.1440 | Test 0.1465
loading full batch data spends  0.0015151500701904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337739944458008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338216781616211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10500955581665039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7937166690826416
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7937 | Train 0.3429 | Val 0.1600 | Test 0.1446
loading full batch data spends  0.001501321792602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444456100463867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444493293762207  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10549664497375488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8140885829925537
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.8141 | Train 0.3143 | Val 0.1500 | Test 0.1306
loading full batch data spends  0.001489877700805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350137710571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350614547729492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10698151588439941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8303380012512207
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.8303 | Train 0.3571 | Val 0.1580 | Test 0.1431
loading full batch data spends  0.001482248306274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044528961181640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044533729553222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09966707229614258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7613797187805176
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7614 | Train 0.2571 | Val 0.1340 | Test 0.1330
loading full batch data spends  0.0015571117401123047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338264465332031  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043387413024902344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09927749633789062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.803370475769043
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.8034 | Train 0.2357 | Val 0.1300 | Test 0.1146
loading full batch data spends  0.0014493465423583984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044617652893066406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04462242126464844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10368657112121582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8056392669677734
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.8056 | Train 0.3429 | Val 0.1660 | Test 0.1605
loading full batch data spends  0.001466989517211914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044342041015625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04434680938720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1030580997467041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.774604082107544
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.7746 | Train 0.3857 | Val 0.1720 | Test 0.1538
loading full batch data spends  0.0014891624450683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346323013305664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346799850463867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10068655014038086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7480512857437134
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.7481 | Train 0.3643 | Val 0.1680 | Test 0.1489
loading full batch data spends  0.0016145706176757812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455232620239258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455709457397461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09856891632080078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7724568843841553
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7725 | Train 0.3857 | Val 0.1820 | Test 0.1499
loading full batch data spends  0.0014421939849853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043469905853271484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043474674224853516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0989983081817627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7532435655593872
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.7532 | Train 0.3857 | Val 0.1760 | Test 0.1644
loading full batch data spends  0.0015358924865722656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341602325439453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342079162597656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10048937797546387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.749444603919983
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.7494 | Train 0.2857 | Val 0.1460 | Test 0.1576
loading full batch data spends  0.0014255046844482422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446697235107422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447174072265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10120916366577148
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7998956441879272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.7999 | Train 0.4000 | Val 0.1680 | Test 0.1630
loading full batch data spends  0.0014624595642089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433802604675293  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338502883911133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10363340377807617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.737636923789978
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.7376 | Train 0.3500 | Val 0.1660 | Test 0.1339
loading full batch data spends  0.0014407634735107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447317123413086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447793960571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10033535957336426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.794031023979187
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.7940 | Train 0.3071 | Val 0.1520 | Test 0.1364
loading full batch data spends  0.001477956771850586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346656799316406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043471336364746094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0997152328491211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8037835359573364
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.8038 | Train 0.3286 | Val 0.1540 | Test 0.1373
loading full batch data spends  0.0014302730560302734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043250083923339844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043254852294921875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09900093078613281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7684435844421387
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.7684 | Train 0.3571 | Val 0.1700 | Test 0.1388
loading full batch data spends  0.0014753341674804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453277587890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453754425048828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09880256652832031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.74443781375885
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.7444 | Train 0.4071 | Val 0.1860 | Test 0.1663
loading full batch data spends  0.0014803409576416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044469356536865234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044474124908447266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09819889068603516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.777996301651001
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.7780 | Train 0.4071 | Val 0.1780 | Test 0.1712
loading full batch data spends  0.0015323162078857422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043521881103515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043526649475097656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10660028457641602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7590560913085938
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.7591 | Train 0.3786 | Val 0.1640 | Test 0.1634
loading full batch data spends  0.001596689224243164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349660873413086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350137710571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10395312309265137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.738799810409546
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.7388 | Train 0.4071 | Val 0.1740 | Test 0.1697
loading full batch data spends  0.0014891624450683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453229904174805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453706741333008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09984922409057617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7200857400894165
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.7201 | Train 0.4000 | Val 0.1840 | Test 0.1572
loading full batch data spends  0.0014836788177490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044577598571777344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044582366943359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10526394844055176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7336164712905884
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.7336 | Train 0.3857 | Val 0.1760 | Test 0.1538
loading full batch data spends  0.001657724380493164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043523311614990234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043528079986572266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10132193565368652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7222330570220947
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.7222 | Train 0.3714 | Val 0.1800 | Test 0.1509
loading full batch data spends  0.0016088485717773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043283939361572266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0432887077331543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10131692886352539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7643970251083374
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.7644 | Train 0.3786 | Val 0.1780 | Test 0.1518
loading full batch data spends  0.001538991928100586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043365478515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337024688720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10302901268005371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012015342712402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7346038818359375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.7346 | Train 0.3929 | Val 0.1800 | Test 0.1634
loading full batch data spends  0.0015969276428222656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463052749633789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463529586791992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10037684440612793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7489569187164307
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7490 | Train 0.4214 | Val 0.1920 | Test 0.1707
loading full batch data spends  0.0016620159149169922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.00543212890625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341840744018555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342317581176758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09881448745727539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012047290802001953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.746065616607666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.7461 | Train 0.4143 | Val 0.1900 | Test 0.1697
loading full batch data spends  0.001855611801147461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.602836608886719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043570518493652344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043575286865234375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09999918937683105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7369297742843628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.7369 | Train 0.4143 | Val 0.1880 | Test 0.1683
loading full batch data spends  0.0015521049499511719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044638633728027344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044643402099609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10415506362915039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7473406791687012
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.7473 | Train 0.4000 | Val 0.1860 | Test 0.1659
loading full batch data spends  0.001435995101928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043367862701416016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337263107299805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10076069831848145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7211968898773193
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.7212 | Train 0.3643 | Val 0.1840 | Test 0.1610
loading full batch data spends  0.0016109943389892578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334735870361328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335212707519531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10520029067993164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01203155517578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.708946943283081
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.7089 | Train 0.3857 | Val 0.1900 | Test 0.1620
loading full batch data spends  0.0014240741729736328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339456558227539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339933395385742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09563279151916504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.702150821685791
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.7022 | Train 0.4000 | Val 0.1820 | Test 0.1702
loading full batch data spends  0.0014939308166503906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342365264892578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342842102050781  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10532331466674805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.706510305404663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.7065 | Train 0.4143 | Val 0.1880 | Test 0.1794
loading full batch data spends  0.001500844955444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341411590576172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341888427734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10548138618469238
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7132383584976196
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.7132 | Train 0.4214 | Val 0.2240 | Test 0.2157
loading full batch data spends  0.0015621185302734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044565677642822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445704460144043  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10640382766723633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7066235542297363
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.7066 | Train 0.2786 | Val 0.1120 | Test 0.1291
loading full batch data spends  0.0014331340789794922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459428787231445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044599056243896484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10042357444763184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.829100489616394
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.8291 | Train 0.3429 | Val 0.2220 | Test 0.1925
loading full batch data spends  0.0015265941619873047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352760314941406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043532371520996094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09852147102355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.702574610710144
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.7026 | Train 0.4000 | Val 0.2160 | Test 0.1958
loading full batch data spends  0.0014336109161376953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447126388549805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447603225708008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10367059707641602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6825764179229736
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.6826 | Train 0.3643 | Val 0.1760 | Test 0.1794
loading full batch data spends  0.0014846324920654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043433189392089844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043437957763671875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10403585433959961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7583365440368652
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.7583 | Train 0.3786 | Val 0.1820 | Test 0.1881
loading full batch data spends  0.0014257431030273438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043419837951660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342460632324219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09903645515441895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7387913465499878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.7388 | Train 0.3929 | Val 0.2080 | Test 0.2021
loading full batch data spends  0.0014863014221191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04437971115112305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438447952270508  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09503841400146484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.699386715888977
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.6994 | Train 0.4571 | Val 0.2440 | Test 0.2282
loading full batch data spends  0.00160980224609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043276309967041016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04328107833862305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09856271743774414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6590789556503296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.6591 | Train 0.3786 | Val 0.2280 | Test 0.2050
loading full batch data spends  0.0015187263488769531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455423355102539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455900192260742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09854841232299805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6717675924301147
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.6718 | Train 0.3643 | Val 0.1760 | Test 0.1557
loading full batch data spends  0.0014891624450683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350137710571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350614547729492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0987846851348877
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.771773099899292
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.7718 | Train 0.3786 | Val 0.2200 | Test 0.1992
loading full batch data spends  0.0015044212341308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044513702392578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044518470764160156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09849119186401367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6887235641479492
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.6887 | Train 0.4214 | Val 0.2520 | Test 0.2278
loading full batch data spends  0.0014553070068359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336690902709961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337167739868164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09882760047912598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6707229614257812
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.6707 | Train 0.3857 | Val 0.2180 | Test 0.2012
loading full batch data spends  0.0016329288482666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043521881103515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043526649475097656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11246848106384277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.707126498222351
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.7071 | Train 0.3786 | Val 0.2080 | Test 0.1886
loading full batch data spends  0.0014896392822265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043521881103515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043526649475097656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10320925712585449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.671519160270691
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.6715 | Train 0.3857 | Val 0.2100 | Test 0.1862
loading full batch data spends  0.001527547836303711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348564147949219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349040985107422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10079193115234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7228566408157349
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.7229 | Train 0.3786 | Val 0.2120 | Test 0.1992
loading full batch data spends  0.0015153884887695312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438161849975586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438638687133789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10592818260192871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6715178489685059
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.6715 | Train 0.4071 | Val 0.2340 | Test 0.2239
loading full batch data spends  0.0015528202056884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459571838378906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044600486755371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10322809219360352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013074398040771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6765542030334473
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.6766 | Train 0.3929 | Val 0.2280 | Test 0.2234
loading full batch data spends  0.0014595985412597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044686317443847656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04469108581542969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09893178939819336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012226581573486328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6886775493621826
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.6887 | Train 0.3929 | Val 0.2300 | Test 0.2224
loading full batch data spends  0.0014731884002685547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331636428833008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332113265991211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09599542617797852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012047290802001953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6872501373291016
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.6873 | Train 0.4000 | Val 0.2000 | Test 0.2065
loading full batch data spends  0.0016067028045654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043537139892578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043541908264160156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10138654708862305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7163151502609253
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.7163 | Train 0.3714 | Val 0.2180 | Test 0.2016
loading full batch data spends  0.001474618911743164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043375492095947266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433802604675293  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09880948066711426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01201009750366211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.669284701347351
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.6693 | Train 0.4571 | Val 0.2360 | Test 0.2297
loading full batch data spends  0.001443624496459961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043511390686035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351615905761719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09958815574645996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6696912050247192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.6697 | Train 0.4571 | Val 0.2560 | Test 0.2365
loading full batch data spends  0.0015015602111816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337263107299805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337739944458008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10838699340820312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6542787551879883
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.6543 | Train 0.4500 | Val 0.2420 | Test 0.2321
loading full batch data spends  0.0014231204986572266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331064224243164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331541061401367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10464072227478027
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01222372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6465766429901123
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.6466 | Train 0.4714 | Val 0.2380 | Test 0.2195
loading full batch data spends  0.0014834403991699219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044550418853759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445551872253418  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10544800758361816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013071537017822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.67128586769104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.6713 | Train 0.4286 | Val 0.2240 | Test 0.2041
Total (block generation + training)time/epoch 0.11449378132820129
pure train time/epoch 0.1040070543483812

num_input_list  [1342, 1367, 1347, 1359, 1348, 1366, 1368, 1366, 1349, 1361, 1357, 1358, 1365, 1368, 1384, 1357, 1358, 1357, 1369, 1358, 1364, 1348, 1358, 1350, 1348, 1364, 1361, 1358, 1368, 1373, 1357, 1371, 1355, 1347, 1351, 1354, 1369, 1368, 1381, 1358, 1358, 1366, 1358, 1360, 1365, 1366, 1350, 1365, 1367, 1347, 1356, 1349, 1374, 1357, 1359, 1365, 1371, 1352, 1359, 1354, 1367, 1356, 1369, 1352, 1364, 1352, 1362, 1359, 1358, 1361, 1346, 1362, 1370, 1364, 1368, 1359, 1359, 1361, 1351, 1366, 1363, 1354, 1365, 1351, 1373, 1371, 1352, 1348, 1370, 1368, 1343, 1357, 1352, 1368, 1354, 1356, 1363, 1375, 1352, 1364, 1352, 1357, 1353, 1367, 1355, 1346, 1364, 1358, 1374, 1370, 1356, 1370, 1367, 1364, 1361, 1363, 1361, 1371, 1368, 1353, 1352, 1354, 1349, 1363, 1363, 1359, 1360, 1350, 1346, 1363, 1368, 1367, 1355, 1363, 1367, 1366, 1356, 1356, 1363, 1360, 1357, 1354, 1364, 1357, 1367, 1354, 1374, 1362, 1357, 1360, 1352, 1354, 1361, 1359, 1360, 1356, 1346, 1369, 1361, 1354, 1347, 1362, 1371, 1353, 1347, 1345, 1366, 1351, 1346, 1368, 1341, 1348, 1359, 1357, 1357, 1367, 1371, 1357, 1365, 1355, 1358, 1366, 1359, 1364, 1359, 1365, 1357, 1354, 1356, 1358, 1362, 1379, 1360, 1351, 1355, 1344, 1357, 1354, 1350, 1362]
