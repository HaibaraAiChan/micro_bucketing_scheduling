main start at this time 1696112594.0918262
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.002942323684692383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.027371883392333984  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.027376651763916016  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

pure train time  0.5297915935516357
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.560546875 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9514591693878174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9515 | Train 0.1429 | Val 0.3000 | Test 0.3017
loading full batch data spends  0.001928567886352539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02871084213256836  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02871561050415039  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

pure train time  0.12271785736083984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9480329751968384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9480 | Train 0.1357 | Val 0.2220 | Test 0.2355
loading full batch data spends  0.001981496810913086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028789043426513672  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028793811798095703  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

pure train time  0.1202230453491211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9489327669143677
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9489 | Train 0.1357 | Val 0.1320 | Test 0.1281
loading full batch data spends  0.001971006393432617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028819561004638672  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028824329376220703  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.12133121490478516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9449717998504639
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9450 | Train 0.1786 | Val 0.1160 | Test 0.0957
loading full batch data spends  0.0019257068634033203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886056900024414  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.11961746215820312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9445648193359375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9446 | Train 0.1929 | Val 0.1200 | Test 0.0977
loading full batch data spends  0.001943349838256836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809547424316406  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028814315795898438  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.11687207221984863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9444730281829834
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9445 | Train 0.1857 | Val 0.1140 | Test 0.0977
loading full batch data spends  0.001958608627319336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029912948608398438  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991771697998047  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.11771202087402344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9456636905670166
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9457 | Train 0.1929 | Val 0.1140 | Test 0.0957
loading full batch data spends  0.0019843578338623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028905868530273438  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891063690185547  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

pure train time  0.11201310157775879
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9424830675125122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9425 | Train 0.2000 | Val 0.1140 | Test 0.0953
loading full batch data spends  0.0019605159759521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02996826171875  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02997303009033203  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

pure train time  0.11201167106628418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.936394214630127
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9364 | Train 0.1857 | Val 0.1140 | Test 0.0933
loading full batch data spends  0.001989126205444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922557830810547  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028927326202392578  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.10557889938354492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9391765594482422
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9392 | Train 0.1714 | Val 0.1020 | Test 0.0899
loading full batch data spends  0.0019369125366210938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028779983520507812  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028784751892089844  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.10848546028137207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9359418153762817
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9359 | Train 0.1714 | Val 0.1020 | Test 0.0885
loading full batch data spends  0.0019795894622802734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888965606689453  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894424438476562  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.1022789478302002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.935853123664856
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9359 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.001931905746459961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029868125915527344  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029872894287109375  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11665773391723633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.933876633644104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9339 | Train 0.1714 | Val 0.1100 | Test 0.0914
loading full batch data spends  0.001970529556274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028825759887695312  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028830528259277344  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11424446105957031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9243148565292358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9243 | Train 0.1714 | Val 0.1200 | Test 0.0972
loading full batch data spends  0.0019435882568359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898406982421875  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898883819580078  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11677098274230957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9270368814468384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9270 | Train 0.1857 | Val 0.1200 | Test 0.1035
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989339828491211  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989816665649414  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11947488784790039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9258078336715698
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9258 | Train 0.1929 | Val 0.1140 | Test 0.1098
loading full batch data spends  0.0019521713256835938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028802871704101562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11124062538146973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9355741739273071
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9356 | Train 0.1714 | Val 0.1140 | Test 0.1170
loading full batch data spends  0.001984834671020508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028886795043945312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028891563415527344  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11824440956115723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.919065237045288
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9191 | Train 0.1714 | Val 0.1180 | Test 0.1190
loading full batch data spends  0.001941680908203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878284454345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028787612915039062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11867904663085938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.932547688484192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9325 | Train 0.1786 | Val 0.1200 | Test 0.1199
loading full batch data spends  0.0019969940185546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994394302368164  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029948711395263672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12287545204162598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9048391580581665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9048 | Train 0.1857 | Val 0.1180 | Test 0.1267
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028892040252685547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12142372131347656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9328999519348145
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9329 | Train 0.2071 | Val 0.1240 | Test 0.1190
loading full batch data spends  0.001990079879760742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028765201568603516  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028769969940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12065744400024414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9063441753387451
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9063 | Train 0.2286 | Val 0.1280 | Test 0.1165
loading full batch data spends  0.001947164535522461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02897930145263672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898406982421875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12276625633239746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9077383279800415
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9077 | Train 0.2071 | Val 0.1300 | Test 0.1146
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028852462768554688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12243413925170898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8988981246948242
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.8989 | Train 0.2000 | Val 0.1300 | Test 0.1141
loading full batch data spends  0.001926422119140625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028725624084472656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028730392456054688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12400507926940918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.90152108669281
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9015 | Train 0.1929 | Val 0.1320 | Test 0.1132
loading full batch data spends  0.0019989013671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891063690185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289154052734375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12143325805664062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9118000268936157
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9118 | Train 0.1929 | Val 0.1320 | Test 0.1103
loading full batch data spends  0.0019538402557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029935359954833984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029940128326416016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12468719482421875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9062052965164185
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9062 | Train 0.2071 | Val 0.1260 | Test 0.1127
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028842926025390625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10967397689819336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8948966264724731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.8949 | Train 0.1929 | Val 0.1260 | Test 0.1107
loading full batch data spends  0.0019245147705078125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892446517944336  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892923355102539  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11438608169555664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.900148630142212
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9001 | Train 0.1929 | Val 0.1260 | Test 0.1103
loading full batch data spends  0.0020453929901123047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892303466796875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10926151275634766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9009928703308105
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9010 | Train 0.1929 | Val 0.1300 | Test 0.1161
loading full batch data spends  0.0019855499267578125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888774871826172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889251708984375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10915851593017578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.908573865890503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9086 | Train 0.1929 | Val 0.1300 | Test 0.1161
loading full batch data spends  0.00197601318359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029902935028076172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029907703399658203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10658669471740723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.898650884628296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.8987 | Train 0.2071 | Val 0.1260 | Test 0.1175
loading full batch data spends  0.0019450187683105469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028878211975097656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10857439041137695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8837350606918335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.8837 | Train 0.2286 | Val 0.1220 | Test 0.1107
loading full batch data spends  0.001976490020751953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908252716064453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913021087646484  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11075258255004883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896426796913147
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.8964 | Train 0.2143 | Val 0.1260 | Test 0.1059
loading full batch data spends  0.0019335746765136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895498275756836  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895975112915039  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1198275089263916
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8869833946228027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8870 | Train 0.2143 | Val 0.1300 | Test 0.1069
loading full batch data spends  0.0020751953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891683578491211  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892160415649414  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11088681221008301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8623912334442139
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8624 | Train 0.2429 | Val 0.1300 | Test 0.1151
loading full batch data spends  0.0020606517791748047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.695487976074219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867721557617188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12024855613708496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9135994911193848
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9136 | Train 0.2500 | Val 0.1320 | Test 0.1190
loading full batch data spends  0.0020003318786621094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881622314453125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882099151611328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10057497024536133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00917673110961914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8942557573318481
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8943 | Train 0.2643 | Val 0.1400 | Test 0.1214
loading full batch data spends  0.0019505023956298828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028727054595947266  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028731822967529297  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09985065460205078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.870881199836731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8709 | Train 0.2429 | Val 0.1380 | Test 0.1257
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028783798217773438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878856658935547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1041109561920166
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8889648914337158
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8890 | Train 0.2643 | Val 0.1300 | Test 0.1248
loading full batch data spends  0.0019321441650390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10040998458862305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8907817602157593
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.8908 | Train 0.2714 | Val 0.1260 | Test 0.1228
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886056900024414  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865337371826172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10291457176208496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8914387226104736
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8914 | Train 0.2643 | Val 0.1300 | Test 0.1257
loading full batch data spends  0.0021598339080810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891397476196289  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028918743133544922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1075899600982666
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9038604497909546
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.9039 | Train 0.2357 | Val 0.1300 | Test 0.1238
loading full batch data spends  0.0019953250885009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994251251220703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029947280883789062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1060476303100586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8966022729873657
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.8966 | Train 0.2357 | Val 0.1360 | Test 0.1136
loading full batch data spends  0.0019583702087402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028860092163085938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886486053466797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10665392875671387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8778352737426758
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8778 | Train 0.2357 | Val 0.1320 | Test 0.1136
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09981155395507812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9013887643814087
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.9014 | Train 0.2643 | Val 0.1480 | Test 0.1161
loading full batch data spends  0.0019383430480957031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884817123413086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885293960571289  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09812283515930176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9159632921218872
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.9160 | Train 0.2429 | Val 0.1440 | Test 0.1262
loading full batch data spends  0.001985311508178711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809070587158203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028813838958740234  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11037588119506836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009171485900878906  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.903079867362976
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.9031 | Train 0.2357 | Val 0.1440 | Test 0.1267
loading full batch data spends  0.0019333362579345703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09632301330566406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8865282535552979
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8865 | Train 0.2357 | Val 0.1440 | Test 0.1301
loading full batch data spends  0.0019843578338623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029945850372314453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029950618743896484  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10219764709472656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8627192974090576
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8627 | Train 0.2571 | Val 0.1460 | Test 0.1281
loading full batch data spends  0.001978635787963867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028932571411132812  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028937339782714844  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10712909698486328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8875468969345093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8875 | Train 0.2500 | Val 0.1360 | Test 0.1199
loading full batch data spends  0.002007007598876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029940128326416016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029944896697998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10252571105957031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8977844715118408
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8978 | Train 0.2429 | Val 0.1340 | Test 0.1161
loading full batch data spends  0.0019335746765136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851032257080078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10434269905090332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8711282014846802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8711 | Train 0.2500 | Val 0.1340 | Test 0.1214
loading full batch data spends  0.002004861831665039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889108657836914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028895854949951172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1048281192779541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.887689471244812
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8877 | Train 0.2643 | Val 0.1300 | Test 0.1267
loading full batch data spends  0.0019526481628417969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028954029083251953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028958797454833984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09795713424682617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8891710042953491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8892 | Train 0.2714 | Val 0.1320 | Test 0.1272
loading full batch data spends  0.001974344253540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028871536254882812  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876304626464844  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09952640533447266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8961329460144043
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8961 | Train 0.2714 | Val 0.1300 | Test 0.1252
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028994083404541016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028998851776123047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10551834106445312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8792835474014282
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8793 | Train 0.2714 | Val 0.1260 | Test 0.1209
loading full batch data spends  0.001972675323486328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02894735336303711  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895212173461914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10247063636779785
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.895725965499878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8957 | Train 0.2500 | Val 0.1240 | Test 0.1219
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029947280883789062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029952049255371094  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09927654266357422
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8696086406707764
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8696 | Train 0.2643 | Val 0.1160 | Test 0.1252
loading full batch data spends  0.002001047134399414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915809631347656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029920578002929688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10591602325439453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8736249208450317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8736 | Train 0.2786 | Val 0.1040 | Test 0.1175
loading full batch data spends  0.001935720443725586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028828144073486328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883291244506836  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10344052314758301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.875145435333252
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8751 | Train 0.2786 | Val 0.1000 | Test 0.1088
loading full batch data spends  0.0019221305847167969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028840065002441406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028844833374023438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10026001930236816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8887895345687866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8888 | Train 0.3000 | Val 0.1040 | Test 0.1127
loading full batch data spends  0.0019278526306152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028863906860351562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10595393180847168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896431565284729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8964 | Train 0.3000 | Val 0.1260 | Test 0.1281
loading full batch data spends  0.0019817352294921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028862476348876953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867244720458984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10258984565734863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.849827766418457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8498 | Train 0.2643 | Val 0.1280 | Test 0.1359
loading full batch data spends  0.0018870830535888672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029963016510009766  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029967784881591797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10405254364013672
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.875855565071106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8759 | Train 0.2714 | Val 0.1360 | Test 0.1310
loading full batch data spends  0.0019791126251220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028832435607910156  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028837203979492188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10205888748168945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8692387342453003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8692 | Train 0.2714 | Val 0.1360 | Test 0.1320
loading full batch data spends  0.0019745826721191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028808116912841797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812885284423828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1008141040802002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8600877523422241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8601 | Train 0.2857 | Val 0.1360 | Test 0.1315
loading full batch data spends  0.001995563507080078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928756713867188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893352508544922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10414791107177734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00922250747680664  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8739726543426514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8740 | Train 0.2929 | Val 0.1360 | Test 0.1315
loading full batch data spends  0.0019528865814208984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827190399169922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028831958770751953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10251927375793457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8617926836013794
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8618 | Train 0.2786 | Val 0.1320 | Test 0.1344
loading full batch data spends  0.001998424530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029882431030273438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02988719940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10067081451416016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8749784231185913
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8750 | Train 0.2714 | Val 0.1280 | Test 0.1354
loading full batch data spends  0.001941680908203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028802871704101562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0992586612701416
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8693560361862183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8694 | Train 0.2857 | Val 0.1340 | Test 0.1339
loading full batch data spends  0.0019369125366210938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028900146484375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890491485595703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09564566612243652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0092010498046875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8729987144470215
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8730 | Train 0.2643 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0018820762634277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10125374794006348
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8468230962753296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8468 | Train 0.2643 | Val 0.1440 | Test 0.1310
loading full batch data spends  0.0020148754119873047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893543243408203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028940200805664062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10321974754333496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8492405414581299
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8492 | Train 0.2714 | Val 0.1400 | Test 0.1301
loading full batch data spends  0.0019578933715820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890634536743164  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028911113739013672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10771846771240234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8257081508636475
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8257 | Train 0.2786 | Val 0.1300 | Test 0.1252
loading full batch data spends  0.001974821090698242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028795242309570312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028800010681152344  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11474275588989258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009198188781738281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8692816495895386
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8693 | Train 0.2786 | Val 0.1300 | Test 0.1306
loading full batch data spends  0.0019905567169189453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.00543212890625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028907299041748047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028912067413330078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10612154006958008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.835121989250183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8351 | Train 0.2929 | Val 0.1260 | Test 0.1330
loading full batch data spends  0.0020051002502441406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028917312622070312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09883427619934082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8423446416854858
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8423 | Train 0.3000 | Val 0.1260 | Test 0.1325
loading full batch data spends  0.0020411014556884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886962890625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887439727783203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09508490562438965
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.845557689666748
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8456 | Train 0.2929 | Val 0.1320 | Test 0.1354
loading full batch data spends  0.0020644664764404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028968334197998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028973102569580078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10088491439819336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009249210357666016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8229293823242188
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8229 | Train 0.3000 | Val 0.1220 | Test 0.1349
loading full batch data spends  0.00194549560546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028883934020996094  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10024237632751465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8424129486083984
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8424 | Train 0.2929 | Val 0.1180 | Test 0.1397
loading full batch data spends  0.0019965171813964844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028982162475585938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898693084716797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1019582748413086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00927591323852539  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.829278826713562
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8293 | Train 0.3071 | Val 0.1300 | Test 0.1335
loading full batch data spends  0.0019676685333251953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881908416748047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0288238525390625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10442876815795898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8185945749282837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8186 | Train 0.3214 | Val 0.1220 | Test 0.1407
loading full batch data spends  0.0019991397857666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029882431030273438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02988719940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09911322593688965
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8039209842681885
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8039 | Train 0.3286 | Val 0.1460 | Test 0.1335
loading full batch data spends  0.001932382583618164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873920440673828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09907865524291992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8338418006896973
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8338 | Train 0.3429 | Val 0.1420 | Test 0.1335
loading full batch data spends  0.001973390579223633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029910564422607422  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915332794189453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1058657169342041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.78671133518219
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.7867 | Train 0.3500 | Val 0.1560 | Test 0.1422
loading full batch data spends  0.0019485950469970703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881622314453125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882099151611328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09846329689025879
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7971242666244507
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.7971 | Train 0.3286 | Val 0.1660 | Test 0.1557
loading full batch data spends  0.0019893646240234375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028904438018798828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890920639038086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10466265678405762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009251594543457031  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7957136631011963
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.7957 | Train 0.3357 | Val 0.1660 | Test 0.1547
loading full batch data spends  0.0019404888153076172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028765201568603516  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028769969940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10531330108642578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.803650975227356
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8037 | Train 0.3214 | Val 0.1500 | Test 0.1494
loading full batch data spends  0.0020546913146972656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02984905242919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02985382080078125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10179543495178223
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8177106380462646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8177 | Train 0.3500 | Val 0.1740 | Test 0.1485
loading full batch data spends  0.001928567886352539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028930187225341797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028934955596923828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10531949996948242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7735748291015625
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.7736 | Train 0.3500 | Val 0.1580 | Test 0.1417
loading full batch data spends  0.0019989013671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029938697814941406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029943466186523438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10206294059753418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7737176418304443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.7737 | Train 0.3571 | Val 0.1480 | Test 0.1397
loading full batch data spends  0.0019483566284179688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887725830078125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10428810119628906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8096063137054443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8096 | Train 0.3500 | Val 0.1560 | Test 0.1436
loading full batch data spends  0.0019996166229248047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892303466796875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1089482307434082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8054730892181396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8055 | Train 0.3214 | Val 0.1620 | Test 0.1460
loading full batch data spends  0.0019388198852539062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10076236724853516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.758834719657898
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.7588 | Train 0.2714 | Val 0.1460 | Test 0.1388
loading full batch data spends  0.001994609832763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030003070831298828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.03000783920288086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10393404960632324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8338433504104614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8338 | Train 0.3429 | Val 0.1620 | Test 0.1470
loading full batch data spends  0.0019402503967285156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882980346679688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888774871826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09992456436157227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7677662372589111
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.7678 | Train 0.3786 | Val 0.1700 | Test 0.1426
loading full batch data spends  0.002010822296142578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886819839477539  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028872966766357422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1073141098022461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7588863372802734
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.7589 | Train 0.3429 | Val 0.1620 | Test 0.1402
loading full batch data spends  0.001959562301635742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877664566040039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028781414031982422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10031723976135254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7280462980270386
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.7280 | Train 0.3500 | Val 0.1560 | Test 0.1378
loading full batch data spends  0.0019812583923339844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884387969970703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028848648071289062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10660457611083984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8390474319458008
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8390 | Train 0.3643 | Val 0.1620 | Test 0.1494
loading full batch data spends  0.0019555091857910156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890609741210938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889537811279297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10082077980041504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7702662944793701
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.7703 | Train 0.3857 | Val 0.1820 | Test 0.1557
loading full batch data spends  0.0019788742065429688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028815746307373047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028820514678955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10446882247924805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7594424486160278
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.7594 | Train 0.3857 | Val 0.1740 | Test 0.1673
loading full batch data spends  0.0019457340240478516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992868423461914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029933452606201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10175609588623047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7744519710540771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.7745 | Train 0.3714 | Val 0.1800 | Test 0.1659
loading full batch data spends  0.001989126205444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992534637451172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993011474609375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09683656692504883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.764054775238037
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.7641 | Train 0.3429 | Val 0.1820 | Test 0.1634
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028942108154296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028946876525878906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11250519752502441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7511576414108276
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.7512 | Train 0.3786 | Val 0.1820 | Test 0.1683
loading full batch data spends  0.0020008087158203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028892040252685547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10555648803710938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7480570077896118
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.7481 | Train 0.3429 | Val 0.1820 | Test 0.1746
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887582778930664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11215567588806152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7507737874984741
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.7508 | Train 0.3357 | Val 0.1700 | Test 0.1678
loading full batch data spends  0.001519918441772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.574920654296875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991962432861328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924392700195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1004335880279541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7703828811645508
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.7704 | Train 0.3357 | Val 0.1680 | Test 0.1659
loading full batch data spends  0.0015039443969726562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029910564422607422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915332794189453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09894895553588867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7449413537979126
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.7449 | Train 0.3143 | Val 0.1700 | Test 0.1659
loading full batch data spends  0.001476287841796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028835773468017578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884054183959961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09898900985717773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7384856939315796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.7385 | Train 0.3714 | Val 0.1800 | Test 0.1721
loading full batch data spends  0.0014491081237792969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028874874114990234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09822416305541992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.737897515296936
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.7379 | Train 0.3857 | Val 0.2120 | Test 0.1876
loading full batch data spends  0.0015201568603515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028902530670166016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028907299041748047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10278844833374023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7346775531768799
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.7347 | Train 0.3929 | Val 0.2120 | Test 0.1939
loading full batch data spends  0.0014357566833496094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029930591583251953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029935359954833984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10571002960205078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7005236148834229
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.7005 | Train 0.3500 | Val 0.1900 | Test 0.1779
loading full batch data spends  0.0015249252319335938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879642486572266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028884410858154297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10393381118774414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7807666063308716
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.7808 | Train 0.4000 | Val 0.2060 | Test 0.1978
loading full batch data spends  0.001447439193725586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894424438476562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028899192810058594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10492801666259766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7616593837738037
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.7617 | Train 0.4071 | Val 0.2160 | Test 0.2045
loading full batch data spends  0.001459360122680664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029868125915527344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029872894287109375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1007695198059082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7562979459762573
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.7563 | Train 0.3857 | Val 0.1940 | Test 0.1828
loading full batch data spends  0.0017337799072265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.7206878662109375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10448527336120605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.720542311668396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.7205 | Train 0.4000 | Val 0.1960 | Test 0.1712
loading full batch data spends  0.001501321792602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924392700195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029929161071777344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10423636436462402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6968084573745728
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.6968 | Train 0.3786 | Val 0.1820 | Test 0.1625
loading full batch data spends  0.001482248306274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028825759887695312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028830528259277344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10227513313293457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7750931978225708
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.7751 | Train 0.3786 | Val 0.1920 | Test 0.1721
loading full batch data spends  0.0014617443084716797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10232830047607422
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7690761089324951
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.7691 | Train 0.3929 | Val 0.2120 | Test 0.1838
loading full batch data spends  0.0015020370483398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890157699584961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09918808937072754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7016878128051758
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.7017 | Train 0.4071 | Val 0.2120 | Test 0.2026
loading full batch data spends  0.0015120506286621094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.743171691894531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847217559814453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851985931396484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09726786613464355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7019176483154297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.7019 | Train 0.3643 | Val 0.2020 | Test 0.1886
loading full batch data spends  0.0014767646789550781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028698444366455078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02870321273803711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1025850772857666
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.760772705078125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.7608 | Train 0.2571 | Val 0.1560 | Test 0.1494
loading full batch data spends  0.0015406608581542969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02995920181274414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029963970184326172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1038815975189209
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8208166360855103
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8208 | Train 0.3571 | Val 0.2000 | Test 0.1915
loading full batch data spends  0.001476287841796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029821395874023438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02982616424560547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10355758666992188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7509000301361084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.7509 | Train 0.4000 | Val 0.2080 | Test 0.2074
loading full batch data spends  0.0014753341674804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028829097747802734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028833866119384766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10454154014587402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.695979118347168
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.6960 | Train 0.3786 | Val 0.2180 | Test 0.1809
loading full batch data spends  0.0014252662658691406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10425686836242676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7144641876220703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.7145 | Train 0.3571 | Val 0.1960 | Test 0.1789
loading full batch data spends  0.0015149116516113281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890157699584961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890634536743164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09697151184082031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7546312808990479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.7546 | Train 0.3500 | Val 0.2000 | Test 0.1765
loading full batch data spends  0.0014264583587646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1007535457611084
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7845039367675781
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.7845 | Train 0.3571 | Val 0.2000 | Test 0.1750
loading full batch data spends  0.001482248306274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1029059886932373
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7630610466003418
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.7631 | Train 0.3929 | Val 0.2080 | Test 0.1857
loading full batch data spends  0.001425027847290039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028931140899658203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028935909271240234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10320019721984863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7630914449691772
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.7631 | Train 0.4071 | Val 0.2260 | Test 0.1929
loading full batch data spends  0.0015091896057128906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028822898864746094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827667236328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09711980819702148
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7130213975906372
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.7130 | Train 0.4214 | Val 0.2260 | Test 0.2162
loading full batch data spends  0.0014319419860839844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827667236328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028832435607910156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09827494621276855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7251245975494385
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.7251 | Train 0.4143 | Val 0.2300 | Test 0.2220
loading full batch data spends  0.001520395278930664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879476547241211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879953384399414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09728789329528809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7271524667739868
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.7272 | Train 0.4000 | Val 0.2100 | Test 0.2089
loading full batch data spends  0.0014438629150390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09768915176391602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7263083457946777
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.7263 | Train 0.3571 | Val 0.2000 | Test 0.1871
loading full batch data spends  0.0014717578887939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899789810180664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029002666473388672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09345197677612305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009281158447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7447614669799805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7448 | Train 0.3786 | Val 0.1980 | Test 0.2060
loading full batch data spends  0.0015985965728759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028742313385009766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028747081756591797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09366559982299805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7336971759796143
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.7337 | Train 0.3857 | Val 0.2300 | Test 0.2045
loading full batch data spends  0.0015172958374023438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994060516357422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994537353515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09635305404663086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7163194417953491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7163 | Train 0.4071 | Val 0.2160 | Test 0.1881
loading full batch data spends  0.0014407634735107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028975486755371094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028980255126953125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10155558586120605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6960757970809937
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.6961 | Train 0.3786 | Val 0.1960 | Test 0.1852
loading full batch data spends  0.001459360122680664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028855323791503906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028860092163085938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1034393310546875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7665326595306396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.7665 | Train 0.3929 | Val 0.2080 | Test 0.1886
loading full batch data spends  0.0016269683837890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029955387115478516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029960155487060547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10204935073852539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.738595962524414
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.7386 | Train 0.3929 | Val 0.2060 | Test 0.1862
loading full batch data spends  0.0014727115631103516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10178303718566895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7709498405456543
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7709 | Train 0.4071 | Val 0.2140 | Test 0.1929
loading full batch data spends  0.0014200210571289062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029962539672851562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029967308044433594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10146737098693848
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7539085149765015
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.7539 | Train 0.4286 | Val 0.2180 | Test 0.2026
loading full batch data spends  0.0014824867248535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028874874114990234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10012531280517578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7112160921096802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.7112 | Train 0.4000 | Val 0.2160 | Test 0.2132
loading full batch data spends  0.0014421939849853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028853416442871094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028858184814453125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09758329391479492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7380053997039795
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7380 | Train 0.4071 | Val 0.2220 | Test 0.2249
loading full batch data spends  0.001491546630859375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029900550842285156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029905319213867188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10413217544555664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7378672361373901
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.7379 | Train 0.4071 | Val 0.2220 | Test 0.2253
loading full batch data spends  0.0014464855194091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882671356201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883148193359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10490226745605469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7486690282821655
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.7487 | Train 0.4286 | Val 0.2280 | Test 0.2278
loading full batch data spends  0.0014934539794921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029860496520996094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029865264892578125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0987706184387207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6813805103302002
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.6814 | Train 0.4214 | Val 0.2240 | Test 0.2210
loading full batch data spends  0.0014259815216064453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028975963592529297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028980731964111328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09830331802368164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6902856826782227
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.6903 | Train 0.4357 | Val 0.2220 | Test 0.2137
loading full batch data spends  0.0014984607696533203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028859615325927734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0959470272064209
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7015326023101807
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7015 | Train 0.4571 | Val 0.2280 | Test 0.2162
loading full batch data spends  0.0014827251434326172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02875518798828125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02875995635986328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10570096969604492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.696135401725769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.6961 | Train 0.4500 | Val 0.2280 | Test 0.2089
loading full batch data spends  0.0015234947204589844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883148193359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883625030517578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10280370712280273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7089391946792603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.7089 | Train 0.4500 | Val 0.2280 | Test 0.2074
loading full batch data spends  0.0015218257904052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895212173461914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028956890106201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10130620002746582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7219061851501465
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.7219 | Train 0.4571 | Val 0.2120 | Test 0.2079
loading full batch data spends  0.0020987987518310547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.1975250244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890777587890625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1014394760131836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7414371967315674
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.7414 | Train 0.4714 | Val 0.2280 | Test 0.2195
loading full batch data spends  0.0014789104461669922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891683578491211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892160415649414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09549212455749512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7083059549331665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.7083 | Train 0.5071 | Val 0.2460 | Test 0.2345
loading full batch data spends  0.0014605522155761719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028793811798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028798580169677734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09366607666015625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.690354585647583
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.6904 | Train 0.5286 | Val 0.2660 | Test 0.2505
loading full batch data spends  0.0014107227325439453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993917465209961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994394302368164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10013341903686523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7024753093719482
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.7025 | Train 0.5429 | Val 0.2600 | Test 0.2553
loading full batch data spends  0.0014908313751220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867721557617188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09587335586547852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7265666723251343
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.7266 | Train 0.5071 | Val 0.2580 | Test 0.2548
loading full batch data spends  0.0014202594757080078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028972625732421875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028977394104003906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10402083396911621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7139997482299805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.7140 | Train 0.4786 | Val 0.2480 | Test 0.2539
loading full batch data spends  0.0015325546264648438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028883934020996094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09601974487304688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6994026899337769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.6994 | Train 0.4643 | Val 0.2300 | Test 0.2447
loading full batch data spends  0.0014133453369140625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028954029083251953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028958797454833984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09322834014892578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.656182050704956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.6562 | Train 0.4429 | Val 0.2200 | Test 0.2379
loading full batch data spends  0.0015070438385009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029897689819335938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990245819091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10348153114318848
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6399480104446411
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.6399 | Train 0.4286 | Val 0.2500 | Test 0.2481
loading full batch data spends  0.0014448165893554688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028824329376220703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028829097747802734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09974813461303711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6872050762176514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.6872 | Train 0.4071 | Val 0.2740 | Test 0.2669
loading full batch data spends  0.0015087127685546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028877735137939453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882503509521484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1020212173461914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6776256561279297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.6776 | Train 0.4071 | Val 0.2640 | Test 0.2650
loading full batch data spends  0.0014193058013916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879476547241211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879953384399414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0986325740814209
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6801539659500122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.6802 | Train 0.4429 | Val 0.2800 | Test 0.2737
loading full batch data spends  0.0015270709991455078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028923511505126953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10325956344604492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6404833793640137
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.6405 | Train 0.5000 | Val 0.2860 | Test 0.2853
loading full batch data spends  0.0014955997467041016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028897762298583984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028902530670166016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1017003059387207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7010693550109863
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7011 | Train 0.4857 | Val 0.2880 | Test 0.2800
loading full batch data spends  0.0014870166778564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029956817626953125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029961585998535156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10347175598144531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6688976287841797
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.6689 | Train 0.4643 | Val 0.2820 | Test 0.2708
loading full batch data spends  0.001451253890991211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932498931884766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029937267303466797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1037602424621582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.674656629562378
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.6747 | Train 0.4643 | Val 0.3040 | Test 0.2742
loading full batch data spends  0.0015246868133544922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028863906860351562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09948873519897461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6646161079406738
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.6646 | Train 0.4500 | Val 0.2840 | Test 0.2747
loading full batch data spends  0.0014197826385498047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890132904052734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09594440460205078
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6263314485549927
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.6263 | Train 0.4357 | Val 0.2800 | Test 0.2640
loading full batch data spends  0.001512289047241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028889179229736328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889394760131836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09667372703552246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6540660858154297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.6541 | Train 0.4500 | Val 0.2800 | Test 0.2568
loading full batch data spends  0.0014529228210449219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0950322151184082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6061370372772217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.6061 | Train 0.4500 | Val 0.2800 | Test 0.2606
loading full batch data spends  0.0017039775848388672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865814208984375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870582580566406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09379458427429199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6491283178329468
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.6491 | Train 0.4857 | Val 0.2840 | Test 0.2597
loading full batch data spends  0.0014815330505371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876424789428711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876901626586914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09147810935974121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6801989078521729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.6802 | Train 0.4571 | Val 0.2840 | Test 0.2660
loading full batch data spends  0.0015125274658203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029936790466308594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029941558837890625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09446525573730469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6670364141464233
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.6670 | Train 0.4571 | Val 0.2960 | Test 0.2718
loading full batch data spends  0.0014328956604003906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993488311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029939651489257812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09649825096130371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6456063985824585
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.6456 | Train 0.4857 | Val 0.2820 | Test 0.2858
loading full batch data spends  0.0014758110046386719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029950618743896484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029955387115478516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10213613510131836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6595045328140259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.6595 | Train 0.4643 | Val 0.3080 | Test 0.2945
loading full batch data spends  0.0014264583587646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029901981353759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029906749725341797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0974578857421875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6254693269729614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.6255 | Train 0.5000 | Val 0.3040 | Test 0.3017
loading full batch data spends  0.0015246868133544922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028871536254882812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876304626464844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10196757316589355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6455408334732056
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.6455 | Train 0.5000 | Val 0.2860 | Test 0.2959
loading full batch data spends  0.0014274120330810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028933048248291016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09539937973022461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.645879864692688
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.6459 | Train 0.5071 | Val 0.3020 | Test 0.3095
loading full batch data spends  0.0014729499816894531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028774738311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877950668334961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1031334400177002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6381101608276367
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.6381 | Train 0.4214 | Val 0.2700 | Test 0.2911
loading full batch data spends  0.0014286041259765625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028858661651611328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886343002319336  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1003108024597168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6277326345443726
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.6277 | Train 0.4214 | Val 0.2600 | Test 0.2829
loading full batch data spends  0.0014977455139160156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922080993652344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028926849365234375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09590816497802734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923299789428711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5927530527114868
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.5928 | Train 0.3714 | Val 0.2520 | Test 0.2403
loading full batch data spends  0.001703023910522461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992725372314453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932022094726562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10322904586791992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6762608289718628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.6763 | Train 0.3714 | Val 0.2560 | Test 0.2427
loading full batch data spends  0.001474142074584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899456024169922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899932861328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1047673225402832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009259700775146484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6616902351379395
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.6617 | Train 0.4286 | Val 0.2900 | Test 0.2945
loading full batch data spends  0.0015058517456054688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812408447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10173916816711426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6499933004379272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.6500 | Train 0.4214 | Val 0.2780 | Test 0.2703
loading full batch data spends  0.0016164779663085938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.695487976074219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028859615325927734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09344100952148438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6770025491714478
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.6770 | Train 0.4143 | Val 0.2820 | Test 0.2689
loading full batch data spends  0.001493215560913086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10273289680480957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6733310222625732
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.6733 | Train 0.4786 | Val 0.2940 | Test 0.3022
loading full batch data spends  0.0015001296997070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865337371826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0959465503692627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6725414991378784
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.6725 | Train 0.4571 | Val 0.2980 | Test 0.3017
loading full batch data spends  0.0014257431030273438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886199951171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886676788330078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10472226142883301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.613718032836914
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.6137 | Train 0.4643 | Val 0.2500 | Test 0.2742
loading full batch data spends  0.0014672279357910156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851032257080078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10431957244873047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.663222074508667
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.6632 | Train 0.4286 | Val 0.2560 | Test 0.2679
loading full batch data spends  0.0014727115631103516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893686294555664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028941631317138672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09680390357971191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6590068340301514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.6590 | Train 0.4214 | Val 0.2560 | Test 0.2679
loading full batch data spends  0.0015137195587158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990436553955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029909133911132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10094285011291504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.614711880683899
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.6147 | Train 0.4500 | Val 0.2860 | Test 0.2993
loading full batch data spends  0.00144195556640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09901237487792969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6390023231506348
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.6390 | Train 0.4571 | Val 0.3020 | Test 0.2945
loading full batch data spends  0.0015377998352050781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028920650482177734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028925418853759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11122250556945801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6373786926269531
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.6374 | Train 0.4214 | Val 0.2620 | Test 0.2664
loading full batch data spends  0.0014307498931884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028831958770751953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028836727142333984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09843897819519043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.617598056793213
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.6176 | Train 0.4000 | Val 0.2500 | Test 0.2529
loading full batch data spends  0.0014774799346923828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030035972595214844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030040740966796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10454511642456055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6277471780776978
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.6277 | Train 0.4071 | Val 0.2700 | Test 0.2548
loading full batch data spends  0.001483917236328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.719329833984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02868795394897461  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02869272232055664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09479665756225586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.604048252105713
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.6040 | Train 0.4429 | Val 0.2900 | Test 0.2882
loading full batch data spends  0.0015134811401367188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029891014099121094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029895782470703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09682989120483398
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5915499925613403
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.5915 | Train 0.4429 | Val 0.3280 | Test 0.2916
loading full batch data spends  0.0014309883117675781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990436553955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029909133911132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10229706764221191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5798813104629517
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00200 | Loss 1.5799 | Train 0.4357 | Val 0.3180 | Test 0.2921
loading full batch data spends  0.0014765262603759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02880573272705078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028810501098632812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10350751876831055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009209156036376953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5907180309295654
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00201 | Loss 1.5907 | Train 0.5214 | Val 0.3300 | Test 0.3133
loading full batch data spends  0.001491546630859375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896331787109375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028901100158691406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10743427276611328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6229534149169922
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00202 | Loss 1.6230 | Train 0.5643 | Val 0.3340 | Test 0.3259
loading full batch data spends  0.001546621322631836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028936386108398438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02894115447998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1022958755493164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009249210357666016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5661776065826416
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00203 | Loss 1.5662 | Train 0.5071 | Val 0.3400 | Test 0.3206
loading full batch data spends  0.0014379024505615234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0288238525390625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882862091064453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10169219970703125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6273189783096313
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00204 | Loss 1.6273 | Train 0.4929 | Val 0.3320 | Test 0.3133
loading full batch data spends  0.001483917236328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827190399169922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028831958770751953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0971517562866211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.677093267440796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00205 | Loss 1.6771 | Train 0.5357 | Val 0.3400 | Test 0.3216
loading full batch data spends  0.0014827251434326172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892160415649414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028926372528076172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09546971321105957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6222697496414185
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00206 | Loss 1.6223 | Train 0.5429 | Val 0.3560 | Test 0.3312
loading full batch data spends  0.001497030258178711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02880573272705078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028810501098632812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10294461250305176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6085282564163208
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00207 | Loss 1.6085 | Train 0.4929 | Val 0.3520 | Test 0.3240
loading full batch data spends  0.0014240741729736328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876138687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028766155242919922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09796333312988281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5839743614196777
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00208 | Loss 1.5840 | Train 0.4714 | Val 0.3500 | Test 0.3235
loading full batch data spends  0.0015227794647216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029912471771240234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029917240142822266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10330557823181152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5948247909545898
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00209 | Loss 1.5948 | Train 0.4571 | Val 0.3200 | Test 0.3158
loading full batch data spends  0.001489400863647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029009342193603516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029014110565185547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10115480422973633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6353412866592407
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00210 | Loss 1.6353 | Train 0.4500 | Val 0.3060 | Test 0.3017
loading full batch data spends  0.0015344619750976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812408447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10346293449401855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6330746412277222
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00211 | Loss 1.6331 | Train 0.4357 | Val 0.2960 | Test 0.2848
loading full batch data spends  0.001461029052734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029960155487060547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029964923858642578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10037660598754883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5981813669204712
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00212 | Loss 1.5982 | Train 0.4500 | Val 0.3000 | Test 0.2950
loading full batch data spends  0.0014917850494384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029974937438964844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029979705810546875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10420846939086914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6044821739196777
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00213 | Loss 1.6045 | Train 0.4500 | Val 0.3220 | Test 0.2988
loading full batch data spends  0.0014989376068115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029942035675048828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994680404663086  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10226964950561523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6415761709213257
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00214 | Loss 1.6416 | Train 0.4357 | Val 0.3260 | Test 0.2993
loading full batch data spends  0.0015387535095214844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028920650482177734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028925418853759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1051781177520752
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6497361660003662
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00215 | Loss 1.6497 | Train 0.4929 | Val 0.3480 | Test 0.3148
loading full batch data spends  0.0014700889587402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.172325134277344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028933048248291016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10348296165466309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6681495904922485
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00216 | Loss 1.6681 | Train 0.5071 | Val 0.3500 | Test 0.3283
loading full batch data spends  0.0016171932220458984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10396599769592285
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.650472640991211
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00217 | Loss 1.6505 | Train 0.4929 | Val 0.3120 | Test 0.3153
loading full batch data spends  0.0015060901641845703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028842449188232422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847217559814453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09368038177490234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.656110167503357
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00218 | Loss 1.6561 | Train 0.4714 | Val 0.3360 | Test 0.3119
loading full batch data spends  0.0014786720275878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028984546661376953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028989315032958984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09699058532714844
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009281158447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6448020935058594
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00219 | Loss 1.6448 | Train 0.4714 | Val 0.3300 | Test 0.3221
loading full batch data spends  0.0014910697937011719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02875804901123047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0287628173828125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10216140747070312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6745529174804688
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00220 | Loss 1.6746 | Train 0.5286 | Val 0.3480 | Test 0.3399
loading full batch data spends  0.0015110969543457031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028842926025390625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10147881507873535
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.638224482536316
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00221 | Loss 1.6382 | Train 0.5429 | Val 0.3540 | Test 0.3341
loading full batch data spends  0.0014379024505615234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028815269470214844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028820037841796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10352039337158203
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5962169170379639
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00222 | Loss 1.5962 | Train 0.4929 | Val 0.3460 | Test 0.3119
loading full batch data spends  0.0014934539794921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873920440673828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1012108325958252
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.603162169456482
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00223 | Loss 1.6032 | Train 0.4714 | Val 0.3120 | Test 0.2945
loading full batch data spends  0.0014498233795166016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029909133911132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029913902282714844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09565210342407227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5466573238372803
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00224 | Loss 1.5467 | Train 0.4429 | Val 0.2920 | Test 0.2771
loading full batch data spends  0.0015263557434082031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892017364501953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028924942016601562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09771203994750977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6165117025375366
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00225 | Loss 1.6165 | Train 0.4500 | Val 0.2900 | Test 0.2766
loading full batch data spends  0.0014469623565673828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895355224609375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895832061767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09837913513183594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6450380086898804
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00226 | Loss 1.6450 | Train 0.4143 | Val 0.2960 | Test 0.2805
loading full batch data spends  0.0014643669128417969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893209457397461  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893686294555664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09618234634399414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5910166501998901
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00227 | Loss 1.5910 | Train 0.3857 | Val 0.2940 | Test 0.2708
loading full batch data spends  0.001444101333618164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889728546142578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028902053833007812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1007838249206543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5919779539108276
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00228 | Loss 1.5920 | Train 0.4286 | Val 0.3020 | Test 0.2809
loading full batch data spends  0.0014722347259521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289154052734375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892017364501953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1046442985534668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5395259857177734
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00229 | Loss 1.5395 | Train 0.4929 | Val 0.3140 | Test 0.2887
loading full batch data spends  0.0014576911926269531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028920650482177734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028925418853759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10021710395812988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6051937341690063
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00230 | Loss 1.6052 | Train 0.4643 | Val 0.3300 | Test 0.2868
loading full batch data spends  0.0014681816101074219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02985858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029863357543945312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09638762474060059
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6521345376968384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00231 | Loss 1.6521 | Train 0.4714 | Val 0.3140 | Test 0.2814
loading full batch data spends  0.0014796257019042969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028816699981689453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028821468353271484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10450220108032227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6037781238555908
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00232 | Loss 1.6038 | Train 0.4786 | Val 0.3220 | Test 0.2945
loading full batch data spends  0.0015344619750976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029952049255371094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029956817626953125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1033017635345459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6143701076507568
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00233 | Loss 1.6144 | Train 0.4857 | Val 0.3280 | Test 0.3104
loading full batch data spends  0.001425027847290039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924869537353516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029929637908935547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10337471961975098
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.598186731338501
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00234 | Loss 1.5982 | Train 0.4500 | Val 0.2860 | Test 0.2906
loading full batch data spends  0.0014781951904296875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887868881225586  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888345718383789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09505295753479004
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5884134769439697
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00235 | Loss 1.5884 | Train 0.3929 | Val 0.2900 | Test 0.2805
loading full batch data spends  0.0014243125915527344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932022094726562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029936790466308594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09992480278015137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6198511123657227
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00236 | Loss 1.6199 | Train 0.4714 | Val 0.3040 | Test 0.2747
loading full batch data spends  0.0015871524810791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028906822204589844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028911590576171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10279560089111328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.590549349784851
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00237 | Loss 1.5905 | Train 0.4929 | Val 0.3300 | Test 0.2921
loading full batch data spends  0.0014815330505371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029850006103515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029854774475097656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10141682624816895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5591434240341187
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00238 | Loss 1.5591 | Train 0.4929 | Val 0.3200 | Test 0.2872
loading full batch data spends  0.0014758110046386719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922557830810547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028927326202392578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1037755012512207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009227752685546875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6158653497695923
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00239 | Loss 1.6159 | Train 0.4357 | Val 0.2940 | Test 0.2640
loading full batch data spends  0.0014967918395996094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02880716323852539  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028811931610107422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09428668022155762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.584470272064209
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00240 | Loss 1.5845 | Train 0.4214 | Val 0.2740 | Test 0.2553
loading full batch data spends  0.0015549659729003906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028931617736816406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028936386108398438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10377359390258789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6475701332092285
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00241 | Loss 1.6476 | Train 0.4786 | Val 0.3000 | Test 0.2795
loading full batch data spends  0.0014815330505371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892303466796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1038665771484375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5458883047103882
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00242 | Loss 1.5459 | Train 0.5000 | Val 0.3300 | Test 0.2872
loading full batch data spends  0.0015361309051513672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09165072441101074
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5976282358169556
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00243 | Loss 1.5976 | Train 0.4643 | Val 0.2900 | Test 0.2693
loading full batch data spends  0.0014836788177490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888774871826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889251708984375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09563231468200684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5725412368774414
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00244 | Loss 1.5725 | Train 0.4286 | Val 0.2760 | Test 0.2452
loading full batch data spends  0.0014693737030029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028942108154296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028946876525878906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10401797294616699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5842458009719849
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00245 | Loss 1.5842 | Train 0.4214 | Val 0.3000 | Test 0.2737
loading full batch data spends  0.0014264583587646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892589569091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289306640625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1042327880859375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6094608306884766
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00246 | Loss 1.6095 | Train 0.4714 | Val 0.3200 | Test 0.2843
loading full batch data spends  0.0014946460723876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5987625122070312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883625030517578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028841018676757812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10097551345825195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009209156036376953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.569676160812378
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00247 | Loss 1.5697 | Train 0.3786 | Val 0.2560 | Test 0.2176
loading full batch data spends  0.0014774799346923828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888345718383789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028888225555419922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10148882865905762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5965991020202637
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00248 | Loss 1.5966 | Train 0.3500 | Val 0.1900 | Test 0.1920
loading full batch data spends  0.0014731884002685547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865337371826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10400748252868652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009251594543457031  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.624042272567749
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00249 | Loss 1.6240 | Train 0.3571 | Val 0.2040 | Test 0.1949
loading full batch data spends  0.0014820098876953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881765365600586  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882242202758789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1042630672454834
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6155012845993042
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00250 | Loss 1.6155 | Train 0.4786 | Val 0.2840 | Test 0.2481
loading full batch data spends  0.0015215873718261719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029825687408447266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029830455780029297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10098791122436523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.630531907081604
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00251 | Loss 1.6305 | Train 0.5429 | Val 0.3060 | Test 0.2926
loading full batch data spends  0.0014526844024658203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028917789459228516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922557830810547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09631752967834473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.577271580696106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00252 | Loss 1.5773 | Train 0.4643 | Val 0.2420 | Test 0.2413
loading full batch data spends  0.001470804214477539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029909133911132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029913902282714844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10350227355957031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5936857461929321
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00253 | Loss 1.5937 | Train 0.4429 | Val 0.2440 | Test 0.2302
loading full batch data spends  0.0014410018920898438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877950668334961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878427505493164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10531377792358398
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.597159743309021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00254 | Loss 1.5972 | Train 0.4714 | Val 0.2740 | Test 0.2515
loading full batch data spends  0.0015804767608642578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0299835205078125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02998828887939453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10420680046081543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5962092876434326
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00255 | Loss 1.5962 | Train 0.4429 | Val 0.2680 | Test 0.2490
loading full batch data spends  0.0014836788177490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991771697998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0299224853515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10277009010314941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5590546131134033
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00256 | Loss 1.5591 | Train 0.3786 | Val 0.2320 | Test 0.2060
loading full batch data spends  0.0014853477478027344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02995157241821289  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029956340789794922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10222768783569336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6347285509109497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00257 | Loss 1.6347 | Train 0.3786 | Val 0.2220 | Test 0.2031
loading full batch data spends  0.0014157295227050781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886676788330078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028871536254882812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09988832473754883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5798155069351196
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00258 | Loss 1.5798 | Train 0.3929 | Val 0.2440 | Test 0.2084
loading full batch data spends  0.0014681816101074219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028853416442871094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028858184814453125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09658098220825195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6270827054977417
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00259 | Loss 1.6271 | Train 0.4643 | Val 0.2760 | Test 0.2471
loading full batch data spends  0.0014841556549072266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932022094726562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029936790466308594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0996088981628418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.625562071800232
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00260 | Loss 1.6256 | Train 0.5286 | Val 0.2820 | Test 0.2582
loading full batch data spends  0.0014796257019042969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02894306182861328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028947830200195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09539604187011719
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5802110433578491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00261 | Loss 1.5802 | Train 0.5357 | Val 0.2800 | Test 0.2679
loading full batch data spends  0.0014276504516601562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029872417449951172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029877185821533203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09503173828125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5643192529678345
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00262 | Loss 1.5643 | Train 0.5357 | Val 0.2760 | Test 0.2708
loading full batch data spends  0.0014727115631103516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879047393798828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028795242309570312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09487509727478027
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6063123941421509
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00263 | Loss 1.6063 | Train 0.5357 | Val 0.3000 | Test 0.2766
loading full batch data spends  0.0014445781707763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029939651489257812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029944419860839844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09222650527954102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.580043911933899
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00264 | Loss 1.5800 | Train 0.5429 | Val 0.3000 | Test 0.2698
loading full batch data spends  0.0014591217041015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893209457397461  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893686294555664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0970914363861084
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.536093831062317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00265 | Loss 1.5361 | Train 0.4429 | Val 0.2680 | Test 0.2316
loading full batch data spends  0.0014538764953613281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029916763305664062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029921531677246094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09995150566101074
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5844707489013672
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00266 | Loss 1.5845 | Train 0.4071 | Val 0.2400 | Test 0.2099
loading full batch data spends  0.0014705657958984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887439727783203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09673094749450684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009198188781738281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6142891645431519
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00267 | Loss 1.6143 | Train 0.4143 | Val 0.2500 | Test 0.2137
loading full batch data spends  0.001438140869140625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02997589111328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02998065948486328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10331344604492188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5861276388168335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00268 | Loss 1.5861 | Train 0.4571 | Val 0.2620 | Test 0.2234
loading full batch data spends  0.0015025138854980469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02897930145263672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898406982421875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10444521903991699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009281158447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5338518619537354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00269 | Loss 1.5339 | Train 0.4500 | Val 0.2920 | Test 0.2495
loading full batch data spends  0.001462697982788086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029900550842285156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029905319213867188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0962076187133789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5268927812576294
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00270 | Loss 1.5269 | Train 0.4643 | Val 0.2860 | Test 0.2621
loading full batch data spends  0.0015423297882080078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885723114013672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886199951171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10625243186950684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5679361820220947
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00271 | Loss 1.5679 | Train 0.4429 | Val 0.2660 | Test 0.2500
loading full batch data spends  0.0014421939849853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028909683227539062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028914451599121094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09783434867858887
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5959644317626953
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00272 | Loss 1.5960 | Train 0.5000 | Val 0.2780 | Test 0.2655
loading full batch data spends  0.0014662742614746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029917240142822266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029922008514404297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10193395614624023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.583251953125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00273 | Loss 1.5833 | Train 0.5000 | Val 0.2700 | Test 0.2631
loading full batch data spends  0.0014269351959228516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029948711395263672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029953479766845703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0952603816986084
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5932694673538208
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00274 | Loss 1.5933 | Train 0.4929 | Val 0.2640 | Test 0.2471
loading full batch data spends  0.0014715194702148438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028842926025390625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09712839126586914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6280587911605835
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00275 | Loss 1.6281 | Train 0.5071 | Val 0.2640 | Test 0.2495
loading full batch data spends  0.001428365707397461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029801368713378906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029806137084960938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09677386283874512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5949331521987915
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00276 | Loss 1.5949 | Train 0.4786 | Val 0.2640 | Test 0.2568
loading full batch data spends  0.001470327377319336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029829978942871094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029834747314453125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10314393043518066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5616270303726196
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00277 | Loss 1.5616 | Train 0.4571 | Val 0.2560 | Test 0.2350
loading full batch data spends  0.0014233589172363281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851985931396484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028856754302978516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09317874908447266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5440605878829956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00278 | Loss 1.5441 | Train 0.4429 | Val 0.2420 | Test 0.2195
loading full batch data spends  0.0014650821685791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028888702392578125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028893470764160156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0938417911529541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5750205516815186
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00279 | Loss 1.5750 | Train 0.4286 | Val 0.2440 | Test 0.2132
loading full batch data spends  0.0014390945434570312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028862953186035156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867721557617188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1020817756652832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.570091724395752
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00280 | Loss 1.5701 | Train 0.4286 | Val 0.2480 | Test 0.2113
loading full batch data spends  0.0014736652374267578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876781463623047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028881549835205078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10402274131774902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5436556339263916
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00281 | Loss 1.5437 | Train 0.4500 | Val 0.2440 | Test 0.2191
loading full batch data spends  0.0014841556549072266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994251251220703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029947280883789062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09750890731811523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.568655252456665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00282 | Loss 1.5687 | Train 0.4714 | Val 0.2380 | Test 0.2340
loading full batch data spends  0.0015187263488769531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028934001922607422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028938770294189453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10364031791687012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5798778533935547
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00283 | Loss 1.5799 | Train 0.5071 | Val 0.2580 | Test 0.2423
loading full batch data spends  0.001430511474609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029918193817138672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029922962188720703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09761452674865723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5382064580917358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00284 | Loss 1.5382 | Train 0.5071 | Val 0.2760 | Test 0.2616
loading full batch data spends  0.001477956771850586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993297576904297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029937744140625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10039615631103516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.578141450881958
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00285 | Loss 1.5781 | Train 0.5214 | Val 0.2440 | Test 0.2321
loading full batch data spends  0.0014247894287109375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908252716064453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913021087646484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09568095207214355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6274011135101318
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00286 | Loss 1.6274 | Train 0.5071 | Val 0.2480 | Test 0.2292
loading full batch data spends  0.0014777183532714844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10373425483703613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.531574010848999
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00287 | Loss 1.5316 | Train 0.5429 | Val 0.2920 | Test 0.2795
loading full batch data spends  0.0014317035675048828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029918193817138672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029922962188720703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10272884368896484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.536216139793396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00288 | Loss 1.5362 | Train 0.5286 | Val 0.2620 | Test 0.2611
loading full batch data spends  0.0014972686767578125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029900550842285156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029905319213867188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10258698463439941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.601443886756897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00289 | Loss 1.6014 | Train 0.4857 | Val 0.2200 | Test 0.2365
loading full batch data spends  0.0015063285827636719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890777587890625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09766578674316406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5419552326202393
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00290 | Loss 1.5420 | Train 0.4500 | Val 0.2260 | Test 0.2278
loading full batch data spends  0.0014700889587402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028841495513916016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028846263885498047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09736514091491699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5993108749389648
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00291 | Loss 1.5993 | Train 0.4857 | Val 0.2340 | Test 0.2350
loading full batch data spends  0.0014350414276123047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877330780029297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028778076171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09754204750061035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5598047971725464
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00292 | Loss 1.5598 | Train 0.4714 | Val 0.2260 | Test 0.2137
loading full batch data spends  0.001527547836303711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028878211975097656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10017776489257812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.588824987411499
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00293 | Loss 1.5888 | Train 0.3929 | Val 0.1760 | Test 0.1847
loading full batch data spends  0.001504659652709961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028884410858154297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028889179229736328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10299801826477051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.583911657333374
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00294 | Loss 1.5839 | Train 0.3714 | Val 0.1600 | Test 0.1731
loading full batch data spends  0.0015537738800048828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1024484634399414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00922250747680664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.645912528038025
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00295 | Loss 1.6459 | Train 0.3786 | Val 0.1840 | Test 0.1809
loading full batch data spends  0.0014605522155761719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.933906555175781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029918670654296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029923439025878906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10084867477416992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.651271104812622
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00296 | Loss 1.6513 | Train 0.4643 | Val 0.2320 | Test 0.2132
loading full batch data spends  0.0015380382537841797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028925418853759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028930187225341797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10395288467407227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6319520473480225
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00297 | Loss 1.6320 | Train 0.5214 | Val 0.2280 | Test 0.2568
loading full batch data spends  0.0015137195587158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891397476196289  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028918743133544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0927736759185791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5500907897949219
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00298 | Loss 1.5501 | Train 0.4929 | Val 0.2140 | Test 0.2408
loading full batch data spends  0.001514434814453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029917240142822266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029922008514404297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10492348670959473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5932331085205078
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00299 | Loss 1.5932 | Train 0.4714 | Val 0.2220 | Test 0.2384
loading full batch data spends  0.0015020370483398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891063690185547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289154052734375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11003804206848145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.551033854484558
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00300 | Loss 1.5510 | Train 0.4643 | Val 0.2400 | Test 0.2655
loading full batch data spends  0.0015463829040527344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882099151611328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028825759887695312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0997622013092041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5903364419937134
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00301 | Loss 1.5903 | Train 0.5286 | Val 0.2540 | Test 0.2756
loading full batch data spends  0.001459360122680664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029933929443359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029938697814941406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1032259464263916
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6046205759048462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00302 | Loss 1.6046 | Train 0.5143 | Val 0.2320 | Test 0.2606
loading full batch data spends  0.0015056133270263672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994823455810547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0299530029296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10108613967895508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5792919397354126
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00303 | Loss 1.5793 | Train 0.4500 | Val 0.2080 | Test 0.2258
loading full batch data spends  0.0014338493347167969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882503509521484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028887271881103516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09659290313720703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.650301218032837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00304 | Loss 1.6503 | Train 0.4214 | Val 0.2060 | Test 0.2089
loading full batch data spends  0.001493692398071289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029898643493652344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029903411865234375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10817313194274902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.60349702835083
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00305 | Loss 1.6035 | Train 0.4357 | Val 0.2160 | Test 0.2128
loading full batch data spends  0.0016093254089355469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.935264587402344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10553812980651855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5766266584396362
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00306 | Loss 1.5766 | Train 0.4786 | Val 0.2420 | Test 0.2326
loading full batch data spends  0.0015459060668945312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029869556427001953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029874324798583984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10145330429077148
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5706294775009155
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00307 | Loss 1.5706 | Train 0.4857 | Val 0.2600 | Test 0.2534
loading full batch data spends  0.0014319419860839844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992534637451172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993011474609375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09905266761779785
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.536353349685669
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00308 | Loss 1.5364 | Train 0.5286 | Val 0.2760 | Test 0.2626
loading full batch data spends  0.0015339851379394531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029933929443359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029938697814941406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09645628929138184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5839015245437622
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00309 | Loss 1.5839 | Train 0.5214 | Val 0.2600 | Test 0.2606
loading full batch data spends  0.0014405250549316406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029935359954833984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029940128326416016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09850907325744629
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5881366729736328
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00310 | Loss 1.5881 | Train 0.5071 | Val 0.2880 | Test 0.2660
loading full batch data spends  0.0014886856079101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028856277465820312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028861045837402344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10129308700561523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5324864387512207
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00311 | Loss 1.5325 | Train 0.5143 | Val 0.3000 | Test 0.2722
loading full batch data spends  0.0014309883117675781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890634536743164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028911113739013672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10059475898742676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.543299674987793
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00312 | Loss 1.5433 | Train 0.5071 | Val 0.3160 | Test 0.2722
loading full batch data spends  0.0014972686767578125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029980182647705078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02998495101928711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09910297393798828
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5484545230865479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00313 | Loss 1.5485 | Train 0.4857 | Val 0.2920 | Test 0.2587
loading full batch data spends  0.001445770263671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028904438018798828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890920639038086  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09680438041687012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5214418172836304
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00314 | Loss 1.5214 | Train 0.5000 | Val 0.2940 | Test 0.2587
loading full batch data spends  0.0015392303466796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876901626586914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028773784637451172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09661412239074707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5555301904678345
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00315 | Loss 1.5555 | Train 0.5214 | Val 0.2980 | Test 0.2621
loading full batch data spends  0.0014920234680175781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02986431121826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02986907958984375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09927749633789062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.568886399269104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00316 | Loss 1.5689 | Train 0.5143 | Val 0.3040 | Test 0.2655
loading full batch data spends  0.0015263557434082031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029886722564697266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029891490936279297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10177040100097656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5737030506134033
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00317 | Loss 1.5737 | Train 0.5214 | Val 0.3200 | Test 0.2732
loading full batch data spends  0.001486063003540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993011474609375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993488311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09894061088562012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.610300064086914
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00318 | Loss 1.6103 | Train 0.5214 | Val 0.2900 | Test 0.2626
loading full batch data spends  0.0014786720275878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028917312622070312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10420846939086914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5304957628250122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00319 | Loss 1.5305 | Train 0.4643 | Val 0.2340 | Test 0.2176
loading full batch data spends  0.0014874935150146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028862476348876953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867244720458984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09786534309387207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5705814361572266
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00320 | Loss 1.5706 | Train 0.4571 | Val 0.2300 | Test 0.2132
loading full batch data spends  0.0014781951904296875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028893470764160156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028898239135742188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10291671752929688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5980926752090454
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00321 | Loss 1.5981 | Train 0.4714 | Val 0.2360 | Test 0.2268
loading full batch data spends  0.001444101333618164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028878211975097656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1036224365234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5796267986297607
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00322 | Loss 1.5796 | Train 0.5071 | Val 0.2500 | Test 0.2336
loading full batch data spends  0.0014736652374267578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882503509521484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028887271881103516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10417747497558594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5278778076171875
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00323 | Loss 1.5279 | Train 0.5000 | Val 0.2580 | Test 0.2403
loading full batch data spends  0.0014832019805908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02996540069580078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029970169067382812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0981748104095459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5440655946731567
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00324 | Loss 1.5441 | Train 0.5143 | Val 0.2960 | Test 0.2771
loading full batch data spends  0.0014944076538085938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924392700195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029929161071777344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10151886940002441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5604206323623657
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00325 | Loss 1.5604 | Train 0.4786 | Val 0.2680 | Test 0.2500
loading full batch data spends  0.0014925003051757812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029923439025878906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029928207397460938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09591794013977051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5969274044036865
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00326 | Loss 1.5969 | Train 0.4143 | Val 0.2640 | Test 0.2369
loading full batch data spends  0.0015218257904052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028799057006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02880382537841797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10301327705383301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00917673110961914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.639806866645813
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00327 | Loss 1.6398 | Train 0.4500 | Val 0.2520 | Test 0.2403
loading full batch data spends  0.0014271736145019531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991199493408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029916763305664062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09766292572021484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5551917552947998
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00328 | Loss 1.5552 | Train 0.4286 | Val 0.2600 | Test 0.2302
loading full batch data spends  0.001470327377319336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028829097747802734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028833866119384766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1026618480682373
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009187698364257812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5791125297546387
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00329 | Loss 1.5791 | Train 0.4143 | Val 0.2640 | Test 0.2316
loading full batch data spends  0.0014219284057617188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028804779052734375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809547424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0983586311340332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5509452819824219
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00330 | Loss 1.5509 | Train 0.4929 | Val 0.2640 | Test 0.2442
loading full batch data spends  0.0014884471893310547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028898239135742188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890300750732422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0971977710723877
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.522817611694336
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00331 | Loss 1.5228 | Train 0.5143 | Val 0.2900 | Test 0.2645
loading full batch data spends  0.00148773193359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887439727783203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10137724876403809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5921189785003662
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00332 | Loss 1.5921 | Train 0.4786 | Val 0.2860 | Test 0.2674
loading full batch data spends  0.0014796257019042969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028991222381591797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028995990753173828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10491228103637695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009265422821044922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5131791830062866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00333 | Loss 1.5132 | Train 0.4786 | Val 0.2760 | Test 0.2577
loading full batch data spends  0.0015082359313964844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029901981353759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029906749725341797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09802913665771484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5613067150115967
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00334 | Loss 1.5613 | Train 0.4857 | Val 0.2560 | Test 0.2582
loading full batch data spends  0.0015511512756347656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028766155242919922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028770923614501953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10342812538146973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00917673110961914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5238479375839233
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00335 | Loss 1.5238 | Train 0.4786 | Val 0.2520 | Test 0.2515
loading full batch data spends  0.001493692398071289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908729553222656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10085153579711914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5438810586929321
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00336 | Loss 1.5439 | Train 0.5071 | Val 0.2560 | Test 0.2568
loading full batch data spends  0.0014717578887939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932498931884766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029937267303466797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09783792495727539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5721241235733032
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00337 | Loss 1.5721 | Train 0.5000 | Val 0.2720 | Test 0.2602
loading full batch data spends  0.001512289047241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029880523681640625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029885292053222656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10256409645080566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5254052877426147
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00338 | Loss 1.5254 | Train 0.4857 | Val 0.2700 | Test 0.2456
loading full batch data spends  0.0015425682067871094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884054183959961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884531021118164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09857606887817383
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.568676471710205
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00339 | Loss 1.5687 | Train 0.4786 | Val 0.2660 | Test 0.2258
loading full batch data spends  0.0014429092407226562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892780303955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028932571411132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09582972526550293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.513545274734497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00340 | Loss 1.5135 | Train 0.4643 | Val 0.2500 | Test 0.2282
loading full batch data spends  0.0015230178833007812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028872013092041016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876781463623047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10292887687683105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.550679326057434
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00341 | Loss 1.5507 | Train 0.4714 | Val 0.2400 | Test 0.2239
loading full batch data spends  0.0014941692352294922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847217559814453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851985931396484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10297274589538574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5391795635223389
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00342 | Loss 1.5392 | Train 0.4357 | Val 0.1940 | Test 0.1949
loading full batch data spends  0.0015323162078857422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02897167205810547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289764404296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10446882247924805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009243965148925781  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5357811450958252
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00343 | Loss 1.5358 | Train 0.4357 | Val 0.1740 | Test 0.1925
loading full batch data spends  0.00142669677734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02874469757080078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028749465942382812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09650492668151855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.611015796661377
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00344 | Loss 1.6110 | Train 0.4357 | Val 0.2020 | Test 0.2007
loading full batch data spends  0.0014865398406982422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5272369384765625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908252716064453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913021087646484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10428071022033691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5469684600830078
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00345 | Loss 1.5470 | Train 0.4643 | Val 0.2340 | Test 0.2345
loading full batch data spends  0.0014824867248535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888965606689453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894424438476562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10361695289611816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5250078439712524
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00346 | Loss 1.5250 | Train 0.4286 | Val 0.2220 | Test 0.2292
loading full batch data spends  0.001478433609008789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029845714569091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029850482940673828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10268259048461914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5993925333023071
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00347 | Loss 1.5994 | Train 0.4286 | Val 0.2200 | Test 0.2316
loading full batch data spends  0.0014541149139404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887725830078125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.101043701171875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6357991695404053
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00348 | Loss 1.6358 | Train 0.4357 | Val 0.2300 | Test 0.2432
loading full batch data spends  0.0015480518341064453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029888629913330078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989339828491211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10063290596008301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5521690845489502
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00349 | Loss 1.5522 | Train 0.4714 | Val 0.2320 | Test 0.2519
loading full batch data spends  0.0014405250549316406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028840065002441406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028844833374023438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10633254051208496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5144151449203491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00350 | Loss 1.5144 | Train 0.4643 | Val 0.2300 | Test 0.2249
loading full batch data spends  0.0014815330505371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02986907958984375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02987384796142578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10346221923828125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5606250762939453
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00351 | Loss 1.5606 | Train 0.4143 | Val 0.2020 | Test 0.1987
loading full batch data spends  0.0014896392822265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029886722564697266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029891490936279297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09854364395141602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5277608633041382
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00352 | Loss 1.5278 | Train 0.3857 | Val 0.1860 | Test 0.1949
loading full batch data spends  0.0015244483947753906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028944969177246094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028949737548828125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10217142105102539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6121301651000977
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00353 | Loss 1.6121 | Train 0.4286 | Val 0.1980 | Test 0.2050
loading full batch data spends  0.0015075206756591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883625030517578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028841018676757812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10289454460144043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6204705238342285
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00354 | Loss 1.6205 | Train 0.4214 | Val 0.2280 | Test 0.2229
loading full batch data spends  0.003117084503173828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.078315734863281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028752803802490234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028757572174072266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10584592819213867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009209156036376953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5460410118103027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00355 | Loss 1.5460 | Train 0.4286 | Val 0.2420 | Test 0.2369
loading full batch data spends  0.0014390945434570312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876304626464844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028881072998046875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10480284690856934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5790166854858398
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00356 | Loss 1.5790 | Train 0.4500 | Val 0.2420 | Test 0.2418
loading full batch data spends  0.0014977455139160156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028859615325927734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10073328018188477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5539803504943848
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00357 | Loss 1.5540 | Train 0.4500 | Val 0.2720 | Test 0.2539
loading full batch data spends  0.0015170574188232422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.790855407714844e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029891014099121094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029895782470703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10370707511901855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5391429662704468
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00358 | Loss 1.5391 | Train 0.4714 | Val 0.2600 | Test 0.2340
loading full batch data spends  0.0017061233520507812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028811931610107422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028816699981689453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09657955169677734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5639582872390747
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00359 | Loss 1.5640 | Train 0.4214 | Val 0.2240 | Test 0.2103
loading full batch data spends  0.0014336109161376953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028791427612304688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879619598388672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09761953353881836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5693538188934326
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00360 | Loss 1.5694 | Train 0.4286 | Val 0.2160 | Test 0.2045
loading full batch data spends  0.0014996528625488281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028952598571777344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028957366943359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10414481163024902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009259700775146484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5668989419937134
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00361 | Loss 1.5669 | Train 0.4214 | Val 0.2240 | Test 0.2016
loading full batch data spends  0.0014319419860839844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028768062591552734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028772830963134766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10486078262329102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5561109781265259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00362 | Loss 1.5561 | Train 0.4500 | Val 0.2540 | Test 0.2220
loading full batch data spends  0.0014786720275878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028924942016601562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028929710388183594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09992408752441406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.559267520904541
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00363 | Loss 1.5593 | Train 0.4786 | Val 0.3020 | Test 0.2713
loading full batch data spends  0.00142669677734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847217559814453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851985931396484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09228849411010742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.574823021888733
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00364 | Loss 1.5748 | Train 0.4643 | Val 0.3100 | Test 0.2800
loading full batch data spends  0.0015404224395751953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028991222381591797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028995990753173828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09337949752807617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00927591323852539  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5499074459075928
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00365 | Loss 1.5499 | Train 0.4857 | Val 0.3100 | Test 0.2843
loading full batch data spends  0.0014274120330810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885841369628906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890609741210938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10109114646911621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5235799551010132
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00366 | Loss 1.5236 | Train 0.4929 | Val 0.3040 | Test 0.2785
loading full batch data spends  0.0015361309051513672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09513711929321289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5315971374511719
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00367 | Loss 1.5316 | Train 0.4714 | Val 0.2800 | Test 0.2582
loading full batch data spends  0.0014755725860595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028778076171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878284454345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1038210391998291
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5425262451171875
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00368 | Loss 1.5425 | Train 0.4071 | Val 0.2320 | Test 0.2186
loading full batch data spends  0.0015151500701904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993488311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029939651489257812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10604214668273926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5444636344909668
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00369 | Loss 1.5445 | Train 0.4071 | Val 0.2140 | Test 0.2123
loading full batch data spends  0.001447439193725586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028883934020996094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028888702392578125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09765791893005371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.605445384979248
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00370 | Loss 1.6054 | Train 0.4214 | Val 0.2520 | Test 0.2398
loading full batch data spends  0.0014722347259521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029890060424804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989482879638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09685206413269043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5278491973876953
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00371 | Loss 1.5278 | Train 0.4286 | Val 0.2380 | Test 0.2331
loading full batch data spends  0.0014274120330810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028834819793701172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028839588165283203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0991525650024414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5453804731369019
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00372 | Loss 1.5454 | Train 0.4071 | Val 0.2300 | Test 0.2094
loading full batch data spends  0.001512289047241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028903961181640625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908729553222656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09874343872070312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.627586007118225
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00373 | Loss 1.6276 | Train 0.4000 | Val 0.2220 | Test 0.2137
loading full batch data spends  0.00146484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029933929443359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029938697814941406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09482812881469727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5427846908569336
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00374 | Loss 1.5428 | Train 0.4214 | Val 0.2220 | Test 0.2147
loading full batch data spends  0.0015223026275634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028933048248291016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1002500057220459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923299789428711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.511591911315918
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00375 | Loss 1.5116 | Train 0.4571 | Val 0.2300 | Test 0.2278
loading full batch data spends  0.001447439193725586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989959716796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990436553955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10159611701965332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5573604106903076
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00376 | Loss 1.5574 | Train 0.4071 | Val 0.2040 | Test 0.2012
loading full batch data spends  0.0014998912811279297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908252716064453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913021087646484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10222458839416504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5823824405670166
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00377 | Loss 1.5824 | Train 0.4143 | Val 0.1800 | Test 0.1862
loading full batch data spends  0.001476287841796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884197235107422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884674072265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.100250244140625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5526584386825562
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00378 | Loss 1.5527 | Train 0.4000 | Val 0.1880 | Test 0.1934
loading full batch data spends  0.0014688968658447266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028916358947753906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028921127319335938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0925896167755127
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.626890778541565
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00379 | Loss 1.6269 | Train 0.4643 | Val 0.2160 | Test 0.2200
loading full batch data spends  0.0014216899871826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990579605102539  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029910564422607422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09538459777832031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5587151050567627
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00380 | Loss 1.5587 | Train 0.4929 | Val 0.2440 | Test 0.2505
loading full batch data spends  0.0014867782592773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028844356536865234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028849124908447266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10340571403503418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009251594543457031  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.587249755859375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00381 | Loss 1.5872 | Train 0.4643 | Val 0.2360 | Test 0.2631
loading full batch data spends  0.0015022754669189453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029899120330810547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029903888702392578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10018253326416016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5679218769073486
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00382 | Loss 1.5679 | Train 0.4643 | Val 0.2300 | Test 0.2597
loading full batch data spends  0.0014851093292236328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028982162475585938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898693084716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09990048408508301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009265422821044922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.574902057647705
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00383 | Loss 1.5749 | Train 0.4929 | Val 0.2340 | Test 0.2664
loading full batch data spends  0.0014536380767822266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028861522674560547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028866291046142578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10149192810058594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.51250159740448
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00384 | Loss 1.5125 | Train 0.4857 | Val 0.2420 | Test 0.2703
loading full batch data spends  0.0014755725860595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888345718383789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028888225555419922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10297560691833496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5277365446090698
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00385 | Loss 1.5277 | Train 0.4857 | Val 0.2620 | Test 0.2611
loading full batch data spends  0.0016739368438720703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029926300048828125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029931068420410156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10268425941467285
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5861512422561646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00386 | Loss 1.5862 | Train 0.4786 | Val 0.2720 | Test 0.2660
loading full batch data spends  0.0014917850494384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932022094726562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029936790466308594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10429739952087402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.55915367603302
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00387 | Loss 1.5592 | Train 0.4857 | Val 0.2860 | Test 0.2655
loading full batch data spends  0.0014111995697021484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028795242309570312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028800010681152344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10188770294189453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.492531418800354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00388 | Loss 1.4925 | Train 0.4929 | Val 0.2800 | Test 0.2785
loading full batch data spends  0.0014584064483642578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989959716796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990436553955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10141754150390625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.53322172164917
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00389 | Loss 1.5332 | Train 0.4857 | Val 0.2860 | Test 0.2776
loading full batch data spends  0.001489400863647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890132904052734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894901275634766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09653902053833008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5874643325805664
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00390 | Loss 1.5875 | Train 0.4857 | Val 0.2800 | Test 0.2766
loading full batch data spends  0.0014736652374267578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029912471771240234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029917240142822266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10984659194946289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5209492444992065
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00391 | Loss 1.5209 | Train 0.4857 | Val 0.2780 | Test 0.2824
loading full batch data spends  0.001491546630859375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812885284423828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881765365600586  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10791826248168945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.560305118560791
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00392 | Loss 1.5603 | Train 0.4786 | Val 0.2860 | Test 0.2829
loading full batch data spends  0.0015025138854980469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02983570098876953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029840469360351562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11150431632995605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5391525030136108
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00393 | Loss 1.5392 | Train 0.4786 | Val 0.2900 | Test 0.2805
loading full batch data spends  0.0015387535095214844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809070587158203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028813838958740234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10930299758911133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5593851804733276
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00394 | Loss 1.5594 | Train 0.4857 | Val 0.2740 | Test 0.2795
loading full batch data spends  0.0015387535095214844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028891563415527344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896331787109375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11024022102355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.537062168121338
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00395 | Loss 1.5371 | Train 0.5071 | Val 0.2860 | Test 0.2703
loading full batch data spends  0.00144195556640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028808116912841797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812885284423828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10534834861755371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4953912496566772
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00396 | Loss 1.4954 | Train 0.5143 | Val 0.2700 | Test 0.2505
loading full batch data spends  0.0014851093292236328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5987625122070312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028764724731445312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028769493103027344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10663843154907227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009155750274658203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5204801559448242
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00397 | Loss 1.5205 | Train 0.5143 | Val 0.2780 | Test 0.2485
loading full batch data spends  0.0014889240264892578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10804581642150879
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5896354913711548
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00398 | Loss 1.5896 | Train 0.5000 | Val 0.2300 | Test 0.2403
loading full batch data spends  0.0015158653259277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028806209564208984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028810977935791016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10679316520690918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5568422079086304
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00399 | Loss 1.5568 | Train 0.4714 | Val 0.2460 | Test 0.2427
Total (block generation + training)time/epoch 0.11049244880676269
pure train time/epoch 0.10226624240778913

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368, 1367, 1352, 1359, 1359, 1355, 1349, 1360, 1356, 1341, 1367, 1363, 1356, 1368, 1371, 1369, 1363, 1362, 1367, 1357, 1365, 1337, 1358, 1356, 1358, 1371, 1363, 1359, 1364, 1361, 1365, 1353, 1369, 1352, 1374, 1367, 1359, 1369, 1361, 1369, 1355, 1345, 1362, 1363, 1358, 1358, 1365, 1365, 1352, 1356, 1360, 1356, 1366, 1361, 1371, 1354, 1367, 1372, 1380, 1354, 1357, 1367, 1363, 1368, 1356, 1375, 1364, 1367, 1350, 1368, 1365, 1377, 1355, 1363, 1371, 1368, 1354, 1367, 1374, 1355, 1357, 1357, 1358, 1368, 1362, 1370, 1370, 1362, 1362, 1374, 1368, 1363, 1353, 1340, 1356, 1363, 1354, 1366, 1363, 1361, 1366, 1360, 1356, 1368, 1366, 1358, 1371, 1355, 1368, 1369, 1368, 1368, 1356, 1361, 1367, 1357, 1354, 1366, 1372, 1371, 1357, 1352, 1354, 1358, 1353, 1367, 1372, 1368, 1346, 1370, 1348, 1354, 1362, 1362, 1362, 1368, 1346, 1363, 1366, 1367, 1358, 1364, 1358, 1357, 1358, 1348, 1362, 1361, 1368, 1362, 1375, 1349, 1366, 1370, 1365, 1356, 1352, 1365, 1358, 1378, 1353, 1348, 1361, 1355, 1363, 1363, 1364, 1360, 1371, 1345, 1379, 1359, 1369, 1360, 1362, 1377, 1356, 1374, 1359, 1359, 1362, 1378, 1360, 1368, 1362, 1363, 1354, 1375, 1373, 1352, 1372, 1359, 1367, 1355, 1367, 1356, 1361, 1350, 1342, 1356, 1349]
