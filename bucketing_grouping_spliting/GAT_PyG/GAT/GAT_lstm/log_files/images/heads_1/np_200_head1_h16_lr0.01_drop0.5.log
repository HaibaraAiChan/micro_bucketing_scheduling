main start at this time 1696112068.2680647
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.002824068069458008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.4318695068359375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.54296875 GB
    Memory Allocated: 0.018217086791992188  GigaBytes
Max Memory Allocated: 0.018308639526367188  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.54296875 GB
    Memory Allocated: 0.01822185516357422  GigaBytes
Max Memory Allocated: 0.018308639526367188  GigaBytes

pure train time  0.5026082992553711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.55078125 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.018308639526367188  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9517022371292114
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9517 | Train 0.1214 | Val 0.0980 | Test 0.1136
loading full batch data spends  0.001998424530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018477916717529297  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018482685089111328  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

pure train time  0.10765576362609863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007733345031738281  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9494833946228027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9495 | Train 0.1214 | Val 0.1020 | Test 0.1214
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01856088638305664  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018565654754638672  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

pure train time  0.11828041076660156
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.03989553451538086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9469364881515503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9469 | Train 0.1357 | Val 0.0840 | Test 0.0991
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018571853637695312  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018576622009277344  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

pure train time  0.11827611923217773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9465606212615967
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9466 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019659996032714844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.220008850097656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01856088638305664  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018565654754638672  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

pure train time  0.11757302284240723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9441415071487427
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9441 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019927024841308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01854085922241211  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01854562759399414  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

pure train time  0.11906599998474121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9416687488555908
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9417 | Train 0.1857 | Val 0.1140 | Test 0.0948
loading full batch data spends  0.0019500255584716797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632816314697266  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637584686279297  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

pure train time  0.11995244026184082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.039922237396240234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9446429014205933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9446 | Train 0.2000 | Val 0.1140 | Test 0.0953
loading full batch data spends  0.001996278762817383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619060516357422  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018623828887939453  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

pure train time  0.11590266227722168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9420326948165894
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9420 | Train 0.1857 | Val 0.1140 | Test 0.0928
loading full batch data spends  0.0019474029541015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019651412963867188  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01965618133544922  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

pure train time  0.12117981910705566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.039997100830078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9408730268478394
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9409 | Train 0.1714 | Val 0.1060 | Test 0.0919
loading full batch data spends  0.0019898414611816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018637657165527344  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018642425537109375  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.11648201942443848
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9419368505477905
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9419 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019338130950927734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018550395965576172  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018555164337158203  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.11767148971557617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9400397539138794
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9400 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.001990795135498047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018640518188476562  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018645286560058594  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.1087028980255127
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9379993677139282
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9380 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019338130950927734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019612789154052734  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019617557525634766  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.11722612380981445
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9357599020004272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9358 | Train 0.1786 | Val 0.1060 | Test 0.0919
loading full batch data spends  0.001943349838256836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018605709075927734  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018610477447509766  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.11699938774108887
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.929540991783142
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9295 | Train 0.1857 | Val 0.1020 | Test 0.0953
loading full batch data spends  0.0021164417266845703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01866436004638672  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01866912841796875  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.11614608764648438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.928994059562683
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9290 | Train 0.1714 | Val 0.1000 | Test 0.1006
loading full batch data spends  0.0020155906677246094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019631385803222656  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019636154174804688  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

pure train time  0.1169886589050293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.039999961853027344  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.934685230255127
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9347 | Train 0.1571 | Val 0.1040 | Test 0.1074
loading full batch data spends  0.0019485950469970703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018553733825683594  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018558502197265625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11577081680297852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9366625547409058
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9367 | Train 0.1643 | Val 0.1300 | Test 0.1156
loading full batch data spends  0.0019741058349609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018637657165527344  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018642425537109375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11024808883666992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9221465587615967
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9221 | Train 0.1643 | Val 0.1320 | Test 0.1219
loading full batch data spends  0.0019600391387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018511295318603516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018516063690185547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1094510555267334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.930147409439087
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9301 | Train 0.1571 | Val 0.1320 | Test 0.1267
loading full batch data spends  0.0019855499267578125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019635677337646484  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019640445709228516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10655069351196289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9228726625442505
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9229 | Train 0.1571 | Val 0.1300 | Test 0.1306
loading full batch data spends  0.0019347667694091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018618106842041016  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018622875213623047  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11878252029418945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9291011095046997
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9291 | Train 0.1786 | Val 0.1280 | Test 0.1132
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018573284149169922  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018578052520751953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11738777160644531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007733345031738281  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.916445016860962
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9164 | Train 0.1929 | Val 0.1280 | Test 0.1161
loading full batch data spends  0.0019421577453613281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018639087677001953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018643856048583984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1160886287689209
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9164520502090454
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9165 | Train 0.2071 | Val 0.1300 | Test 0.1165
loading full batch data spends  0.0019898414611816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018589496612548828  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859426498413086  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11287665367126465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007744312286376953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.913967490196228
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9140 | Train 0.2143 | Val 0.1320 | Test 0.1161
loading full batch data spends  0.0019321441650390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018517494201660156  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018522262573242188  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10718441009521484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.913151502609253
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9132 | Train 0.2214 | Val 0.1260 | Test 0.1151
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861715316772461  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10751199722290039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007738590240478516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9035099744796753
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9035 | Train 0.2286 | Val 0.1340 | Test 0.1228
loading full batch data spends  0.0019278526306152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019634246826171875  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019639015197753906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10733294486999512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9088557958602905
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9089 | Train 0.2143 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.001987934112548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018598556518554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860332489013672  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10658025741577148
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007733345031738281  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9082157611846924
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.9082 | Train 0.2143 | Val 0.1220 | Test 0.1151
loading full batch data spends  0.0019297599792480469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018638134002685547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018642902374267578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10750865936279297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9061284065246582
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9061 | Train 0.2071 | Val 0.1260 | Test 0.1151
loading full batch data spends  0.0019979476928710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863384246826172  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863861083984375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.12110352516174316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007771015167236328  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9048311710357666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9048 | Train 0.2071 | Val 0.1260 | Test 0.1151
loading full batch data spends  0.0019659996032714844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613815307617188  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861858367919922  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10756468772888184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9057093858718872
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9057 | Train 0.2000 | Val 0.1240 | Test 0.1136
loading full batch data spends  0.0020096302032470703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01962423324584961  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01962900161743164  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10530972480773926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9051213264465332
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.9051 | Train 0.2000 | Val 0.1240 | Test 0.1136
loading full batch data spends  0.0019354820251464844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018627643585205078  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863241195678711  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10849189758300781
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9028949737548828
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.9029 | Train 0.2000 | Val 0.1260 | Test 0.1141
loading full batch data spends  0.002011537551879883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018640995025634766  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018645763397216797  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.12412261962890625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9000440835952759
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.9000 | Train 0.2000 | Val 0.1260 | Test 0.1136
loading full batch data spends  0.002042055130004883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.5299530029296875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01864337921142578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018648147583007812  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1203470230102539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8977771997451782
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8978 | Train 0.2000 | Val 0.1260 | Test 0.1127
loading full batch data spends  0.0019991397857666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018626689910888672  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018631458282470703  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11255192756652832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077762603759765625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9009512662887573
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.9010 | Train 0.1929 | Val 0.1100 | Test 0.1069
loading full batch data spends  0.0019452571868896484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018600940704345703  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018605709075927734  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1103978157043457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.917092204093933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9171 | Train 0.2000 | Val 0.1100 | Test 0.1069
loading full batch data spends  0.002150297164916992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018547534942626953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018552303314208984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10488390922546387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007685184478759766  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.894773244857788
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8948 | Train 0.1929 | Val 0.1140 | Test 0.1069
loading full batch data spends  0.0019505023956298828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01853466033935547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0185394287109375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11809039115905762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8899375200271606
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8899 | Train 0.1929 | Val 0.1140 | Test 0.1064
loading full batch data spends  0.002168416976928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018544673919677734  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018549442291259766  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11379098892211914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007701396942138672  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8992851972579956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8993 | Train 0.1929 | Val 0.1140 | Test 0.1064
loading full batch data spends  0.001954317092895508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018597126007080078  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860189437866211  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1195228099822998
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.901980996131897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.9020 | Train 0.2000 | Val 0.1100 | Test 0.1064
loading full batch data spends  0.0021681785583496094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018634319305419922  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018639087677001953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1171572208404541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.894954800605774
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8950 | Train 0.2071 | Val 0.1120 | Test 0.1083
loading full batch data spends  0.0021266937255859375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018609046936035156  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613815307617188  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11873102188110352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.893761157989502
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.8938 | Train 0.2214 | Val 0.1120 | Test 0.1083
loading full batch data spends  0.002115488052368164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019642353057861328  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964712142944336  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11432528495788574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9067295789718628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.9067 | Train 0.2357 | Val 0.1040 | Test 0.1069
loading full batch data spends  0.0019588470458984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01862621307373047  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186309814453125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11666417121887207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.888274908065796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8883 | Train 0.2286 | Val 0.1040 | Test 0.1001
loading full batch data spends  0.0020990371704101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018598556518554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860332489013672  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11305379867553711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077495574951171875  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8846088647842407
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.8846 | Train 0.2286 | Val 0.1180 | Test 0.1059
loading full batch data spends  0.0021347999572753906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018633365631103516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018638134002685547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10040521621704102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8977066278457642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.8977 | Train 0.2357 | Val 0.1140 | Test 0.1069
loading full batch data spends  0.0021753311157226562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018538951873779297  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018543720245361328  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09834718704223633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007679939270019531  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8882030248641968
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.8882 | Train 0.2286 | Val 0.1060 | Test 0.1011
loading full batch data spends  0.0019981861114501953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018598079681396484  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018602848052978516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10224246978759766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8914213180541992
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8914 | Train 0.2357 | Val 0.1060 | Test 0.1020
loading full batch data spends  0.002187490463256836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019649028778076172  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019653797149658203  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09498739242553711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8845056295394897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8845 | Train 0.2357 | Val 0.1060 | Test 0.1020
loading full batch data spends  0.0019524097442626953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018630027770996094  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018634796142578125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09743094444274902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8857237100601196
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8857 | Train 0.2429 | Val 0.1160 | Test 0.1093
loading full batch data spends  0.0019311904907226562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963949203491211  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964426040649414  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10851836204528809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8972065448760986
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8972 | Train 0.2357 | Val 0.1140 | Test 0.1074
loading full batch data spends  0.0020287036895751953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.790855407714844e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858997344970703  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018594741821289062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10262346267700195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8817806243896484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8818 | Train 0.2357 | Val 0.1140 | Test 0.1074
loading full batch data spends  0.0020072460174560547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01864004135131836  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01864480972290039  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10103321075439453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8778884410858154
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8779 | Train 0.2286 | Val 0.1360 | Test 0.1238
loading full batch data spends  0.0019347667694091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018639087677001953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018643856048583984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10474848747253418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8865197896957397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8865 | Train 0.2286 | Val 0.1360 | Test 0.1238
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018589019775390625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018593788146972656  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10022163391113281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007738590240478516  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8864527940750122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8865 | Train 0.2214 | Val 0.1360 | Test 0.1228
loading full batch data spends  0.0019512176513671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018651485443115234  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018656253814697266  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10932278633117676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8865630626678467
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8866 | Train 0.2357 | Val 0.1300 | Test 0.1310
loading full batch data spends  0.0019834041595458984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018650054931640625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018654823303222656  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10654425621032715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077762603759765625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8862982988357544
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8863 | Train 0.2286 | Val 0.1260 | Test 0.1233
loading full batch data spends  0.0019421577453613281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632339477539062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637107849121094  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10036969184875488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8780162334442139
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8780 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019624710083007812  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019629478454589844  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10258722305297852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8836177587509155
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8836 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.0019686222076416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0185699462890625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01857471466064453  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10586166381835938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.879756212234497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8798 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.00200653076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858806610107422  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859283447265625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.0996408462524414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007744312286376953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8810535669326782
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8811 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859569549560547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186004638671875  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10539674758911133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.876072645187378
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8761 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.0020041465759277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861095428466797  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861572265625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10405850410461426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8763569593429565
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8764 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.001967191696166992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963949203491211  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964426040649414  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10899090766906738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.879573106765747
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8796 | Train 0.2214 | Val 0.1280 | Test 0.1190
loading full batch data spends  0.0019843578338623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858043670654297  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018585205078125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10185480117797852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077228546142578125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.874914526939392
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8749 | Train 0.2429 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019419193267822266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018574237823486328  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01857900619506836  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10918712615966797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8763879537582397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8764 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.001974344253540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018607616424560547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.11054587364196777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007730960845947266  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.876062273979187
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8761 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019540786743164062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018570899963378906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018575668334960938  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1007688045501709
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.876063346862793
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8761 | Train 0.2214 | Val 0.1280 | Test 0.1281
loading full batch data spends  0.0019922256469726562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019622802734375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01962757110595703  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10162997245788574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8743438720703125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8743 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019333362579345703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01857900619506836  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858377456665039  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10308003425598145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8779722452163696
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8780 | Train 0.2357 | Val 0.1300 | Test 0.1277
loading full batch data spends  0.001978635787963867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858997344970703  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018594741821289062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1006927490234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007709503173828125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8724578619003296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8725 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019664764404296875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863241195678711  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863718032836914  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1017606258392334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8712661266326904
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8713 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.002161741256713867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018649578094482422  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018654346466064453  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09610819816589355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8708914518356323
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8709 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019419193267822266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018617630004882812  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018622398376464844  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10797309875488281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.875031590461731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8750 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019915103912353516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018555164337158203  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018559932708740234  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1032865047454834
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007706642150878906  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8765498399734497
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8765 | Train 0.2286 | Val 0.1320 | Test 0.1291
loading full batch data spends  0.001949310302734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018647193908691406  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018651962280273438  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10376238822937012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8697688579559326
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8698 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019845962524414062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861715316772461  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.1018517017364502
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007744312286376953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8698558807373047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8699 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.001939535140991211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860189437866211  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860666275024414  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09868264198303223
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.869231939315796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8692 | Train 0.2214 | Val 0.1280 | Test 0.1281
loading full batch data spends  0.002014636993408203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863718032836914  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018641948699951172  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10233378410339355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007757663726806641  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.874795913696289
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8748 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019085407257080078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018611907958984375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018616676330566406  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10492324829101562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8737913370132446
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8738 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.001991748809814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018665313720703125  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018670082092285156  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10005426406860352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007784366607666016  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.872109055519104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8721 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019631385803222656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018577098846435547  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018581867218017578  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09937191009521484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8700038194656372
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8700 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.001993894577026367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632339477539062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637107849121094  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10041189193725586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.870664119720459
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8707 | Train 0.2214 | Val 0.1280 | Test 0.1281
loading full batch data spends  0.0019533634185791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018614768981933594  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619537353515625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.0991051197052002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8686610460281372
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8687 | Train 0.2429 | Val 0.1320 | Test 0.1286
loading full batch data spends  0.0019872188568115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019629955291748047  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019634723663330078  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09953761100769043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8731434345245361
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8731 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019443035125732422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018578052520751953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018582820892333984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09847593307495117
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8675628900527954
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8676 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019729137420654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619060516357422  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018623828887939453  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10060811042785645
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007760047912597656  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8691397905349731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8691 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019490718841552734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018548965454101562  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018553733825683594  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10302209854125977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8707175254821777
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8707 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0020093917846679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019613265991210938  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01961803436279297  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.09956169128417969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8684707880020142
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8685 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019533634185791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018639087677001953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018643856048583984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10070300102233887
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8677043914794922
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.8677 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.001997232437133789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019646167755126953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019650936126708984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10370111465454102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8668041229248047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.8668 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863384246826172  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863861083984375  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.0974736213684082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8683260679244995
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8683 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.00197601318359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619537353515625  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018624305725097656  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10428810119628906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007744312286376953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8651866912841797
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8652 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019562244415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018608570098876953  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613338470458984  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10537409782409668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8649133443832397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.8649 | Train 0.2214 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.001978158950805664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019659996032714844  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019664764404296875  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

pure train time  0.10631561279296875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008770942687988281  GigaBytes
Max Memory Allocated: 0.0409088134765625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8678562641143799
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8679 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019598007202148438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861715316772461  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10511231422424316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8682653903961182
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.8683 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.002087116241455078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.76837158203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613815307617188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861858367919922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10646414756774902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8654826879501343
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.8655 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019807815551757812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01853656768798828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018541336059570312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10478067398071289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8628088235855103
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.8628 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019991397857666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018610477447509766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018615245819091797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10706233978271484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8675957918167114
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8676 | Train 0.2429 | Val 0.1320 | Test 0.1286
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018590927124023438  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859569549560547  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10033488273620605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.86404550075531
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.8640 | Train 0.2286 | Val 0.1320 | Test 0.1296
loading full batch data spends  0.0019838809967041016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01855182647705078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018556594848632812  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10534834861755371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007701396942138672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8605765104293823
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.8606 | Train 0.2286 | Val 0.1300 | Test 0.1286
loading full batch data spends  0.0019462108612060547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963520050048828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019639968872070312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10681819915771484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8660157918930054
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.8660 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019633769989013672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019638538360595703  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09824347496032715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.861783742904663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.8618 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.002149343490600586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.363059997558594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861572265625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01862049102783203  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1064002513885498
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.865622878074646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.8656 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0019903182983398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018614768981933594  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619537353515625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09897708892822266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8699105978012085
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.8699 | Train 0.2357 | Val 0.1340 | Test 0.1301
loading full batch data spends  0.0021228790283203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186309814453125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863574981689453  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10104870796203613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8638948202133179
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.8639 | Train 0.2286 | Val 0.1340 | Test 0.1301
loading full batch data spends  0.0019390583038330078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019629478454589844  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019634246826171875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10586953163146973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8630911111831665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.8631 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0019452571868896484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963186264038086  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963663101196289  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10130190849304199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8590495586395264
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.8590 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0019409656524658203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613338470458984  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018618106842041016  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10737419128417969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8598004579544067
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.8598 | Train 0.2286 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.002131938934326172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861095428466797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861572265625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10760140419006348
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8648115396499634
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.8648 | Train 0.2429 | Val 0.1300 | Test 0.1296
loading full batch data spends  0.0021812915802001953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018599510192871094  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018604278564453125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.098785400390625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8605972528457642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.8606 | Train 0.2429 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.0021185874938964844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.38690185546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963949203491211  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964426040649414  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10446763038635254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.86468505859375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.8647 | Train 0.2357 | Val 0.1340 | Test 0.1301
loading full batch data spends  0.0019371509552001953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018631458282470703  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018636226654052734  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10209894180297852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007781505584716797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8604538440704346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.8605 | Train 0.2357 | Val 0.1300 | Test 0.1296
loading full batch data spends  0.002002716064453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186309814453125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863574981689453  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10638284683227539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8612079620361328
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.8612 | Train 0.2357 | Val 0.1340 | Test 0.1306
loading full batch data spends  0.0019958019256591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019617557525634766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019622325897216797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10303783416748047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8596183061599731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.8596 | Train 0.2500 | Val 0.1280 | Test 0.1301
loading full batch data spends  0.0018892288208007812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018602848052978516  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018607616424560547  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10447239875793457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.86135995388031
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.8614 | Train 0.2500 | Val 0.1300 | Test 0.1306
loading full batch data spends  0.0021784305572509766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963186264038086  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963663101196289  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10078001022338867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8568165302276611
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.8568 | Train 0.2500 | Val 0.1320 | Test 0.1315
loading full batch data spends  0.0018932819366455078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859903335571289  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018603801727294922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10286593437194824
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8641986846923828
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.8642 | Train 0.2429 | Val 0.1360 | Test 0.1335
loading full batch data spends  0.0019371509552001953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018597126007080078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860189437866211  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10130953788757324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077495574951171875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8597843647003174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.8598 | Train 0.2500 | Val 0.1380 | Test 0.1383
loading full batch data spends  0.0019614696502685547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861858367919922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01862335205078125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09977531433105469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8568239212036133
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.8568 | Train 0.2571 | Val 0.1380 | Test 0.1368
loading full batch data spends  0.0019872188568115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018608570098876953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613338470458984  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10092782974243164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8520047664642334
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.8520 | Train 0.2500 | Val 0.1400 | Test 0.1378
loading full batch data spends  0.0019407272338867188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018498897552490234  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018503665924072266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10408639907836914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8549199104309082
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.8549 | Train 0.2500 | Val 0.1440 | Test 0.1397
loading full batch data spends  0.001977682113647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019650936126708984  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019655704498291016  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10086941719055176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008770942687988281  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8520954847335815
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8521 | Train 0.2500 | Val 0.1480 | Test 0.1397
loading full batch data spends  0.001949310302734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019609928131103516  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019614696502685547  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1053318977355957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8508390188217163
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.8508 | Train 0.2500 | Val 0.1540 | Test 0.1407
loading full batch data spends  0.002040386199951172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018572330474853516  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018577098846435547  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1022953987121582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007738590240478516  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8500971794128418
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.8501 | Train 0.2429 | Val 0.1520 | Test 0.1402
loading full batch data spends  0.001984119415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018589496612548828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01859426498413086  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10481762886047363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8503518104553223
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.8504 | Train 0.2429 | Val 0.1520 | Test 0.1393
loading full batch data spends  0.002169370651245117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186309814453125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863574981689453  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10479879379272461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077762603759765625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8509085178375244
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.8509 | Train 0.2429 | Val 0.1520 | Test 0.1388
loading full batch data spends  0.0019495487213134766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632816314697266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637584686279297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10587358474731445
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.850527048110962
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.8505 | Train 0.2643 | Val 0.1580 | Test 0.1388
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019628047943115234  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632816314697266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10326838493347168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8542693853378296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.8543 | Train 0.2500 | Val 0.1560 | Test 0.1325
loading full batch data spends  0.0019702911376953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861095428466797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861572265625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10508489608764648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.848537802696228
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.8485 | Train 0.2429 | Val 0.1400 | Test 0.1335
loading full batch data spends  0.001988649368286133
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018605709075927734  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018610477447509766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10640621185302734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007771015167236328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8514225482940674
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.8514 | Train 0.2500 | Val 0.1280 | Test 0.1301
loading full batch data spends  0.001966238021850586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860523223876953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018610000610351562  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1045536994934082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8443093299865723
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.8443 | Train 0.2643 | Val 0.1280 | Test 0.1296
loading full batch data spends  0.002012491226196289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018562793731689453  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018567562103271484  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10175800323486328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077228546142578125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8463668823242188
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.8464 | Train 0.2643 | Val 0.1260 | Test 0.1315
loading full batch data spends  0.002137422561645508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637584686279297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019642353057861328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10930752754211426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8476355075836182
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.8476 | Train 0.2714 | Val 0.1240 | Test 0.1349
loading full batch data spends  0.0020024776458740234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018669605255126953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018674373626708984  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.11028075218200684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.00778961181640625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8402483463287354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.8402 | Train 0.2500 | Val 0.1380 | Test 0.1373
loading full batch data spends  0.001981496810913086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01850605010986328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018510818481445312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10785317420959473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.852361798286438
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.8524 | Train 0.2643 | Val 0.1480 | Test 0.1388
loading full batch data spends  0.0021767616271972656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019639968872070312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019644737243652344  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09992623329162598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8395154476165771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.8395 | Train 0.2714 | Val 0.1540 | Test 0.1407
loading full batch data spends  0.0019032955169677734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018659591674804688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01866436004638672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10384535789489746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8431235551834106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.8431 | Train 0.2714 | Val 0.1460 | Test 0.1422
loading full batch data spends  0.0019724369049072266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018583297729492188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858806610107422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10570406913757324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8508840799331665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.8509 | Train 0.2643 | Val 0.1380 | Test 0.1393
loading full batch data spends  0.0019030570983886719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019637584686279297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019642353057861328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10101437568664551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8523043394088745
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.8523 | Train 0.2857 | Val 0.1400 | Test 0.1393
loading full batch data spends  0.001974821090698242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861715316772461  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1055600643157959
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8435518741607666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.8436 | Train 0.2929 | Val 0.1520 | Test 0.1412
loading full batch data spends  0.0021123886108398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019647598266601562  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019652366638183594  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10550141334533691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8459885120391846
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.8460 | Train 0.2857 | Val 0.1560 | Test 0.1431
loading full batch data spends  0.0021657943725585938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861286163330078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018617630004882812  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10409379005432129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8318276405334473
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.8318 | Train 0.2857 | Val 0.1520 | Test 0.1422
loading full batch data spends  0.0018897056579589844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01856851577758789  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018573284149169922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09877181053161621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8440619707107544
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.8441 | Train 0.2786 | Val 0.1380 | Test 0.1402
loading full batch data spends  0.0021696090698242188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01962566375732422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963043212890625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09558749198913574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8478717803955078
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.8479 | Train 0.2643 | Val 0.1440 | Test 0.1378
loading full batch data spends  0.0019519329071044922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018580913543701172  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018585681915283203  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10166406631469727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8380476236343384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.8380 | Train 0.2714 | Val 0.1420 | Test 0.1412
loading full batch data spends  0.0019335746765136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01961803436279297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019622802734375  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10486221313476562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8470304012298584
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.8470 | Train 0.2857 | Val 0.1400 | Test 0.1412
loading full batch data spends  0.002131223678588867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018663406372070312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018668174743652344  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1003119945526123
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8391939401626587
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.8392 | Train 0.2929 | Val 0.1440 | Test 0.1412
loading full batch data spends  0.0021753311157226562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018581390380859375  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018586158752441406  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10130882263183594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8378801345825195
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.8379 | Train 0.3000 | Val 0.1460 | Test 0.1407
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01853179931640625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01853656768798828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10402679443359375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8258756399154663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.8259 | Train 0.2643 | Val 0.1440 | Test 0.1397
loading full batch data spends  0.0020127296447753906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861572265625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01862049102783203  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10136628150939941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8363791704177856
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.8364 | Train 0.2429 | Val 0.1460 | Test 0.1306
loading full batch data spends  0.0019838809967041016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018652915954589844  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018657684326171875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10264706611633301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8419889211654663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.8420 | Train 0.2429 | Val 0.1520 | Test 0.1296
loading full batch data spends  0.0019998550415039062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018644332885742188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01864910125732422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10422348976135254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077762603759765625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8467483520507812
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.8467 | Train 0.2500 | Val 0.1480 | Test 0.1407
loading full batch data spends  0.0019538402557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018605709075927734  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018610477447509766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10077452659606934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8412222862243652
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.8412 | Train 0.2643 | Val 0.1540 | Test 0.1446
loading full batch data spends  0.002027273178100586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018613815307617188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01861858367919922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1000216007232666
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.846805453300476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.8468 | Train 0.2571 | Val 0.1500 | Test 0.1388
loading full batch data spends  0.0019762516021728516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019633769989013672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019638538360595703  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1050868034362793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8390402793884277
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.8390 | Train 0.2571 | Val 0.1520 | Test 0.1339
loading full batch data spends  0.002115488052368164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.696846008300781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018587589263916016  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018592357635498047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10556244850158691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007738590240478516  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8375176191329956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.8375 | Train 0.2429 | Val 0.1540 | Test 0.1315
loading full batch data spends  0.002183675765991211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01865386962890625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01865863800048828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09666180610656738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8436566591262817
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.8437 | Train 0.2500 | Val 0.1600 | Test 0.1368
loading full batch data spends  0.0019979476928710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018597126007080078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860189437866211  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10755419731140137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007744312286376953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.837149739265442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.8371 | Train 0.2500 | Val 0.1420 | Test 0.1412
loading full batch data spends  0.0019686222076416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.743171691894531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018653392791748047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018658161163330078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10047721862792969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8405033349990845
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.8405 | Train 0.2429 | Val 0.1400 | Test 0.1436
loading full batch data spends  0.0019345283508300781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019628047943115234  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019632816314697266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10645723342895508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8415827751159668
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.8416 | Train 0.2357 | Val 0.1380 | Test 0.1412
loading full batch data spends  0.001961231231689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01857614517211914  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018580913543701172  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10370969772338867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8448565006256104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.8449 | Train 0.2643 | Val 0.1500 | Test 0.1417
loading full batch data spends  0.001985788345336914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018636226654052734  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018640995025634766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10490942001342773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8353710174560547
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.8354 | Train 0.2571 | Val 0.1340 | Test 0.1354
loading full batch data spends  0.001967906951904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018568992614746094  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018573760986328125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10614013671875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8391793966293335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.8392 | Train 0.2643 | Val 0.1340 | Test 0.1330
loading full batch data spends  0.0020079612731933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018625736236572266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018630504608154297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09984517097473145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.834415078163147
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.8344 | Train 0.2643 | Val 0.1360 | Test 0.1354
loading full batch data spends  0.0022149085998535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.506111145019531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018575668334960938  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01858043670654297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1121530532836914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8214699029922485
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.8215 | Train 0.2714 | Val 0.1340 | Test 0.1354
loading full batch data spends  0.002182483673095703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01965045928955078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019655227661132812  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10126256942749023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008770942687988281  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.827255129814148
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.8273 | Train 0.2571 | Val 0.1420 | Test 0.1373
loading full batch data spends  0.0019621849060058594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963949203491211  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964426040649414  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1058652400970459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8383233547210693
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.8383 | Train 0.2429 | Val 0.1420 | Test 0.1407
loading full batch data spends  0.0019919872283935547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018607616424560547  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018612384796142578  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10432243347167969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007754802703857422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.829407811164856
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.8294 | Train 0.2714 | Val 0.1360 | Test 0.1422
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018627166748046875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018631935119628906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10256266593933105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.837494134902954
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.8375 | Train 0.2571 | Val 0.1380 | Test 0.1397
loading full batch data spends  0.0019385814666748047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863718032836914  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018641948699951172  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.11095714569091797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007786750793457031  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8391844034194946
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.8392 | Train 0.2714 | Val 0.1400 | Test 0.1407
loading full batch data spends  0.0019729137420654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018631935119628906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018636703491210938  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10669493675231934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8095098733901978
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.8095 | Train 0.2929 | Val 0.1440 | Test 0.1417
loading full batch data spends  0.001995086669921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018622398376464844  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018627166748046875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10164189338684082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.805384635925293
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.8054 | Train 0.3000 | Val 0.1480 | Test 0.1373
loading full batch data spends  0.001961946487426758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018559932708740234  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018564701080322266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10292625427246094
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8141459226608276
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.8141 | Train 0.3071 | Val 0.1540 | Test 0.1388
loading full batch data spends  0.0020172595977783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01965045928955078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019655227661132812  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10245895385742188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8121675252914429
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.8122 | Train 0.3143 | Val 0.1540 | Test 0.1446
loading full batch data spends  0.0019519329071044922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019631385803222656  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019636154174804688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10849714279174805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7867056131362915
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.7867 | Train 0.3214 | Val 0.1640 | Test 0.1494
loading full batch data spends  0.001996278762817383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019648075103759766  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019652843475341797  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10235023498535156
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8038636445999146
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.8039 | Train 0.3429 | Val 0.1540 | Test 0.1514
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019623756408691406  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019628524780273438  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10081291198730469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7825313806533813
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.7825 | Train 0.3214 | Val 0.1500 | Test 0.1489
loading full batch data spends  0.001989603042602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0186004638671875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01860523223876953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09934473037719727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.0077495574951171875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7933191061019897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.7933 | Train 0.3429 | Val 0.1640 | Test 0.1610
loading full batch data spends  0.001943349838256836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018644332885742188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01864910125732422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10774564743041992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.803493857383728
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.8035 | Train 0.3500 | Val 0.1640 | Test 0.1601
loading full batch data spends  0.0034296512603759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.9591064453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018568992614746094  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018573760986328125  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.12517023086547852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.787820816040039
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.7878 | Train 0.3643 | Val 0.1780 | Test 0.1649
loading full batch data spends  0.0022029876708984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.220008850097656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01856708526611328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018571853637695312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10612106323242188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7817929983139038
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.7818 | Train 0.2857 | Val 0.1540 | Test 0.1485
loading full batch data spends  0.0023000240325927734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.981590270996094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018619060516357422  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018623828887939453  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.11063432693481445
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007741451263427734  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7918040752410889
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.7918 | Train 0.3857 | Val 0.1900 | Test 0.1973
loading full batch data spends  0.002160310745239258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.910064697265625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019636154174804688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01964092254638672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10403680801391602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.793878197669983
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.7939 | Train 0.3214 | Val 0.1840 | Test 0.1712
loading full batch data spends  0.002174854278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01865100860595703  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018655776977539062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10277771949768066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007768154144287109  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7901169061660767
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.7901 | Train 0.2929 | Val 0.1740 | Test 0.1610
loading full batch data spends  0.002200603485107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01856708526611328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018571853637695312  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10111021995544434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8300620317459106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.8301 | Train 0.2929 | Val 0.1640 | Test 0.1572
loading full batch data spends  0.0019898414611816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018595218658447266  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018599987030029297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10094690322875977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007733345031738281  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7861028909683228
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.7861 | Train 0.2929 | Val 0.1740 | Test 0.1591
loading full batch data spends  0.002002239227294922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019649505615234375  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019654273986816406  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10541653633117676
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8211325407028198
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.8211 | Train 0.3143 | Val 0.1800 | Test 0.1634
loading full batch data spends  0.0019342899322509766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018591880798339844  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018596649169921875  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09957385063171387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8061325550079346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.8061 | Train 0.3500 | Val 0.1780 | Test 0.1683
loading full batch data spends  0.001909017562866211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018626689910888672  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018631458282470703  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10528683662414551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7644599676132202
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.7645 | Train 0.3857 | Val 0.1820 | Test 0.1775
loading full batch data spends  0.002187967300415039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018586158752441406  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018590927124023438  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10414528846740723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007728099822998047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7841180562973022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.7841 | Train 0.3857 | Val 0.1820 | Test 0.1862
loading full batch data spends  0.0019583702087402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018634319305419922  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018639087677001953  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10838842391967773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007859230041503906  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.786773443222046
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.7868 | Train 0.3571 | Val 0.2020 | Test 0.1896
loading full batch data spends  0.0021882057189941406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963043212890625  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01963520050048828  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09886670112609863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7685600519180298
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.7686 | Train 0.3571 | Val 0.2060 | Test 0.1852
loading full batch data spends  0.0019450187683105469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019646644592285156  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019651412963867188  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10424566268920898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7962417602539062
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.7962 | Train 0.3714 | Val 0.2080 | Test 0.1963
loading full batch data spends  0.0021750926971435547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863241195678711  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01863718032836914  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10580778121948242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007765293121337891  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7781283855438232
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.7781 | Train 0.3929 | Val 0.2120 | Test 0.1992
loading full batch data spends  0.0021355152130126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018630504608154297  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018635272979736328  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10595035552978516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7498486042022705
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.7498 | Train 0.4000 | Val 0.1780 | Test 0.1867
loading full batch data spends  0.002042531967163086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019660472869873047  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019665241241455078  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.10695147514343262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008770942687988281  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7655588388442993
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.7656 | Train 0.3929 | Val 0.1800 | Test 0.1779
loading full batch data spends  0.002135038375854492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.01848459243774414  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.018489360809326172  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.09472489356994629
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.007856369018554688  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7386276721954346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.7386 | Train 0.3714 | Val 0.1820 | Test 0.1683
loading full batch data spends  0.001981019973754883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019618511199951172  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.019623279571533203  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

pure train time  0.1043703556060791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.583984375 GB
    Memory Allocated: 0.008768081665039062  GigaBytes
Max Memory Allocated: 0.04091167449951172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.754387378692627
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 1, 16])
h.size() torch.Size([2708, 16])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.7544 | Train 0.3786 | Val 0.1840 | Test 0.1678
Total (block generation + training)time/epoch 0.11704941868782043
pure train time/epoch 0.10570925717451135

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368]
