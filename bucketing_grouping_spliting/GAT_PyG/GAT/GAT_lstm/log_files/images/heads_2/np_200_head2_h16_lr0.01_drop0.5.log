main start at this time 1696111876.2224143
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.002611875534057617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.546875 GB
    Memory Allocated: 0.021228313446044922  GigaBytes
Max Memory Allocated: 0.02135753631591797  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.546875 GB
    Memory Allocated: 0.021233081817626953  GigaBytes
Max Memory Allocated: 0.02135753631591797  GigaBytes

pure train time  0.49181318283081055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5546875 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.02135753631591797  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9528695344924927
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9529 | Train 0.1429 | Val 0.1480 | Test 0.1620
loading full batch data spends  0.00200653076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021819114685058594  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021823883056640625  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

pure train time  0.10934996604919434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008168220520019531  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9500203132629395
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9500 | Train 0.1429 | Val 0.1420 | Test 0.1426
loading full batch data spends  0.0019512176513671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021895885467529297  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021900653839111328  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

pure train time  0.10865664482116699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.042006492614746094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9458184242248535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9458 | Train 0.1571 | Val 0.1660 | Test 0.1456
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021898746490478516  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021903514862060547  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

pure train time  0.12110042572021484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9461957216262817
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9462 | Train 0.1857 | Val 0.1480 | Test 0.1272
loading full batch data spends  0.0019931793212890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021923542022705078  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192831039428711  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

pure train time  0.11763286590576172
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9473029375076294
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9473 | Train 0.1643 | Val 0.1340 | Test 0.1204
loading full batch data spends  0.0020024776458740234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02189922332763672  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02190399169921875  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

pure train time  0.11766219139099121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9430570602416992
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9431 | Train 0.2000 | Val 0.1440 | Test 0.1136
loading full batch data spends  0.0019528865814208984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299022674560547  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0229949951171875  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

pure train time  0.1139993667602539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04203319549560547  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9452767372131348
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9453 | Train 0.2071 | Val 0.1420 | Test 0.1146
loading full batch data spends  0.002007722854614258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021968364715576172  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

pure train time  0.11027765274047852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9428831338882446
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9429 | Train 0.1857 | Val 0.1320 | Test 0.1146
loading full batch data spends  0.002036571502685547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023009300231933594  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023014068603515625  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

pure train time  0.12232112884521484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04210805892944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.936929702758789
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9369 | Train 0.1786 | Val 0.1240 | Test 0.1127
loading full batch data spends  0.002011537551879883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199554443359375  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02200031280517578  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.1187286376953125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.940555214881897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9406 | Train 0.1643 | Val 0.1260 | Test 0.1127
loading full batch data spends  0.0019690990447998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021894454956054688  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02189922332763672  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.10992741584777832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9344103336334229
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9344 | Train 0.1643 | Val 0.1260 | Test 0.1127
loading full batch data spends  0.002018451690673828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021976470947265625  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021981239318847656  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.12129354476928711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9391416311264038
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9391 | Train 0.1643 | Val 0.1220 | Test 0.1098
loading full batch data spends  0.001950979232788086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022964954376220703  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022969722747802734  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.10762524604797363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9361604452133179
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9362 | Train 0.1643 | Val 0.1180 | Test 0.1074
loading full batch data spends  0.001995086669921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021942138671875  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194690704345703  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.1077423095703125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9263159036636353
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9263 | Train 0.1643 | Val 0.1180 | Test 0.1074
loading full batch data spends  0.0019807815551757812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02202463150024414  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022029399871826172  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.11343050003051758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9256776571273804
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9257 | Train 0.1714 | Val 0.1200 | Test 0.1093
loading full batch data spends  0.0021746158599853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02297687530517578  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022981643676757812  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

pure train time  0.11752009391784668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04211091995239258  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9283909797668457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9284 | Train 0.1714 | Val 0.1240 | Test 0.1069
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021897315979003906  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021902084350585938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11934137344360352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.933725357055664
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9337 | Train 0.1786 | Val 0.1240 | Test 0.1103
loading full batch data spends  0.0020079612731933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021979331970214844  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021984100341796875  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10863804817199707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9216358661651611
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9216 | Train 0.2214 | Val 0.1140 | Test 0.1093
loading full batch data spends  0.001984119415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021869182586669922  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021873950958251953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11090207099914551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9312151670455933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9312 | Train 0.2286 | Val 0.1260 | Test 0.1136
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299976348876953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023004531860351562  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11111831665039062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9151787757873535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9152 | Train 0.2071 | Val 0.1260 | Test 0.1170
loading full batch data spends  0.0021402835845947266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021965980529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021970748901367188  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10677075386047363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.931854486465454
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9319 | Train 0.2143 | Val 0.1300 | Test 0.1132
loading full batch data spends  0.002000093460083008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021897315979003906  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021902084350585938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11949038505554199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008168220520019531  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9097015857696533
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9097 | Train 0.2286 | Val 0.1320 | Test 0.1161
loading full batch data spends  0.0019822120666503906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0220184326171875  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02202320098876953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11841678619384766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9112662076950073
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9113 | Train 0.2214 | Val 0.1280 | Test 0.1165
loading full batch data spends  0.0020194053649902344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194356918334961  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194833755493164  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1194448471069336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008179187774658203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9015568494796753
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9016 | Train 0.2071 | Val 0.1300 | Test 0.1127
loading full batch data spends  0.001951456069946289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021857261657714844  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021862030029296875  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11101102828979492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9062010049819946
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9062 | Train 0.2143 | Val 0.1280 | Test 0.1165
loading full batch data spends  0.0020215511322021484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021966934204101562  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021971702575683594  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10844993591308594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008173465728759766  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9079618453979492
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9080 | Train 0.2071 | Val 0.1280 | Test 0.1141
loading full batch data spends  0.001998424530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022998809814453125  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023003578186035156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11673235893249512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9029390811920166
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9029 | Train 0.2000 | Val 0.1280 | Test 0.1122
loading full batch data spends  0.0020132064819335938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021928787231445312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021933555603027344  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11974143981933594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008168220520019531  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8953593969345093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.8954 | Train 0.2000 | Val 0.1280 | Test 0.1122
loading full batch data spends  0.0020368099212646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.695487976074219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021996021270751953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022000789642333984  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11933326721191406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.904754400253296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9048 | Train 0.1929 | Val 0.1260 | Test 0.1107
loading full batch data spends  0.0020360946655273438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021991252899169922  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021996021270751953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11249923706054688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008205890655517578  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9137425422668457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9137 | Train 0.1929 | Val 0.1260 | Test 0.1107
loading full batch data spends  0.0019702911376953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196788787841797  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197265625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11730408668518066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9072902202606201
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9073 | Train 0.1929 | Val 0.1260 | Test 0.1103
loading full batch data spends  0.0019872188568115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298259735107422  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298736572265625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10775494575500488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9125878810882568
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.9126 | Train 0.1929 | Val 0.1240 | Test 0.1093
loading full batch data spends  0.0019578933715820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197265625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197742462158203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11865448951721191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8961671590805054
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.8962 | Train 0.1929 | Val 0.1240 | Test 0.1093
loading full batch data spends  0.002044200897216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021985530853271484  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021990299224853516  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11567330360412598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9028186798095703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.9028 | Train 0.2000 | Val 0.1200 | Test 0.1069
loading full batch data spends  0.0019779205322265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021997451782226562  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022002220153808594  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10790085792541504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8926517963409424
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8927 | Train 0.1857 | Val 0.1140 | Test 0.1030
loading full batch data spends  0.0020101070404052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021991729736328125  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021996498107910156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.12041020393371582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008211135864257812  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8784589767456055
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8785 | Train 0.1857 | Val 0.1140 | Test 0.1030
loading full batch data spends  0.0019817352294921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.315376281738281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194356918334961  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194833755493164  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.12266778945922852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9180049896240234
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9180 | Train 0.1929 | Val 0.1120 | Test 0.1035
loading full batch data spends  0.002089262008666992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0218963623046875  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02190113067626953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11082029342651367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008120059967041016  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.897098422050476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8971 | Train 0.1929 | Val 0.1140 | Test 0.1054
loading full batch data spends  0.001997709274291992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021856307983398438  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02186107635498047  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10163474082946777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8901172876358032
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8901 | Train 0.1929 | Val 0.1160 | Test 0.1059
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021886348724365234  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021891117095947266  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1042928695678711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008136272430419922  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8983594179153442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8984 | Train 0.1929 | Val 0.1160 | Test 0.1006
loading full batch data spends  0.0021386146545410156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021961688995361328  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196645736694336  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10987138748168945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.894709587097168
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.8947 | Train 0.2000 | Val 0.1160 | Test 0.1006
loading full batch data spends  0.0020339488983154297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021960735321044922  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021965503692626953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10654568672180176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.894339680671692
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8943 | Train 0.2000 | Val 0.1140 | Test 0.1020
loading full batch data spends  0.0019526481628417969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196979522705078  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021974563598632812  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10647201538085938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9024244546890259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.9024 | Train 0.2000 | Val 0.1120 | Test 0.1030
loading full batch data spends  0.0020177364349365234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022990703582763672  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022995471954345703  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10081315040588379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9153300523757935
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.9153 | Train 0.2071 | Val 0.1080 | Test 0.1030
loading full batch data spends  0.001987457275390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021968364715576172  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10414552688598633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8911901712417603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8912 | Train 0.2214 | Val 0.1080 | Test 0.1030
loading full batch data spends  0.002069711685180664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021950721740722656  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021955490112304688  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1056525707244873
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008184432983398438  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.882049560546875
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.8820 | Train 0.2214 | Val 0.1140 | Test 0.1059
loading full batch data spends  0.0019533634185791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021958351135253906  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021963119506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10331583023071289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8982306718826294
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.8982 | Train 0.2286 | Val 0.1180 | Test 0.1103
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021885395050048828  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02189016342163086  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.09709477424621582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008114814758300781  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.899505376815796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.8995 | Train 0.2286 | Val 0.1160 | Test 0.1103
loading full batch data spends  0.002165555953979492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194833755493164  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021953105926513672  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10328817367553711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8877766132354736
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8878 | Train 0.2286 | Val 0.1160 | Test 0.1093
loading full batch data spends  0.00205230712890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0229949951171875  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299976348876953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.09850859642028809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.879266381263733
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8793 | Train 0.2357 | Val 0.1140 | Test 0.1064
loading full batch data spends  0.001966714859008789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199411392211914  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021998882293701172  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10826468467712402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.886875033378601
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8869 | Train 0.2429 | Val 0.1180 | Test 0.1141
loading full batch data spends  0.0020415782928466797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022991657257080078  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299642562866211  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10392355918884277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8965903520584106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8966 | Train 0.2786 | Val 0.1240 | Test 0.1204
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021936893463134766  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021941661834716797  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10143494606018066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8882348537445068
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8882 | Train 0.2786 | Val 0.1340 | Test 0.1262
loading full batch data spends  0.0020329952239990234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197742462158203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021982192993164062  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10646986961364746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8755968809127808
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8756 | Train 0.2643 | Val 0.1320 | Test 0.1190
loading full batch data spends  0.0019714832305908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02200460433959961  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02200937271118164  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10744714736938477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8875638246536255
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8876 | Train 0.2571 | Val 0.1260 | Test 0.1190
loading full batch data spends  0.002015352249145508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021945953369140625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021950721740722656  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10446596145629883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008173465728759766  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8899694681167603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8900 | Train 0.2714 | Val 0.1280 | Test 0.1243
loading full batch data spends  0.001961946487426758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022021770477294922  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022026538848876953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10576176643371582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8786907196044922
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8787 | Train 0.2571 | Val 0.1180 | Test 0.1339
loading full batch data spends  0.0020296573638916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022013187408447266  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022017955780029297  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10873866081237793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008211135864257812  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8853611946105957
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8854 | Train 0.2714 | Val 0.1280 | Test 0.1272
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02301168441772461  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02301645278930664  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11518263816833496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8793500661849976
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8794 | Train 0.2929 | Val 0.1300 | Test 0.1310
loading full batch data spends  0.0020036697387695312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022980690002441406  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022985458374023438  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10082197189331055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8815815448760986
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8816 | Train 0.2929 | Val 0.1220 | Test 0.1315
loading full batch data spends  0.0021424293518066406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021912574768066406  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021917343139648438  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10072660446166992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8727388381958008
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8727 | Train 0.2929 | Val 0.1280 | Test 0.1306
loading full batch data spends  0.0020263195037841797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021934986114501953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021939754486083984  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10161590576171875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008179187774658203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8754364252090454
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8754 | Train 0.3000 | Val 0.1300 | Test 0.1320
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021945476531982422  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021950244903564453  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10278916358947754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8752243518829346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8752 | Train 0.2857 | Val 0.1300 | Test 0.1257
loading full batch data spends  0.0020232200622558594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195262908935547  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0219573974609375  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10092902183532715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.849884033203125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8499 | Train 0.2714 | Val 0.1240 | Test 0.1339
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02301025390625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02301502227783203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10306572914123535
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.874659538269043
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8747 | Train 0.2786 | Val 0.1240 | Test 0.1344
loading full batch data spends  0.002038240432739258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192401885986328  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021928787231445312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10028529167175293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008157730102539062  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8831584453582764
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8832 | Train 0.2500 | Val 0.1200 | Test 0.1335
loading full batch data spends  0.001960277557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021918773651123047  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021923542022705078  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10583710670471191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8691284656524658
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8691 | Train 0.2429 | Val 0.1260 | Test 0.1325
loading full batch data spends  0.0020172595977783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197742462158203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021982192993164062  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1068108081817627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008165836334228516  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8761115074157715
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8761 | Train 0.2571 | Val 0.1340 | Test 0.1325
loading full batch data spends  0.0019834041595458984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021915912628173828  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192068099975586  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10044479370117188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8640371561050415
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8640 | Train 0.2429 | Val 0.1360 | Test 0.1330
loading full batch data spends  0.002004384994506836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022966861724853516  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022971630096435547  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10633134841918945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8655282258987427
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8655 | Train 0.2429 | Val 0.1380 | Test 0.1325
loading full batch data spends  0.0021429061889648438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021914958953857422  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021919727325439453  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.09982514381408691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8666235208511353
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8666 | Train 0.2429 | Val 0.1360 | Test 0.1364
loading full batch data spends  0.00201416015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021945476531982422  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021950244903564453  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10158586502075195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008144378662109375  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8691351413726807
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8691 | Train 0.2571 | Val 0.1460 | Test 0.1320
loading full batch data spends  0.0019669532775878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021972179412841797  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021976947784423828  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1015629768371582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8488216400146484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8488 | Train 0.2857 | Val 0.1480 | Test 0.1335
loading full batch data spends  0.0020194053649902344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199697494506836  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02200174331665039  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10185837745666504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.867287516593933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8673 | Train 0.2714 | Val 0.1460 | Test 0.1349
loading full batch data spends  0.001965045928955078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197408676147461  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197885513305664  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10511422157287598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.848262906074524
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8483 | Train 0.2571 | Val 0.1400 | Test 0.1330
loading full batch data spends  0.002019643783569336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021896839141845703  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021901607513427734  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10551309585571289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008141517639160156  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8518847227096558
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8519 | Train 0.2714 | Val 0.1360 | Test 0.1310
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021988868713378906  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021993637084960938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10317754745483398
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8464810848236084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8465 | Train 0.2714 | Val 0.1360 | Test 0.1330
loading full batch data spends  0.002000570297241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.981590270996094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196645736694336  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197122573852539  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10419034957885742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008179187774658203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.850101113319397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8501 | Train 0.2500 | Val 0.1360 | Test 0.1335
loading full batch data spends  0.0019812583923339844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021944046020507812  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021948814392089844  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10190701484680176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.849518895149231
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8495 | Train 0.2786 | Val 0.1380 | Test 0.1359
loading full batch data spends  0.002115011215209961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.6253204345703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022002220153808594  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022006988525390625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10317206382751465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00819253921508789  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.839544415473938
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8395 | Train 0.2714 | Val 0.1320 | Test 0.1339
loading full batch data spends  0.0020177364349365234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196216583251953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021966934204101562  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10263514518737793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8649483919143677
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8649 | Train 0.2571 | Val 0.1360 | Test 0.1349
loading full batch data spends  0.0020225048065185547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022029399871826172  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022034168243408203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.11063599586486816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008219242095947266  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8505151271820068
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8505 | Train 0.2571 | Val 0.1440 | Test 0.1378
loading full batch data spends  0.001973390579223633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02191925048828125  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192401885986328  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10043907165527344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8395390510559082
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8395 | Train 0.2571 | Val 0.1460 | Test 0.1397
loading full batch data spends  0.0020389556884765625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022979736328125  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298450469970703  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.0975034236907959
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8411442041397095
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8411 | Train 0.2571 | Val 0.1440 | Test 0.1456
loading full batch data spends  0.0019867420196533203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195882797241211  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1038064956665039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8350616693496704
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8351 | Train 0.2929 | Val 0.1400 | Test 0.1470
loading full batch data spends  0.0019609928131103516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022991657257080078  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299642562866211  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10773730278015137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.812505841255188
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8125 | Train 0.3143 | Val 0.1520 | Test 0.1518
loading full batch data spends  0.0025000572204589844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.9604644775390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02190685272216797  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02191162109375  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10010886192321777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8217194080352783
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8217 | Train 0.2929 | Val 0.1440 | Test 0.1572
loading full batch data spends  0.0020265579223632812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196979522705078  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021974563598632812  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10057783126831055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008194923400878906  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8121265172958374
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8121 | Train 0.3071 | Val 0.1540 | Test 0.1547
loading full batch data spends  0.001924276351928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02187967300415039  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021884441375732422  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10938048362731934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8213763236999512
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8214 | Train 0.2929 | Val 0.1540 | Test 0.1518
loading full batch data spends  0.0020203590393066406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022950172424316406  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022954940795898438  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10114455223083496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8244432210922241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8244 | Train 0.2500 | Val 0.1260 | Test 0.1170
loading full batch data spends  0.0019910335540771484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021992206573486328  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199697494506836  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10468459129333496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.810102939605713
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.8101 | Train 0.3143 | Val 0.1600 | Test 0.1552
loading full batch data spends  0.0020275115966796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298736572265625  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299213409423828  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.1083211898803711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7871192693710327
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.7871 | Train 0.2929 | Val 0.1560 | Test 0.1480
loading full batch data spends  0.0019714832305908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021965503692626953  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021970272064208984  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10655713081359863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8391644954681396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8392 | Train 0.2929 | Val 0.1620 | Test 0.1489
loading full batch data spends  0.0020101070404052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021977901458740234  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021982669830322266  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10207223892211914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008179187774658203  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8303608894348145
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8304 | Train 0.3000 | Val 0.1620 | Test 0.1470
loading full batch data spends  0.00196075439453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021953105926513672  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021957874298095703  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10679864883422852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8098011016845703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.8098 | Train 0.3071 | Val 0.1680 | Test 0.1475
loading full batch data spends  0.002310514450073242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.172325134277344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02303028106689453  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023035049438476562  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

pure train time  0.10480475425720215
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009205818176269531  GigaBytes
Max Memory Allocated: 0.043019771575927734  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8311188220977783
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8311 | Train 0.3429 | Val 0.1780 | Test 0.1533
loading full batch data spends  0.0021543502807617188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195596694946289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021960735321044922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10693883895874023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8136587142944336
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.8137 | Train 0.3500 | Val 0.1680 | Test 0.1654
loading full batch data spends  0.0020411014556884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021954059600830078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195882797241211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10060620307922363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.768437385559082
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.7684 | Train 0.3357 | Val 0.1560 | Test 0.1538
loading full batch data spends  0.0019795894622802734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02188396453857422  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02188873291015625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10940241813659668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.775272250175476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.7753 | Train 0.3429 | Val 0.1540 | Test 0.1533
loading full batch data spends  0.002211332321166992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194690704345703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021951675415039062  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10065722465515137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8450312614440918
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8450 | Train 0.3571 | Val 0.1580 | Test 0.1538
loading full batch data spends  0.001997709274291992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195882797241211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.11093878746032715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7774385213851929
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.7774 | Train 0.3429 | Val 0.1680 | Test 0.1688
loading full batch data spends  0.0021882057189941406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02190256118774414  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021907329559326172  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09986448287963867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008136272430419922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7679426670074463
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.7679 | Train 0.3143 | Val 0.1640 | Test 0.1649
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298736572265625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299213409423828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10810446739196777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.809705138206482
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.8097 | Train 0.3643 | Val 0.1740 | Test 0.1663
loading full batch data spends  0.001994609832763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022984027862548828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298879623413086  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10977578163146973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7656095027923584
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.7656 | Train 0.3571 | Val 0.1740 | Test 0.1750
loading full batch data spends  0.0019714832305908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021976947784423828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02198171615600586  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1055748462677002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7678147554397583
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.7678 | Train 0.3786 | Val 0.1920 | Test 0.1842
loading full batch data spends  0.00202178955078125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197408676147461  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197885513305664  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1023111343383789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.760462760925293
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.7605 | Train 0.3929 | Val 0.1900 | Test 0.1833
loading full batch data spends  0.0019834041595458984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021968364715576172  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10470461845397949
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.77964448928833
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.7796 | Train 0.3929 | Val 0.1900 | Test 0.1881
loading full batch data spends  0.0035245418548583984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   6.771087646484375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022985458374023438  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299022674560547  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.14955687522888184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.753365159034729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.7534 | Train 0.4000 | Val 0.1960 | Test 0.1891
loading full batch data spends  0.0019233226776123047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298879623413086  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299356460571289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10730910301208496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7370474338531494
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.7370 | Train 0.4143 | Val 0.2140 | Test 0.1934
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021941661834716797  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021946430206298828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10574173927307129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7298119068145752
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.7298 | Train 0.4000 | Val 0.2200 | Test 0.2079
loading full batch data spends  0.0020186901092529297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194976806640625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195453643798828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10593318939208984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7403532266616821
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.7404 | Train 0.3500 | Val 0.2180 | Test 0.2045
loading full batch data spends  0.002027750015258789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021964550018310547  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021969318389892578  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10022974014282227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.745751142501831
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.7458 | Train 0.3643 | Val 0.2180 | Test 0.2070
loading full batch data spends  0.001981973648071289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299213409423828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022996902465820312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10588788986206055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.737654209136963
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.7377 | Train 0.3857 | Val 0.2300 | Test 0.2205
loading full batch data spends  0.002048492431640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021970748901367188  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197551727294922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10449957847595215
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008216381072998047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7450950145721436
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.7451 | Train 0.3857 | Val 0.2260 | Test 0.2200
loading full batch data spends  0.002168416976928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021988391876220703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021993160247802734  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09931802749633789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.728866696357727
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.7289 | Train 0.3857 | Val 0.2280 | Test 0.2205
loading full batch data spends  0.0027446746826171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   0.0001323223114013672
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022953033447265625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022957801818847656  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10260510444641113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7693920135498047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.7694 | Train 0.3929 | Val 0.2240 | Test 0.2171
loading full batch data spends  0.0019783973693847656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195119857788086  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195596694946289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1061863899230957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7123138904571533
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.7123 | Train 0.3929 | Val 0.2260 | Test 0.2099
loading full batch data spends  0.0025207996368408203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.698204040527344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022986888885498047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022991657257080078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09599423408508301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6779890060424805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.6780 | Train 0.4000 | Val 0.2260 | Test 0.2142
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195119857788086  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195596694946289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10207414627075195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.770020604133606
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.7700 | Train 0.4429 | Val 0.2300 | Test 0.2191
loading full batch data spends  0.0020236968994140625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021944522857666016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021949291229248047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10524296760559082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008184432983398438  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.733743667602539
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.7337 | Train 0.4071 | Val 0.2280 | Test 0.2055
loading full batch data spends  0.0019221305847167969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196979522705078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021974563598632812  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10520362854003906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7487205266952515
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.7487 | Train 0.4143 | Val 0.2160 | Test 0.2191
loading full batch data spends  0.002007722854614258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021949291229248047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021954059600830078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10570955276489258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7279107570648193
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.7279 | Train 0.4357 | Val 0.2080 | Test 0.2045
loading full batch data spends  0.0019872188568115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021820068359375  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02182483673095703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.100494384765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7926883697509766
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.7927 | Train 0.3786 | Val 0.2020 | Test 0.1784
loading full batch data spends  0.002001523971557617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023011207580566406  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023015975952148438  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10123682022094727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009205818176269531  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.806343674659729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8063 | Train 0.3643 | Val 0.1940 | Test 0.1779
loading full batch data spends  0.0019626617431640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022937774658203125  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022942543029785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10669755935668945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7588574886322021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.7589 | Train 0.4071 | Val 0.2000 | Test 0.1896
loading full batch data spends  0.0020644664764404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021929264068603516  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021934032440185547  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10118865966796875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008173465728759766  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7363783121109009
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.7364 | Train 0.4500 | Val 0.2180 | Test 0.2234
loading full batch data spends  0.0019524097442626953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02193164825439453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021936416625976562  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10666012763977051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.711229920387268
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.7112 | Train 0.4500 | Val 0.2660 | Test 0.2534
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021991252899169922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021996021270751953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10775184631347656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008211135864257812  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7415823936462402
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.7416 | Train 0.4000 | Val 0.2100 | Test 0.2026
loading full batch data spends  0.001974344253540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022987842559814453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022992610931396484  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10324501991271973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7731199264526367
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.7731 | Train 0.4000 | Val 0.2000 | Test 0.2055
loading full batch data spends  0.002180337905883789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022991657257080078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299642562866211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09868836402893066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7129086256027222
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.7129 | Train 0.4286 | Val 0.2440 | Test 0.2379
loading full batch data spends  0.001951456069946289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021979331970214844  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021984100341796875  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10520577430725098
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7348389625549316
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.7348 | Train 0.4357 | Val 0.2400 | Test 0.2408
loading full batch data spends  0.002011537551879883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021941661834716797  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021946430206298828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10218501091003418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008205890655517578  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6956361532211304
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.6956 | Train 0.4286 | Val 0.2200 | Test 0.2205
loading full batch data spends  0.0019910335540771484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021934986114501953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021939754486083984  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10291504859924316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7237274646759033
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.7237 | Train 0.4357 | Val 0.2040 | Test 0.2036
loading full batch data spends  0.002001047134399414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021906375885009766  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021911144256591797  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10272932052612305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008157730102539062  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7371208667755127
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.7371 | Train 0.4214 | Val 0.2020 | Test 0.2031
loading full batch data spends  0.0019516944885253906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02297687530517578  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022981643676757812  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10498952865600586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7256256341934204
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.7256 | Train 0.4071 | Val 0.2100 | Test 0.2065
loading full batch data spends  0.0020399093627929688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022038936614990234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022043704986572266  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10995292663574219
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0082244873046875  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.730017066001892
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7300 | Train 0.3929 | Val 0.2180 | Test 0.2084
loading full batch data spends  0.0020499229431152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021858692169189453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021863460540771484  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10544323921203613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7274036407470703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.7274 | Train 0.3786 | Val 0.2280 | Test 0.2244
loading full batch data spends  0.0019974708557128906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022998332977294922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023003101348876953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09682941436767578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7201780080795288
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7202 | Train 0.3786 | Val 0.2380 | Test 0.2321
loading full batch data spends  0.0019550323486328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022032737731933594  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022037506103515625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10236597061157227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6629084348678589
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.6629 | Train 0.3929 | Val 0.2280 | Test 0.2253
loading full batch data spends  0.0020313262939453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021944046020507812  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021948814392089844  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1041867733001709
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7401998043060303
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.7402 | Train 0.4071 | Val 0.2320 | Test 0.2336
loading full batch data spends  0.0019562244415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023007869720458984  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023012638092041016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1042177677154541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7035012245178223
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.7035 | Train 0.4143 | Val 0.2380 | Test 0.2389
loading full batch data spends  0.002066373825073242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021956443786621094  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021961212158203125  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10740303993225098
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7592331171035767
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7592 | Train 0.4143 | Val 0.2460 | Test 0.2234
loading full batch data spends  0.0019876956939697266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023012638092041016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023017406463623047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.11363554000854492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7803658246994019
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.7804 | Train 0.4143 | Val 0.2440 | Test 0.2311
loading full batch data spends  0.0021812915802001953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195882797241211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196359634399414  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10255599021911621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6843459606170654
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.6843 | Train 0.4429 | Val 0.2440 | Test 0.2403
loading full batch data spends  0.0020215511322021484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021926403045654297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021931171417236328  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10522866249084473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.722525715827942
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7225 | Train 0.4214 | Val 0.2260 | Test 0.2340
loading full batch data spends  0.0020151138305664062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298116683959961  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02298593521118164  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10201048851013184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.714167833328247
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.7142 | Train 0.4071 | Val 0.2200 | Test 0.2205
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021923542022705078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192831039428711  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10747432708740234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7023652791976929
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.7024 | Train 0.4071 | Val 0.2120 | Test 0.2074
loading full batch data spends  0.002850055694580078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   0.00013399124145507812
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022958755493164062  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022963523864746094  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09557223320007324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7332295179367065
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.7332 | Train 0.4214 | Val 0.2280 | Test 0.2200
loading full batch data spends  0.0019822120666503906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022014617919921875  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022019386291503906  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10996150970458984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7197198867797852
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.7197 | Train 0.4643 | Val 0.2480 | Test 0.2466
loading full batch data spends  0.0025234222412109375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.745887756347656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021931171417236328  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02193593978881836  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10382342338562012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7165569067001343
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7166 | Train 0.4571 | Val 0.2700 | Test 0.2621
loading full batch data spends  0.001969575881958008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021861553192138672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021866321563720703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10736727714538574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.704666256904602
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.7047 | Train 0.4357 | Val 0.2720 | Test 0.2626
loading full batch data spends  0.002043008804321289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021942615509033203  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021947383880615234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10529088973999023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.682557225227356
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.6826 | Train 0.4571 | Val 0.2600 | Test 0.2611
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022000789642333984  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022005558013916016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10220122337341309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6954171657562256
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.6954 | Train 0.4143 | Val 0.2360 | Test 0.2398
loading full batch data spends  0.002003908157348633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02198314666748047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0219879150390625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1011955738067627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008211135864257812  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6694236993789673
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.6694 | Train 0.3857 | Val 0.2340 | Test 0.2258
loading full batch data spends  0.001987934112548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021969318389892578  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197408676147461  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10432600975036621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7049083709716797
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.7049 | Train 0.3643 | Val 0.2380 | Test 0.2234
loading full batch data spends  0.0019860267639160156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02192401885986328  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021928787231445312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10249996185302734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6581298112869263
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.6581 | Train 0.4000 | Val 0.2400 | Test 0.2423
loading full batch data spends  0.002004384994506836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022988319396972656  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022993087768554688  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10755491256713867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.677520990371704
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.6775 | Train 0.3786 | Val 0.2400 | Test 0.2510
loading full batch data spends  0.002027750015258789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021944522857666016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021949291229248047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.13020944595336914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008173465728759766  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6960093975067139
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.6960 | Train 0.4214 | Val 0.2520 | Test 0.2573
loading full batch data spends  0.002008199691772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022022724151611328  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02202749252319336  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10494112968444824
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7013965845108032
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.7014 | Train 0.3786 | Val 0.2440 | Test 0.2452
loading full batch data spends  0.002019643783569336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021949291229248047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021954059600830078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10806488990783691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008179187774658203  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.675841212272644
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.6758 | Train 0.3786 | Val 0.2460 | Test 0.2292
loading full batch data spends  0.0020198822021484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022016525268554688  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02202129364013672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10546183586120605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6542412042617798
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.6542 | Train 0.3714 | Val 0.2400 | Test 0.2336
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02297496795654297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022979736328125  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10017824172973633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6236064434051514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.6236 | Train 0.3786 | Val 0.2400 | Test 0.2456
loading full batch data spends  0.0019800662994384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021916866302490234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021921634674072266  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10680389404296875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6718262434005737
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.6718 | Train 0.3857 | Val 0.2620 | Test 0.2698
loading full batch data spends  0.002007007598876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021977901458740234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021982669830322266  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1094059944152832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6736087799072266
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.6736 | Train 0.3714 | Val 0.2740 | Test 0.2577
loading full batch data spends  0.0021431446075439453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021899700164794922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021904468536376953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.0987396240234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7005518674850464
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.7006 | Train 0.4286 | Val 0.2720 | Test 0.2756
loading full batch data spends  0.0020227432250976562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02198505401611328  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021989822387695312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10472631454467773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6573688983917236
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.6574 | Train 0.4286 | Val 0.2540 | Test 0.2539
loading full batch data spends  0.0019114017486572266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021954059600830078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02195882797241211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10580301284790039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7187570333480835
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7188 | Train 0.4429 | Val 0.2460 | Test 0.2553
loading full batch data spends  0.002022981643676758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023009300231933594  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023014068603515625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.11403536796569824
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009205818176269531  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6934490203857422
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.6934 | Train 0.4571 | Val 0.2600 | Test 0.2660
loading full batch data spends  0.0019750595092773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299213409423828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022996902465820312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10412812232971191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7097383737564087
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.7097 | Train 0.4857 | Val 0.2840 | Test 0.2979
loading full batch data spends  0.0020172595977783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0219573974609375  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02196216583251953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10838747024536133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008189678192138672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.673452377319336
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.6735 | Train 0.4714 | Val 0.2760 | Test 0.2838
loading full batch data spends  0.0019559860229492188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021975994110107422  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021980762481689453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10191798210144043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6670721769332886
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.6671 | Train 0.4500 | Val 0.2580 | Test 0.2703
loading full batch data spends  0.0019943714141845703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021984577178955078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02198934555053711  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10351371765136719
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008221626281738281  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.672240138053894
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.6722 | Train 0.4857 | Val 0.2760 | Test 0.2964
loading full batch data spends  0.002015829086303711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197265625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02197742462158203  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10692954063415527
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6399377584457397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.6399 | Train 0.4714 | Val 0.2820 | Test 0.2693
loading full batch data spends  0.002020120620727539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021953105926513672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021957874298095703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10460782051086426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.650246500968933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.6502 | Train 0.4286 | Val 0.2440 | Test 0.2307
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021889209747314453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021893978118896484  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10597705841064453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7362266778945923
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.7362 | Train 0.4143 | Val 0.2480 | Test 0.2374
loading full batch data spends  0.0020074844360351562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023000717163085938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02300548553466797  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10117268562316895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6522676944732666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.6523 | Train 0.3929 | Val 0.2460 | Test 0.2582
loading full batch data spends  0.0019578933715820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022991657257080078  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299642562866211  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09911417961120605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6373026371002197
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.6373 | Train 0.3786 | Val 0.2540 | Test 0.2587
loading full batch data spends  0.0020132064819335938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022998332977294922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023003101348876953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10413455963134766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6816517114639282
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.6817 | Train 0.3857 | Val 0.2540 | Test 0.2602
loading full batch data spends  0.0019505023956298828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022984981536865234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022989749908447266  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10299468040466309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.629286527633667
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.6293 | Train 0.4071 | Val 0.2580 | Test 0.2732
loading full batch data spends  0.002015352249145508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021955013275146484  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021959781646728516  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09947991371154785
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008184432983398438  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6568667888641357
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.6569 | Train 0.4071 | Val 0.2660 | Test 0.2868
loading full batch data spends  0.0019049644470214844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.0219879150390625  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199268341064453  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10496354103088379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6373270750045776
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.6373 | Train 0.4500 | Val 0.2800 | Test 0.2887
loading full batch data spends  0.001991748809814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02189493179321289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021899700164794922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09953618049621582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6692907810211182
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.6693 | Train 0.4643 | Val 0.2800 | Test 0.2747
loading full batch data spends  0.0021331310272216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021924495697021484  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021929264068603516  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09978389739990234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6962677240371704
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.6963 | Train 0.4714 | Val 0.2940 | Test 0.2906
loading full batch data spends  0.001999378204345703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021976947784423828  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02198171615600586  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10745739936828613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008176326751708984  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.655601143836975
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.6556 | Train 0.4929 | Val 0.3320 | Test 0.3148
loading full batch data spends  0.0019745826721191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022990703582763672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022995471954345703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10891461372375488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6936657428741455
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.6937 | Train 0.3071 | Val 0.2080 | Test 0.2070
loading full batch data spends  0.0020666122436523438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022021770477294922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022026538848876953  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10016822814941406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820302963256836  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6753568649291992
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.6754 | Train 0.2929 | Val 0.1780 | Test 0.1750
loading full batch data spends  0.0021371841430664062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021907806396484375  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021912574768066406  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.0953068733215332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.729956865310669
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.7300 | Train 0.3214 | Val 0.2240 | Test 0.2220
loading full batch data spends  0.0020101070404052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02193307876586914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021937847137451172  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10055160522460938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008168220520019531  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7026216983795166
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.7026 | Train 0.4143 | Val 0.3000 | Test 0.3032
loading full batch data spends  0.0021576881408691406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02299356460571289  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022998332977294922  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10547566413879395
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6622294187545776
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.6622 | Train 0.3786 | Val 0.2420 | Test 0.2350
loading full batch data spends  0.0019981861114501953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194833755493164  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021953105926513672  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.09666705131530762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7353532314300537
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.7354 | Train 0.3571 | Val 0.2360 | Test 0.2147
loading full batch data spends  0.001974821090698242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021944522857666016  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021949291229248047  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10265588760375977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6755822896957397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.6756 | Train 0.3714 | Val 0.2320 | Test 0.2142
loading full batch data spends  0.001996278762817383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021938800811767578  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02194356918334961  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10070681571960449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008162975311279297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.845428228378296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.8454 | Train 0.3643 | Val 0.2320 | Test 0.2224
loading full batch data spends  0.0019600391387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02199411392211914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021998882293701172  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10199403762817383
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008294105529785156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.730072021484375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.7301 | Train 0.3786 | Val 0.2580 | Test 0.2568
loading full batch data spends  0.001994609832763672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022975921630859375  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022980690002441406  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10829472541809082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6637117862701416
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.6637 | Train 0.3714 | Val 0.2600 | Test 0.2524
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022994041442871094  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022998809814453125  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10419511795043945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6848620176315308
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.6849 | Train 0.2857 | Val 0.2040 | Test 0.1823
loading full batch data spends  0.001955270767211914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021991729736328125  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021996498107910156  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10748624801635742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.00820016860961914  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8210636377334595
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.8211 | Train 0.2786 | Val 0.1480 | Test 0.1441
loading full batch data spends  0.0019278526306152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021952152252197266  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021956920623779297  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10466599464416504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7868471145629883
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.7868 | Train 0.2857 | Val 0.1560 | Test 0.1504
loading full batch data spends  0.002015352249145508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.02304553985595703  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.023050308227539062  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10381770133972168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009205818176269531  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7811185121536255
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.7811 | Train 0.3000 | Val 0.1680 | Test 0.1639
loading full batch data spends  0.002000570297241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021820545196533203  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.021825313568115234  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.1118316650390625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.008291244506835938  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8023065328598022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.8023 | Train 0.3857 | Val 0.2040 | Test 0.2123
loading full batch data spends  0.002133607864379883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022974014282226562  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.022978782653808594  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

pure train time  0.10730385780334473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5859375 GB
    Memory Allocated: 0.009202957153320312  GigaBytes
Max Memory Allocated: 0.04302263259887695  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7069053649902344
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 2, 16])
h.size() torch.Size([2708, 32])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.7069 | Train 0.4857 | Val 0.2680 | Test 0.2650
Total (block generation + training)time/epoch 0.11756149172782898
pure train time/epoch 0.1063127225759078

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368]
