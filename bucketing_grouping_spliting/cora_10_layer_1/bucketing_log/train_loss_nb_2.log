main start at this time 1697828225.3888755
-----------------------------------------before load data 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

----------------------------------------start of run function 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

generate_dataloader_bucket_block=======
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  10
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
memory_constraint:  18.0
the grouping_fanout_cora called successfully
capacity  27
 
sorted_dict  {9: 9, 3: 8, 2: 7, 4: 7, 5: 6, 1: 4, 8: 3, 6: 2, 7: 2, 0: 1}

weights after sort [9, 8, 7, 7, 6, 4, 3, 2, 2, 1]
res_tmp  [9 7 7 4]

remove bucket_id:  [0, 2, 3, 5]
original bucket_id :,  [9, 2, 4, 1]
remove weights:  [9 7 7 4], 		------------sum 27

before remove weights,  [9, 8, 7, 7, 6, 4, 3, 2, 2, 1]
after remove pre pack weights,  [8, 6, 3, 2, 2, 1]
G_BUCKET_ID_list [[9, 2, 4, 1], [3, 5, 8, 6, 7, 0]]
Groups_mem_list  [[9, 7, 7, 4], [8, 6, 3, 2, 2, 1]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.001497507095336914
current group_mem  0.029115311801433563
current group_mem  0.025175616145133972
batches output list generation spend  0.0006759166717529297
self.weights_list  [0.5428571428571428, 0.45714285714285713]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0012712478637695312
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0022432804107666016
len local_batched_seeds_list  2
partition total batch output list spend :  0.004577159881591797
self.buckets_partition() spend  sec:  0.003532886505126953
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.00012493133544921875

in edges time spent  0.0006117820739746094
local to global src and eids time spent  0.0002186298370361328
time gen tails  0.0001227855682373047
res  length 2
block collection to dataloader spend  4.0531158447265625e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.513671875 GB
    Memory Allocated: 0.06446409225463867  GigaBytes
Max Memory Allocated: 0.06446409225463867  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.62890625 GB
    Memory Allocated: 0.09037351608276367  GigaBytes
Max Memory Allocated: 0.09280538558959961  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.62890625 GB
    Memory Allocated: 0.0903768539428711  GigaBytes
Max Memory Allocated: 0.09280538558959961  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.76171875 GB
    Memory Allocated: 0.12681293487548828  GigaBytes
Max Memory Allocated: 0.21385526657104492  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.76171875 GB
    Memory Allocated: 0.14921903610229492  GigaBytes
Max Memory Allocated: 0.21385526657104492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.76171875 GB
    Memory Allocated: 0.14922142028808594  GigaBytes
Max Memory Allocated: 0.21385526657104492  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 1.91796875 GB
    Memory Allocated: 0.25205230712890625  GigaBytes
Max Memory Allocated: 0.34580230712890625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.946556806564331
pure train time :  0.4693334102630615
train time :  1.0137183666229248
end to end time :  1.030277967453003
connection check time:  0.0013554096221923828
block generation time  0.009697914123535156
generate_dataloader_bucket_block=======
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  10
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
memory_constraint:  18.0
the grouping_fanout_cora called successfully
capacity  27
 
sorted_dict  {9: 9, 3: 8, 2: 7, 4: 7, 5: 6, 1: 4, 8: 3, 6: 2, 7: 2, 0: 1}

weights after sort [9, 8, 7, 7, 6, 4, 3, 2, 2, 1]
res_tmp  [9 7 7 4]

remove bucket_id:  [0, 2, 3, 5]
original bucket_id :,  [9, 2, 4, 1]
remove weights:  [9 7 7 4], 		------------sum 27

before remove weights,  [9, 8, 7, 7, 6, 4, 3, 2, 2, 1]
after remove pre pack weights,  [8, 6, 3, 2, 2, 1]
G_BUCKET_ID_list [[9, 2, 4, 1], [3, 5, 8, 6, 7, 0]]
Groups_mem_list  [[9, 7, 7, 4], [8, 6, 3, 2, 2, 1]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.0022361278533935547
current group_mem  0.029115311801433563
current group_mem  0.025175616145133972
batches output list generation spend  9.250640869140625e-05
self.weights_list  [0.5428571428571428, 0.45714285714285713]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0012862682342529297
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.002410888671875
len local_batched_seeds_list  2
partition total batch output list spend :  0.004713296890258789
self.buckets_partition() spend  sec:  0.0037207603454589844
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.0002627372741699219

in edges time spent  0.0010294914245605469
local to global src and eids time spent  0.0007166862487792969
time gen tails  0.00017642974853515625
res  length 2
block collection to dataloader spend  5.9604644775390625e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.91796875 GB
    Memory Allocated: 0.2523317337036133  GigaBytes
Max Memory Allocated: 0.34580230712890625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.919921875 GB
    Memory Allocated: 0.27823925018310547  GigaBytes
Max Memory Allocated: 0.34580230712890625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.919921875 GB
    Memory Allocated: 0.2782421112060547  GigaBytes
Max Memory Allocated: 0.34580230712890625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.951171875 GB
    Memory Allocated: 0.25204944610595703  GigaBytes
Max Memory Allocated: 0.4016451835632324  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.951171875 GB
    Memory Allocated: 0.27445554733276367  GigaBytes
Max Memory Allocated: 0.4016451835632324  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.951171875 GB
    Memory Allocated: 0.2744579315185547  GigaBytes
Max Memory Allocated: 0.4016451835632324  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 1.951171875 GB
    Memory Allocated: 0.25205230712890625  GigaBytes
Max Memory Allocated: 0.4016451835632324  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9235013723373413
pure train time :  0.06936931610107422
train time :  0.07227873802185059
end to end time :  0.08499026298522949
connection check time:  0.0025072097778320312
block generation time  0.004758358001708984
end to end time  0.08990216255187988
Total (block generation + training)time/epoch 0.08990216255187988
pure train time per /epoch  [0.4693334102630615, 0.06936931610107422]
pure train time average  nan
input num list  [639, 641]
