main start at this time 1690944613.1864712
-----------------------------------------before load data 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.015841245651245117
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.0004024505615234375
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.017700910568237305
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.01645970344543457
len local_batched_seeds_list  2
partition total batch output list spend :  0.3937041759490967
self.buckets_partition() spend  sec:  0.03417563438415527
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.23797869682312012

in edges time spent  0.37265682220458984
local to global src and eids time spent  0.8489928245544434
time gen tails  0.15722894668579102
res  length 2
block collection to dataloader spend  8.344650268554688e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.66796875 GB
    Memory Allocated: 0.22406864166259766  GigaBytes
Max Memory Allocated: 0.22406864166259766  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.16015625 GB
    Memory Allocated: 5.851899147033691  GigaBytes
Max Memory Allocated: 6.541579246520996  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.16015625 GB
    Memory Allocated: 5.869050979614258  GigaBytes
Max Memory Allocated: 6.541579246520996  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.16796875 GB
    Memory Allocated: 0.24213075637817383  GigaBytes
Max Memory Allocated: 6.541579246520996  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 5.869976997375488  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 5.88724422454834  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 0.2513723373413086  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 4.791176795959473
pure train time :  0.47780489921569824
train time :  1.2502970695495605
end to end time :  5.041835784912109
connection check time:  1.9169354438781738
block generation time  1.404325008392334
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.044547080993652344
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.00033974647521972656
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015223264694213867
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.045006752014160156
len local_batched_seeds_list  2
partition total batch output list spend :  0.4286623001098633
self.buckets_partition() spend  sec:  0.06024646759033203
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.23694467544555664

in edges time spent  0.36910104751586914
local to global src and eids time spent  0.9086422920227051
time gen tails  0.1677844524383545
res  length 2
block collection to dataloader spend  7.62939453125e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 0.24211692810058594  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 5.853571891784668  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 5.870723247528076  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.548828125 GB
    Memory Allocated: 0.2425975799560547  GigaBytes
Max Memory Allocated: 6.576242923736572  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.870791435241699  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888058662414551  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2515115737915039  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 4.168598175048828
pure train time :  0.0669853687286377
train time :  0.29227328300476074
end to end time :  4.109980583190918
connection check time:  2.0119218826293945
block generation time  1.307077169418335
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.028117895126342773
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.0003104209899902344
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015383005142211914
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.028547048568725586
len local_batched_seeds_list  2
partition total batch output list spend :  0.40759873390197754
self.buckets_partition() spend  sec:  0.04394674301147461
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.23466897010803223

in edges time spent  0.36566877365112305
local to global src and eids time spent  0.9165184497833252
time gen tails  0.21712613105773926
res  length 2
block collection to dataloader spend  7.62939453125e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24207639694213867  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854127883911133  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871395111083984  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2428574562072754  GigaBytes
Max Memory Allocated: 6.577057361602783  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871580123901367  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888847351074219  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.25142383575439453  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.464874505996704
pure train time :  0.06992483139038086
train time :  0.36196327209472656
end to end time :  4.708653450012207
connection check time:  2.0652074813842773
block generation time  1.798377275466919
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.04438281059265137
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.002276182174682617
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015310049057006836
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.04676008224487305
len local_batched_seeds_list  2
partition total batch output list spend :  0.4390735626220703
self.buckets_partition() spend  sec:  0.0620875358581543
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.22887158393859863

in edges time spent  0.38813161849975586
local to global src and eids time spent  0.8694343566894531
time gen tails  0.19708681106567383
res  length 2
block collection to dataloader spend  1.1682510375976562e-05
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2421717643737793  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854308605194092  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871575832366943  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2430100440979004  GigaBytes
Max Memory Allocated: 6.577846050262451  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871732711791992  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888999938964844  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.25157642364501953  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.5736942291259766
pure train time :  0.06928491592407227
train time :  0.3129563331604004
end to end time :  4.102891683578491
connection check time:  1.9946041107177734
block generation time  1.274400234222412
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.029137611389160156
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.00028133392333984375
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.016808748245239258
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.02951502799987793
len local_batched_seeds_list  2
partition total batch output list spend :  0.4167609214782715
self.buckets_partition() spend  sec:  0.04633831977844238
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.24392032623291016

in edges time spent  0.3668699264526367
local to global src and eids time spent  0.8525576591491699
time gen tails  0.2038872241973877
res  length 2
block collection to dataloader spend  1.049041748046875e-05
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24339532852172852  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.855532169342041  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.872799396514893  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24239730834960938  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871119976043701  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888387203216553  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2509636878967285  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.6083626747131348
pure train time :  0.07291889190673828
train time :  0.35280680656433105
end to end time :  5.045863151550293
connection check time:  1.9923009872436523
block generation time  2.1973371505737305
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.032895565032958984
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.0003650188446044922
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.017156362533569336
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.033365726470947266
len local_batched_seeds_list  2
partition total batch output list spend :  0.41298532485961914
self.buckets_partition() spend  sec:  0.050538063049316406
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.23011040687561035

in edges time spent  0.34619593620300293
local to global src and eids time spent  0.8081355094909668
time gen tails  0.15891766548156738
res  length 2
block collection to dataloader spend  6.9141387939453125e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24234342575073242  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854480266571045  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.8717474937438965  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24261045455932617  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871333122253418  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.8886003494262695  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2511768341064453  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7723702192306519
pure train time :  0.059699058532714844
train time :  0.2893233299255371
end to end time :  3.8069472312927246
connection check time:  1.8473973274230957
block generation time  1.1783356666564941
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.016837120056152344
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.006840229034423828
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.017431259155273438
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.023757219314575195
len local_batched_seeds_list  2
partition total batch output list spend :  0.28898167610168457
self.buckets_partition() spend  sec:  0.04120039939880371
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.17594075202941895

in edges time spent  0.20555663108825684
local to global src and eids time spent  0.37741589546203613
time gen tails  0.12778878211975098
res  length 2
block collection to dataloader spend  6.4373016357421875e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24200963973999023  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854127883911133  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871395111083984  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2429971694946289  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871719837188721  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888987064361572  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.25156354904174805  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7147263288497925
pure train time :  0.05378127098083496
train time :  0.2798476219177246
end to end time :  2.6078381538391113
connection check time:  1.037567138671875
block generation time  0.9387078285217285
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.019146442413330078
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.00032019615173339844
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015454292297363281
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.019537687301635742
len local_batched_seeds_list  2
partition total batch output list spend :  0.16879487037658691
self.buckets_partition() spend  sec:  0.035002946853637695
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.17893743515014648

in edges time spent  0.20996427536010742
local to global src and eids time spent  0.3966207504272461
time gen tails  0.11844277381896973
res  length 2
block collection to dataloader spend  6.198883056640625e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2422637939453125  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854400634765625  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871667861938477  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24263811111450195  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871360778808594  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888628005981445  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2512044906616211  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6542432308197021
pure train time :  0.0666203498840332
train time :  0.3300948143005371
end to end time :  3.0603034496307373
connection check time:  1.0472216606140137
block generation time  1.4679012298583984
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.01422262191772461
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.00023436546325683594
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.014894485473632812
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.01453399658203125
len local_batched_seeds_list  2
partition total batch output list spend :  0.15615010261535645
self.buckets_partition() spend  sec:  0.029438495635986328
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.17037153244018555

in edges time spent  0.19762873649597168
local to global src and eids time spent  0.37326550483703613
time gen tails  0.1176152229309082
res  length 2
block collection to dataloader spend  1.0013580322265625e-05
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24339532852172852  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.855532169342041  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.872799396514893  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24259376525878906  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871316432952881  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888583660125732  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2511601448059082  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.463709831237793
pure train time :  0.05470848083496094
train time :  0.3088047504425049
end to end time :  2.4399638175964355
connection check time:  1.016756296157837
block generation time  0.909658670425415
generate_dataloader_bucket_block=======
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  192609
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
type of fanout_dst_nids  <class 'torch.Tensor'>
the grouping_fanout_1 called successfully
capacity  100
sorted_dict  {8: 34, 6: 30, 7: 27, 5: 22, 4: 14, 3: 12, 2: 7, 1: 2, 0: 1}

weights after sort [34, 30, 27, 22, 14, 12, 7, 2, 1]

remove bucket_id:  [1, 3, 4]
original bucket_id :,  [6, 5, 4]
remove weights:  [30 22 14], 		------------sum 66

before remove weights,  [34, 30, 27, 22, 14, 12, 7, 2, 1]
after remove pre pack weights,  [34, 27, 12, 7, 2, 1]
G_BUCKET_ID_list [[6, 5, 4], [8, 7, 3, 2, 1, 0]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.019333839416503906
len(g_bucket_nids_list)  2
len(local_split_batches_nid_list)  2
current group_mem  0.06779283285140991
current group_mem  0.08610561490058899
batches output list generation spend  0.00034546852111816406
self.weights_list  [0.49831867365989896, 0.501681326340101]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.015034914016723633
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.019767284393310547
len local_batched_seeds_list  2
partition total batch output list spend :  0.16044116020202637
self.buckets_partition() spend  sec:  0.034815073013305664
layer  0
 the number of batches:  2
check_connections_block*********************************

the find indices time spent  0.17214012145996094

in edges time spent  0.19591569900512695
local to global src and eids time spent  0.3746054172515869
time gen tails  0.1157066822052002
res  length 2
block collection to dataloader spend  6.4373016357421875e-06
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.24198436737060547  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.854121208190918  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.8713884353637695  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.2429194450378418  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.871642112731934  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 5.888909339904785  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 8.55078125 GB
    Memory Allocated: 0.25148582458496094  GigaBytes
Max Memory Allocated: 6.577998638153076  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2881395816802979
pure train time :  0.05413198471069336
train time :  0.2791256904602051
end to end time :  2.3856923580169678
connection check time:  1.0014541149139404
block generation time  0.9022688865661621
end to end time  2.504847526550293
Total (block generation + training)time/epoch 2.504847526550293
pure train time per /epoch  [0.47780489921569824, 0.0669853687286377, 0.06992483139038086, 0.06928491592407227, 0.07291889190673828, 0.059699058532714844, 0.05378127098083496, 0.0666203498840332, 0.05470848083496094, 0.05413198471069336]
pure train time average  0.06159213611057827
input num list  [1159767, 1158585, 1159174, 1159840, 1159017, 1159228, 1159370, 1159088, 1159721, 1159094]
