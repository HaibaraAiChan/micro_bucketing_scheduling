main start at this time 1696032184.0527232
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.0028123855590820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
first layer else: feat size  torch.Size([1342, 1433])
src_prefix_shape  torch.Size([1342])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1342, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1342, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([630, 8, 8])
h_dst  torch.Size([630, 1433])
dst_prefix_shape  (630,)
el size torch.Size([1342, 8, 1])
er szie torch.Size([630, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 46, 2: 97, 3: 105, 4: 109, 5: 83, 6: 58, 7: 34, 8: 23, 9: 8, 10: 67}
--else: feat size  torch.Size([630, 64])
src_prefix_shape  torch.Size([630])
h_src = h_dst = self.feat_drop(feat)  torch.Size([630, 64])
feat_src = feat_dst  torch.Size([630, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 20, 2: 25, 3: 26, 4: 22, 5: 15, 6: 11, 7: 4, 8: 3, 9: 4, 10: 3, 11: 1, 12: 2, 19: 1, 21: 1, 25: 2}
return rst 
torch.Size([1, 140, 7])
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.02727031707763672  GigaBytes
Max Memory Allocated: 0.027710914611816406  GigaBytes

batch_pred  torch.Size([140, 7])
batch_labels torch.Size([140])
----------------------------------------after loss function
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.02727508544921875  GigaBytes
Max Memory Allocated: 0.027710914611816406  GigaBytes

pure train time  0.5126540660858154
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.5625 GB
    Memory Allocated: 0.009155750274658203  GigaBytes
Max Memory Allocated: 0.027710914611816406  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9514586925506592
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1753, 1433])
first layer else: feat size  torch.Size([1753, 1433])
src_prefix_shape  torch.Size([1753])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1753, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1753, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([739, 8, 8])
h_dst  torch.Size([739, 1433])
dst_prefix_shape  (739,)
el size torch.Size([1753, 8, 1])
er szie torch.Size([739, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 43, 2: 105, 3: 147, 4: 122, 5: 105, 6: 51, 7: 41, 8: 28, 9: 12, 10: 16, 11: 9, 12: 10, 13: 2, 14: 5, 15: 5, 16: 6, 17: 7, 18: 2, 19: 4, 21: 2, 22: 1, 23: 2, 29: 1, 30: 2, 31: 1, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([739, 8, 8])
h.size() torch.Size([739, 64])
layer  1
--else: feat size  torch.Size([739, 64])
src_prefix_shape  torch.Size([739])
h_src = h_dst = self.feat_drop(feat)  torch.Size([739, 64])
feat_src = feat_dst  torch.Size([739, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 18, 2: 33, 3: 30, 4: 16, 5: 12, 6: 8, 7: 5, 8: 6, 9: 1, 11: 2, 12: 2, 14: 2, 15: 1, 18: 1, 19: 1, 21: 1, 168: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1655, 1433])
first layer else: feat size  torch.Size([1655, 1433])
src_prefix_shape  torch.Size([1655])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1655, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1655, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([564, 8, 8])
h_dst  torch.Size([564, 1433])
dst_prefix_shape  (564,)
el size torch.Size([1655, 8, 1])
er szie torch.Size([564, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 58, 2: 80, 3: 99, 4: 72, 5: 77, 6: 43, 7: 32, 8: 30, 9: 9, 10: 9, 11: 7, 12: 9, 13: 3, 14: 3, 15: 1, 16: 3, 17: 6, 18: 1, 19: 3, 21: 1, 22: 1, 23: 2, 26: 1, 29: 1, 30: 2, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([564, 8, 8])
h.size() torch.Size([564, 64])
layer  1
--else: feat size  torch.Size([564, 64])
src_prefix_shape  torch.Size([564])
h_src = h_dst = self.feat_drop(feat)  torch.Size([564, 64])
feat_src = feat_dst  torch.Size([564, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 33, 2: 25, 3: 23, 4: 23, 5: 16, 6: 8, 7: 3, 8: 3, 11: 1, 12: 3, 22: 1, 44: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1597, 1433])
first layer else: feat size  torch.Size([1597, 1433])
src_prefix_shape  torch.Size([1597])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1597, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1597, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([568, 8, 8])
h_dst  torch.Size([568, 1433])
dst_prefix_shape  (568,)
el size torch.Size([1597, 8, 1])
er szie torch.Size([568, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 52, 2: 94, 3: 105, 4: 84, 5: 80, 6: 44, 7: 30, 8: 9, 9: 10, 10: 8, 11: 4, 12: 9, 13: 1, 14: 4, 15: 4, 16: 4, 17: 4, 18: 2, 19: 2, 21: 1, 22: 1, 23: 1, 26: 1, 29: 1, 30: 2, 31: 1, 32: 1, 33: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([568, 8, 8])
h.size() torch.Size([568, 64])
layer  1
--else: feat size  torch.Size([568, 64])
src_prefix_shape  torch.Size([568])
h_src = h_dst = self.feat_drop(feat)  torch.Size([568, 64])
feat_src = feat_dst  torch.Size([568, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 28, 2: 31, 3: 27, 4: 17, 5: 10, 6: 13, 7: 6, 8: 1, 9: 2, 10: 1, 11: 1, 14: 1, 17: 1, 30: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1648, 1433])
first layer else: feat size  torch.Size([1648, 1433])
src_prefix_shape  torch.Size([1648])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1648, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1648, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([567, 8, 8])
h_dst  torch.Size([567, 1433])
dst_prefix_shape  (567,)
el size torch.Size([1648, 8, 1])
er szie torch.Size([567, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 50, 2: 88, 3: 103, 4: 86, 5: 67, 6: 36, 7: 20, 8: 29, 9: 13, 10: 12, 11: 5, 12: 13, 13: 3, 14: 5, 15: 4, 16: 6, 17: 6, 18: 1, 19: 2, 21: 1, 23: 3, 26: 1, 30: 2, 31: 1, 32: 1, 33: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([567, 8, 8])
h.size() torch.Size([567, 64])
layer  1
--else: feat size  torch.Size([567, 64])
src_prefix_shape  torch.Size([567])
h_src = h_dst = self.feat_drop(feat)  torch.Size([567, 64])
feat_src = feat_dst  torch.Size([567, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 26, 2: 28, 3: 28, 4: 23, 5: 13, 6: 5, 7: 3, 8: 6, 9: 2, 10: 1, 12: 1, 13: 1, 15: 2, 16: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1643, 1433])
first layer else: feat size  torch.Size([1643, 1433])
src_prefix_shape  torch.Size([1643])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1643, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1643, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([568, 8, 8])
h_dst  torch.Size([568, 1433])
dst_prefix_shape  (568,)
el size torch.Size([1643, 8, 1])
er szie torch.Size([568, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 47, 2: 85, 3: 94, 4: 89, 5: 84, 6: 36, 7: 28, 8: 17, 9: 15, 10: 16, 11: 5, 12: 10, 13: 2, 14: 3, 15: 4, 16: 3, 17: 5, 19: 4, 21: 1, 22: 1, 23: 3, 26: 1, 29: 1, 30: 2, 31: 1, 32: 2, 33: 1, 34: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([568, 8, 8])
h.size() torch.Size([568, 64])
layer  1
--else: feat size  torch.Size([568, 64])
src_prefix_shape  torch.Size([568])
h_src = h_dst = self.feat_drop(feat)  torch.Size([568, 64])
feat_src = feat_dst  torch.Size([568, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 27, 2: 31, 3: 27, 4: 21, 5: 12, 6: 6, 7: 3, 8: 1, 9: 2, 10: 6, 11: 1, 13: 1, 23: 1, 33: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1687, 1433])
first layer else: feat size  torch.Size([1687, 1433])
src_prefix_shape  torch.Size([1687])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1687, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1687, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([642, 8, 8])
h_dst  torch.Size([642, 1433])
dst_prefix_shape  (642,)
el size torch.Size([1687, 8, 1])
er szie torch.Size([642, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 59, 2: 94, 3: 116, 4: 108, 5: 77, 6: 40, 7: 33, 8: 18, 9: 14, 10: 19, 11: 9, 12: 9, 13: 1, 14: 4, 15: 5, 16: 3, 17: 5, 18: 3, 19: 5, 21: 2, 22: 1, 23: 2, 26: 1, 29: 1, 30: 2, 31: 1, 32: 1, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([642, 8, 8])
h.size() torch.Size([642, 64])
layer  1
--else: feat size  torch.Size([642, 64])
src_prefix_shape  torch.Size([642])
h_src = h_dst = self.feat_drop(feat)  torch.Size([642, 64])
feat_src = feat_dst  torch.Size([642, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 22, 2: 29, 3: 33, 4: 22, 5: 13, 6: 7, 7: 5, 8: 1, 9: 1, 10: 1, 11: 2, 12: 1, 21: 1, 30: 1, 74: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1683, 1433])
first layer else: feat size  torch.Size([1683, 1433])
src_prefix_shape  torch.Size([1683])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1683, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1683, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([593, 8, 8])
h_dst  torch.Size([593, 1433])
dst_prefix_shape  (593,)
el size torch.Size([1683, 8, 1])
er szie torch.Size([593, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 49, 2: 84, 3: 113, 4: 95, 5: 79, 6: 49, 7: 21, 8: 24, 9: 14, 10: 12, 11: 7, 12: 7, 13: 3, 14: 1, 15: 1, 16: 4, 17: 3, 18: 2, 19: 5, 21: 2, 22: 1, 23: 3, 26: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([593, 8, 8])
h.size() torch.Size([593, 64])
layer  1
--else: feat size  torch.Size([593, 64])
src_prefix_shape  torch.Size([593])
h_src = h_dst = self.feat_drop(feat)  torch.Size([593, 64])
feat_src = feat_dst  torch.Size([593, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 27, 2: 26, 3: 26, 4: 25, 5: 11, 6: 9, 7: 2, 8: 7, 9: 1, 10: 1, 12: 1, 16: 1, 17: 1, 18: 1, 31: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1591, 1433])
first layer else: feat size  torch.Size([1591, 1433])
src_prefix_shape  torch.Size([1591])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1591, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1591, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([543, 8, 8])
h_dst  torch.Size([543, 1433])
dst_prefix_shape  (543,)
el size torch.Size([1591, 8, 1])
er szie torch.Size([543, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 55, 2: 75, 3: 113, 4: 76, 5: 68, 6: 34, 7: 24, 8: 20, 9: 8, 10: 13, 11: 7, 12: 9, 14: 2, 15: 3, 16: 6, 17: 4, 18: 1, 19: 4, 21: 1, 22: 1, 23: 3, 29: 1, 30: 2, 31: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([543, 8, 8])
h.size() torch.Size([543, 64])
layer  1
--else: feat size  torch.Size([543, 64])
src_prefix_shape  torch.Size([543])
h_src = h_dst = self.feat_drop(feat)  torch.Size([543, 64])
feat_src = feat_dst  torch.Size([543, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 28, 2: 27, 3: 25, 4: 20, 5: 23, 6: 3, 7: 5, 9: 1, 10: 2, 12: 2, 14: 1, 15: 1, 16: 1, 17: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1543, 1433])
first layer else: feat size  torch.Size([1543, 1433])
src_prefix_shape  torch.Size([1543])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1543, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1543, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([524, 8, 8])
h_dst  torch.Size([524, 1433])
dst_prefix_shape  (524,)
el size torch.Size([1543, 8, 1])
er szie torch.Size([524, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 46, 2: 72, 3: 96, 4: 76, 5: 69, 6: 36, 7: 30, 8: 23, 9: 10, 10: 8, 11: 12, 12: 10, 13: 2, 14: 1, 15: 1, 16: 3, 17: 4, 18: 2, 19: 3, 21: 2, 22: 1, 23: 1, 26: 1, 29: 1, 30: 2, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([524, 8, 8])
h.size() torch.Size([524, 64])
layer  1
--else: feat size  torch.Size([524, 64])
src_prefix_shape  torch.Size([524])
h_src = h_dst = self.feat_drop(feat)  torch.Size([524, 64])
feat_src = feat_dst  torch.Size([524, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 29, 2: 27, 3: 28, 4: 21, 5: 15, 6: 7, 7: 2, 8: 7, 9: 1, 10: 1, 17: 1, 32: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1576, 1433])
first layer else: feat size  torch.Size([1576, 1433])
src_prefix_shape  torch.Size([1576])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1576, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1576, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([553, 8, 8])
h_dst  torch.Size([553, 1433])
dst_prefix_shape  (553,)
el size torch.Size([1576, 8, 1])
er szie torch.Size([553, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 48, 2: 93, 3: 93, 4: 95, 5: 67, 6: 34, 7: 32, 8: 15, 9: 12, 10: 11, 11: 6, 12: 8, 13: 2, 14: 3, 15: 3, 16: 4, 17: 6, 18: 2, 19: 2, 21: 2, 23: 3, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([553, 8, 8])
h.size() torch.Size([553, 64])
layer  1
--else: feat size  torch.Size([553, 64])
src_prefix_shape  torch.Size([553])
h_src = h_dst = self.feat_drop(feat)  torch.Size([553, 64])
feat_src = feat_dst  torch.Size([553, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 22, 2: 33, 3: 31, 4: 23, 5: 16, 6: 4, 7: 4, 8: 3, 9: 1, 10: 1, 16: 1, 65: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1694, 1433])
first layer else: feat size  torch.Size([1694, 1433])
src_prefix_shape  torch.Size([1694])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1694, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1694, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([576, 8, 8])
h_dst  torch.Size([576, 1433])
dst_prefix_shape  (576,)
el size torch.Size([1694, 8, 1])
er szie torch.Size([576, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 52, 2: 91, 3: 89, 4: 88, 5: 80, 6: 45, 7: 29, 8: 17, 9: 11, 10: 10, 11: 7, 12: 7, 13: 3, 14: 4, 15: 3, 16: 6, 17: 5, 18: 3, 19: 4, 21: 3, 23: 3, 26: 1, 29: 1, 30: 1, 31: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([576, 8, 8])
h.size() torch.Size([576, 64])
layer  1
--else: feat size  torch.Size([576, 64])
src_prefix_shape  torch.Size([576])
h_src = h_dst = self.feat_drop(feat)  torch.Size([576, 64])
feat_src = feat_dst  torch.Size([576, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 22, 2: 37, 3: 21, 4: 17, 5: 13, 6: 6, 7: 11, 8: 4, 9: 3, 10: 2, 11: 2, 12: 1, 16: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1596, 1433])
first layer else: feat size  torch.Size([1596, 1433])
src_prefix_shape  torch.Size([1596])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1596, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1596, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([519, 8, 8])
h_dst  torch.Size([519, 1433])
dst_prefix_shape  (519,)
el size torch.Size([1596, 8, 1])
er szie torch.Size([519, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 46, 2: 80, 3: 92, 4: 74, 5: 65, 6: 37, 7: 30, 8: 20, 9: 6, 10: 13, 11: 8, 12: 10, 13: 4, 14: 3, 15: 2, 16: 3, 17: 4, 18: 2, 19: 3, 21: 1, 22: 1, 26: 1, 30: 1, 31: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([519, 8, 8])
h.size() torch.Size([519, 64])
layer  1
--else: feat size  torch.Size([519, 64])
src_prefix_shape  torch.Size([519])
h_src = h_dst = self.feat_drop(feat)  torch.Size([519, 64])
feat_src = feat_dst  torch.Size([519, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 27, 2: 35, 3: 36, 4: 13, 5: 12, 6: 5, 7: 4, 8: 3, 9: 3, 11: 1, 13: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1718, 1433])
first layer else: feat size  torch.Size([1718, 1433])
src_prefix_shape  torch.Size([1718])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1718, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1718, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([604, 8, 8])
h_dst  torch.Size([604, 1433])
dst_prefix_shape  (604,)
el size torch.Size([1718, 8, 1])
er szie torch.Size([604, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 54, 2: 93, 3: 110, 4: 91, 5: 76, 6: 43, 7: 29, 8: 22, 9: 8, 10: 14, 11: 8, 12: 8, 13: 5, 14: 5, 15: 3, 16: 3, 17: 5, 18: 1, 19: 4, 21: 2, 22: 1, 23: 2, 26: 1, 29: 1, 30: 2, 31: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([604, 8, 8])
h.size() torch.Size([604, 64])
layer  1
--else: feat size  torch.Size([604, 64])
src_prefix_shape  torch.Size([604])
h_src = h_dst = self.feat_drop(feat)  torch.Size([604, 64])
feat_src = feat_dst  torch.Size([604, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 27, 2: 30, 3: 26, 4: 19, 5: 17, 6: 6, 7: 3, 8: 3, 9: 1, 10: 2, 13: 1, 19: 1, 21: 1, 23: 1, 29: 1, 40: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1572, 1433])
first layer else: feat size  torch.Size([1572, 1433])
src_prefix_shape  torch.Size([1572])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1572, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1572, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([545, 8, 8])
h_dst  torch.Size([545, 1433])
dst_prefix_shape  (545,)
el size torch.Size([1572, 8, 1])
er szie torch.Size([545, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 46, 2: 77, 3: 105, 4: 81, 5: 77, 6: 40, 7: 23, 8: 26, 9: 9, 10: 12, 11: 6, 12: 6, 13: 2, 14: 2, 15: 5, 16: 4, 17: 3, 18: 2, 19: 3, 21: 3, 22: 1, 23: 1, 29: 1, 32: 1, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([545, 8, 8])
h.size() torch.Size([545, 64])
layer  1
--else: feat size  torch.Size([545, 64])
src_prefix_shape  torch.Size([545])
h_src = h_dst = self.feat_drop(feat)  torch.Size([545, 64])
feat_src = feat_dst  torch.Size([545, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 25, 2: 30, 3: 28, 4: 15, 5: 17, 6: 10, 7: 7, 8: 2, 10: 2, 11: 1, 12: 3}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1651, 1433])
first layer else: feat size  torch.Size([1651, 1433])
src_prefix_shape  torch.Size([1651])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1651, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1651, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([565, 8, 8])
h_dst  torch.Size([565, 1433])
dst_prefix_shape  (565,)
el size torch.Size([1651, 8, 1])
er szie torch.Size([565, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 49, 2: 78, 3: 99, 4: 101, 5: 66, 6: 35, 7: 32, 8: 23, 9: 9, 10: 10, 11: 5, 12: 10, 13: 4, 14: 2, 15: 4, 16: 5, 17: 6, 18: 3, 19: 3, 21: 3, 22: 1, 23: 3, 26: 1, 29: 1, 30: 2, 31: 1, 32: 1, 33: 1, 36: 1, 40: 1, 42: 1, 44: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([565, 8, 8])
h.size() torch.Size([565, 64])
layer  1
--else: feat size  torch.Size([565, 64])
src_prefix_shape  torch.Size([565])
h_src = h_dst = self.feat_drop(feat)  torch.Size([565, 64])
feat_src = feat_dst  torch.Size([565, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 21, 2: 25, 3: 34, 4: 27, 5: 17, 6: 5, 7: 3, 8: 3, 9: 1, 11: 1, 12: 1, 23: 1, 26: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1646, 1433])
first layer else: feat size  torch.Size([1646, 1433])
src_prefix_shape  torch.Size([1646])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1646, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1646, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([554, 8, 8])
h_dst  torch.Size([554, 1433])
dst_prefix_shape  (554,)
el size torch.Size([1646, 8, 1])
er szie torch.Size([554, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 38, 2: 81, 3: 105, 4: 79, 5: 84, 6: 36, 7: 32, 8: 24, 9: 10, 10: 7, 11: 5, 12: 7, 13: 4, 14: 4, 15: 3, 16: 6, 17: 5, 18: 2, 19: 4, 21: 2, 22: 1, 23: 2, 29: 1, 30: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([554, 8, 8])
h.size() torch.Size([554, 64])
layer  1
--else: feat size  torch.Size([554, 64])
src_prefix_shape  torch.Size([554])
h_src = h_dst = self.feat_drop(feat)  torch.Size([554, 64])
feat_src = feat_dst  torch.Size([554, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 23, 2: 40, 3: 29, 4: 13, 5: 17, 6: 9, 7: 4, 8: 2, 9: 1, 18: 1, 36: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1609, 1433])
first layer else: feat size  torch.Size([1609, 1433])
src_prefix_shape  torch.Size([1609])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1609, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1609, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([560, 8, 8])
h_dst  torch.Size([560, 1433])
dst_prefix_shape  (560,)
el size torch.Size([1609, 8, 1])
er szie torch.Size([560, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 46, 2: 86, 3: 102, 4: 84, 5: 62, 6: 45, 7: 29, 8: 22, 9: 12, 10: 16, 11: 7, 12: 6, 13: 1, 14: 2, 15: 6, 16: 5, 17: 6, 18: 2, 19: 1, 21: 2, 23: 3, 29: 1, 30: 2, 31: 1, 32: 2, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([560, 8, 8])
h.size() torch.Size([560, 64])
layer  1
--else: feat size  torch.Size([560, 64])
src_prefix_shape  torch.Size([560])
h_src = h_dst = self.feat_drop(feat)  torch.Size([560, 64])
feat_src = feat_dst  torch.Size([560, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 22, 2: 36, 3: 31, 4: 18, 5: 14, 6: 5, 7: 2, 8: 3, 9: 1, 10: 2, 11: 2, 14: 1, 16: 1, 17: 1, 34: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1578, 1433])
first layer else: feat size  torch.Size([1578, 1433])
src_prefix_shape  torch.Size([1578])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1578, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1578, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([596, 8, 8])
h_dst  torch.Size([596, 1433])
dst_prefix_shape  (596,)
el size torch.Size([1578, 8, 1])
er szie torch.Size([596, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 52, 2: 73, 3: 122, 4: 96, 5: 83, 6: 45, 7: 27, 8: 22, 9: 11, 10: 10, 11: 3, 12: 11, 13: 3, 14: 2, 15: 3, 16: 3, 17: 7, 18: 1, 19: 3, 21: 2, 22: 1, 23: 1, 26: 1, 29: 1, 30: 2, 31: 1, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([596, 8, 8])
h.size() torch.Size([596, 64])
layer  1
--else: feat size  torch.Size([596, 64])
src_prefix_shape  torch.Size([596])
h_src = h_dst = self.feat_drop(feat)  torch.Size([596, 64])
feat_src = feat_dst  torch.Size([596, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 30, 2: 23, 3: 36, 4: 19, 5: 13, 6: 6, 7: 4, 8: 2, 9: 2, 10: 1, 12: 1, 17: 1, 19: 1, 78: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1665, 1433])
first layer else: feat size  torch.Size([1665, 1433])
src_prefix_shape  torch.Size([1665])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1665, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1665, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([608, 8, 8])
h_dst  torch.Size([608, 1433])
dst_prefix_shape  (608,)
el size torch.Size([1665, 8, 1])
er szie torch.Size([608, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 42, 2: 93, 3: 94, 4: 121, 5: 79, 6: 48, 7: 33, 8: 23, 9: 8, 10: 13, 11: 7, 12: 8, 14: 3, 15: 4, 16: 3, 17: 5, 18: 2, 19: 3, 21: 1, 23: 2, 26: 1, 29: 1, 30: 2, 32: 2, 33: 1, 34: 1, 36: 1, 40: 1, 42: 1, 44: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([608, 8, 8])
h.size() torch.Size([608, 64])
layer  1
--else: feat size  torch.Size([608, 64])
src_prefix_shape  torch.Size([608])
h_src = h_dst = self.feat_drop(feat)  torch.Size([608, 64])
feat_src = feat_dst  torch.Size([608, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 20, 2: 28, 3: 24, 4: 31, 5: 14, 6: 9, 7: 5, 9: 1, 10: 1, 12: 2, 15: 2, 16: 1, 17: 1, 42: 1}
return rst 
torch.Size([1, 140, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([140, 7])
shape of y[output_nodes]  torch.Size([140, 7])
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([1132, 1433])
first layer else: feat size  torch.Size([1132, 1433])
src_prefix_shape  torch.Size([1132])
h_src = h_dst = self.feat_drop(feat)  torch.Size([1132, 1433])
self._num_heads 8
self._out_feats 8
feat_src = feat_dst = self.fc(h_src).view  torch.Size([1132, 8, 8])
***** graph.is_block 
feat_dst  torch.Size([287, 8, 8])
h_dst  torch.Size([287, 1433])
dst_prefix_shape  (287,)
el size torch.Size([1132, 8, 1])
er szie torch.Size([287, 8, 1])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 14, 2: 36, 3: 62, 4: 47, 5: 39, 6: 23, 7: 13, 8: 14, 9: 2, 10: 4, 11: 2, 12: 4, 13: 2, 14: 1, 15: 3, 17: 4, 18: 1, 19: 3, 21: 2, 29: 1, 30: 1, 31: 1, 32: 2, 40: 1, 42: 1, 65: 1, 74: 1, 78: 1, 168: 1}
shape of h =layer(block, h)  torch.Size([287, 8, 8])
h.size() torch.Size([287, 64])
layer  1
--else: feat size  torch.Size([287, 64])
src_prefix_shape  torch.Size([287])
h_src = h_dst = self.feat_drop(feat)  torch.Size([287, 64])
feat_src = feat_dst  torch.Size([287, 1, 7])
 builtin mfunc 
 not builtin rfunc 
graph-in degree
{1: 8, 2: 9, 3: 10, 4: 6, 5: 6, 7: 1, 10: 2, 13: 1, 14: 1, 17: 1, 19: 2, 32: 1}
return rst 
torch.Size([1, 48, 7])
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([48, 7])
shape of y[output_nodes]  torch.Size([48, 7])

Run 00 | Epoch 00000 | Loss 1.9515 | Train 0.1429 | Val 0.3000 | Test 0.3022
Total (block generation + training)time/epoch 1.0417582988739014
pure train time/epoch nan

num_input_list  [1342]
