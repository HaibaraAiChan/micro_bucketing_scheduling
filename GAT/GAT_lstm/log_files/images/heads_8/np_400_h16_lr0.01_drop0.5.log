main start at this time 1696110983.4798377
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.002838611602783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.56640625 GB
    Memory Allocated: 0.039813995361328125  GigaBytes
Max Memory Allocated: 0.04107046127319336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.56640625 GB
    Memory Allocated: 0.039818763732910156  GigaBytes
Max Memory Allocated: 0.04107046127319336  GigaBytes

pure train time  0.5115985870361328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.57421875 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.04107046127319336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9523134231567383
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9523 | Train 0.1429 | Val 0.2960 | Test 0.3032
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.043238162994384766  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.0432429313659668  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.11680436134338379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9492558240890503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9493 | Train 0.1357 | Val 0.1260 | Test 0.1460
loading full batch data spends  0.0019865036010742188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.04330301284790039  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.04330778121948242  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.11935830116271973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9459069967269897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9459 | Train 0.1500 | Val 0.1180 | Test 0.1199
loading full batch data spends  0.002008199691772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.043352603912353516  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.04335737228393555  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.12032032012939453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.607421875 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9452165365219116
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9452 | Train 0.2000 | Val 0.1180 | Test 0.1020
loading full batch data spends  0.0019686222076416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043459415435791016  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346418380737305  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.1066596508026123
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9437003135681152
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9437 | Train 0.1857 | Val 0.1140 | Test 0.1025
loading full batch data spends  0.0020055770874023438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043384552001953125  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043389320373535156  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.11720800399780273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9430912733078003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9431 | Train 0.1786 | Val 0.1160 | Test 0.0996
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449892044067383  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450368881225586  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.11537408828735352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9457684755325317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9458 | Train 0.1929 | Val 0.1160 | Test 0.0953
loading full batch data spends  0.002021312713623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349803924560547  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435028076171875  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

pure train time  0.11476421356201172
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.05620145797729492  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9397635459899902
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9398 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459333419799805  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459810256958008  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

pure train time  0.11910176277160645
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9374932050704956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9375 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0021715164184570312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043515682220458984  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043520450592041016  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

pure train time  0.10938048362731934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.056301116943359375  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9411489963531494
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9411 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.001956939697265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04330635070800781  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043311119079589844  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.1198110580444336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9367573261260986
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9368 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019881725311279297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343557357788086  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344034194946289  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.12264847755432129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9356567859649658
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9357 | Train 0.1643 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0019040107727050781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442024230957031  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044425010681152344  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.12152504920959473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9326435327529907
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9326 | Train 0.2000 | Val 0.1200 | Test 0.1035
loading full batch data spends  0.001991748809814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043335914611816406  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334068298339844  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.11926865577697754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9234784841537476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9235 | Train 0.1500 | Val 0.1220 | Test 0.1228
loading full batch data spends  0.0019502639770507812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361295700073242  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361772537231445  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.1189126968383789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9233596324920654
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9234 | Train 0.1571 | Val 0.1200 | Test 0.1233
loading full batch data spends  0.002168893814086914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445075988769531  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044455528259277344  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

pure train time  0.11879158020019531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.056333065032958984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.927176833152771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9272 | Train 0.1643 | Val 0.1180 | Test 0.1219
loading full batch data spends  0.0019576549530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043346405029296875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043351173400878906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11949324607849121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9349005222320557
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9349 | Train 0.1714 | Val 0.1200 | Test 0.1214
loading full batch data spends  0.001985788345336914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043426513671875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343128204345703  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11934685707092285
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9233551025390625
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9234 | Train 0.1714 | Val 0.1200 | Test 0.1223
loading full batch data spends  0.001981019973754883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335927963256836  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336404800415039  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11249685287475586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9326976537704468
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9327 | Train 0.1714 | Val 0.1200 | Test 0.1228
loading full batch data spends  0.001985788345336914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455900192260742  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04456377029418945  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1216592788696289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9039764404296875
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9040 | Train 0.1786 | Val 0.1160 | Test 0.1199
loading full batch data spends  0.001950979232788086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346799850463867  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434727668762207  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11886024475097656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9311059713363647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9311 | Train 0.2000 | Val 0.1180 | Test 0.1078
loading full batch data spends  0.0019981861114501953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324197769165039  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324674606323242  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12400245666503906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9098820686340332
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9099 | Train 0.2143 | Val 0.1180 | Test 0.1074
loading full batch data spends  0.0021314620971679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04364204406738281  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043646812438964844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12122917175292969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9053599834442139
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9054 | Train 0.2143 | Val 0.1200 | Test 0.1083
loading full batch data spends  0.0020041465759277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434112548828125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341602325439453  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1226658821105957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.901540756225586
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9015 | Train 0.2071 | Val 0.1240 | Test 0.1054
loading full batch data spends  0.002126932144165039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0432281494140625  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04323291778564453  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12066841125488281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.899631142616272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.8996 | Train 0.2071 | Val 0.1300 | Test 0.1044
loading full batch data spends  0.002023458480834961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043515682220458984  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043520450592041016  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10685038566589355
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9059511423110962
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9060 | Train 0.2071 | Val 0.1280 | Test 0.1151
loading full batch data spends  0.0021288394927978516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454469680786133  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454946517944336  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11928868293762207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9053401947021484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9053 | Train 0.2000 | Val 0.1260 | Test 0.1141
loading full batch data spends  0.0021691322326660156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043375492095947266  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433802604675293  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11778926849365234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9019557237625122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.9020 | Train 0.2071 | Val 0.1240 | Test 0.1132
loading full batch data spends  0.0021402835845947266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435175895690918  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352235794067383  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12517046928405762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8946332931518555
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.8946 | Train 0.2000 | Val 0.1300 | Test 0.1146
loading full batch data spends  0.002201080322265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350090026855469  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350566864013672  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11065244674682617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9024344682693481
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9024 | Train 0.2143 | Val 0.1340 | Test 0.1214
loading full batch data spends  0.002120494842529297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434718132019043  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347658157348633  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1087338924407959
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9052082300186157
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9052 | Train 0.2571 | Val 0.1220 | Test 0.1151
loading full batch data spends  0.0021746158599853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044489383697509766  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444941520690918  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11183333396911621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8861417770385742
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.8861 | Train 0.2643 | Val 0.1220 | Test 0.1132
loading full batch data spends  0.002034902572631836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.076957702636719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341268539428711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341745376586914  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12084150314331055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8929766416549683
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.8930 | Train 0.2286 | Val 0.1460 | Test 0.1233
loading full batch data spends  0.0022242069244384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.933906555175781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348039627075195  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043485164642333984  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1241142749786377
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9002351760864258
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.9002 | Train 0.1857 | Val 0.1340 | Test 0.1175
loading full batch data spends  0.0021741390228271484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04357004165649414  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04357481002807617  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12076354026794434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8910869359970093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8911 | Train 0.2429 | Val 0.1540 | Test 0.1194
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351615905761719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352092742919922  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12282586097717285
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8781459331512451
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8781 | Train 0.2500 | Val 0.1320 | Test 0.1170
loading full batch data spends  0.0019154548645019531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.267692565917969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043435096740722656  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343986511230469  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12024307250976562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9111627340316772
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9112 | Train 0.2429 | Val 0.1240 | Test 0.1165
loading full batch data spends  0.001993417739868164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338645935058594  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339122772216797  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11988234519958496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012020587921142578  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.891265630722046
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8913 | Train 0.2500 | Val 0.1220 | Test 0.1156
loading full batch data spends  0.002134561538696289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04319906234741211  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04320383071899414  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11881089210510254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.878268837928772
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8783 | Train 0.2571 | Val 0.1160 | Test 0.1146
loading full batch data spends  0.002171039581298828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.981590270996094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331350326538086  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331827163696289  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12223172187805176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012036800384521484  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8836345672607422
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8836 | Train 0.2429 | Val 0.1020 | Test 0.1049
loading full batch data spends  0.0019469261169433594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043473243713378906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347801208496094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10900163650512695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8989654779434204
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.8990 | Train 0.2429 | Val 0.1080 | Test 0.1098
loading full batch data spends  0.0020051002502441406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337024688720703  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337501525878906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10998320579528809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8861500024795532
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8862 | Train 0.2429 | Val 0.1060 | Test 0.1107
loading full batch data spends  0.0019481182098388672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043534278869628906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353904724121094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1200094223022461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.899042010307312
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.8990 | Train 0.2500 | Val 0.1180 | Test 0.1151
loading full batch data spends  0.002256631851196289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044556617736816406  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04456138610839844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11590147018432617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9140822887420654
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.9141 | Train 0.2500 | Val 0.1200 | Test 0.1112
loading full batch data spends  0.0021491050720214844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337882995605469  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338359832763672  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11952424049377441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8866634368896484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8867 | Train 0.2714 | Val 0.1180 | Test 0.1074
loading full batch data spends  0.002184629440307617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339456558227539  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339933395385742  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12163591384887695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8673852682113647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.8674 | Train 0.2786 | Val 0.1160 | Test 0.1127
loading full batch data spends  0.0022399425506591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.981590270996094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335165023803711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335641860961914  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.12496089935302734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9074499607086182
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.9074 | Train 0.2571 | Val 0.1180 | Test 0.1165
loading full batch data spends  0.002173900604248047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1484832763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043383121490478516  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338788986206055  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11063122749328613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012015342712402344  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8954306840896606
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.8954 | Train 0.2500 | Val 0.1140 | Test 0.1136
loading full batch data spends  0.001979351043701172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344367980957031  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043448448181152344  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10305905342102051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8868626356124878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8869 | Train 0.2643 | Val 0.1200 | Test 0.1175
loading full batch data spends  0.0022058486938476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445551872253418  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455995559692383  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11015748977661133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8599317073822021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8599 | Train 0.2571 | Val 0.1220 | Test 0.1175
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043544769287109375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043549537658691406  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10351943969726562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8741525411605835
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8742 | Train 0.2643 | Val 0.1220 | Test 0.1180
loading full batch data spends  0.0022001266479492188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454517364501953  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454994201660156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10297703742980957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8916339874267578
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8916 | Train 0.2643 | Val 0.1220 | Test 0.1180
loading full batch data spends  0.0021371841430664062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340934753417969  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341411590576172  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11474347114562988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8766307830810547
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8766 | Train 0.2714 | Val 0.1240 | Test 0.1199
loading full batch data spends  0.002010345458984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043433189392089844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043437957763671875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11031293869018555
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.872037410736084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8720 | Train 0.2714 | Val 0.1160 | Test 0.1156
loading full batch data spends  0.0019488334655761719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04358530044555664  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04359006881713867  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10948419570922852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8946585655212402
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8947 | Train 0.2786 | Val 0.1160 | Test 0.1175
loading full batch data spends  0.0019872188568115234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346323013305664  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346799850463867  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1034698486328125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8889689445495605
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8890 | Train 0.2714 | Val 0.1060 | Test 0.1151
loading full batch data spends  0.0021283626556396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04365682601928711  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04366159439086914  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10317301750183105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8770149946212769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8770 | Train 0.2857 | Val 0.1100 | Test 0.1151
loading full batch data spends  0.002010345458984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043552398681640625  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043557167053222656  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10841584205627441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8813928365707397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8814 | Train 0.2786 | Val 0.1200 | Test 0.1204
loading full batch data spends  0.002226591110229492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044579505920410156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04458427429199219  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10304641723632812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.858145833015442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8581 | Train 0.2214 | Val 0.1260 | Test 0.1335
loading full batch data spends  0.0020384788513183594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451417922973633  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451894760131836  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1047658920288086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8704968690872192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8705 | Train 0.2143 | Val 0.1300 | Test 0.1388
loading full batch data spends  0.0019659996032714844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043381690979003906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338645935058594  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10749340057373047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8866723775863647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8867 | Train 0.2357 | Val 0.1280 | Test 0.1402
loading full batch data spends  0.0019998550415039062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043389320373535156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339408874511719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10326838493347168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8894582986831665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8895 | Train 0.2500 | Val 0.1260 | Test 0.1301
loading full batch data spends  0.0019507408142089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343700408935547  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434417724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10593533515930176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8595550060272217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8596 | Train 0.2786 | Val 0.1120 | Test 0.1030
loading full batch data spends  0.002042055130004883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340553283691406  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043410301208496094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10907602310180664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8710647821426392
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8711 | Train 0.2571 | Val 0.1180 | Test 0.1194
loading full batch data spends  0.0021495819091796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044605255126953125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044610023498535156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1103203296661377
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8545945882797241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8546 | Train 0.2714 | Val 0.1240 | Test 0.1228
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338836669921875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339313507080078  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10603117942810059
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8614412546157837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8614 | Train 0.2429 | Val 0.1360 | Test 0.1252
loading full batch data spends  0.002145528793334961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333639144897461  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334115982055664  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10775566101074219
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8681424856185913
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8681 | Train 0.2214 | Val 0.1420 | Test 0.1209
loading full batch data spends  0.0021767616271972656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356050491333008  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356527328491211  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1056218147277832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012066364288330078  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8894596099853516
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8895 | Train 0.2286 | Val 0.1360 | Test 0.1262
loading full batch data spends  0.0015110969543457031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338502883911133  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338979721069336  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10697436332702637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8528231382369995
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8528 | Train 0.2357 | Val 0.1360 | Test 0.1277
loading full batch data spends  0.0016398429870605469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044442176818847656  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444694519042969  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1025383472442627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.872530460357666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8725 | Train 0.2786 | Val 0.1220 | Test 0.1233
loading full batch data spends  0.0014336109161376953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331064224243164  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331541061401367  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.0984501838684082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8672059774398804
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8672 | Train 0.3000 | Val 0.1220 | Test 0.1214
loading full batch data spends  0.0014934539794921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351329803466797  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351806640625  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09700202941894531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012044906616210938  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8679065704345703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8679 | Train 0.3000 | Val 0.1040 | Test 0.1199
loading full batch data spends  0.0015988349914550781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342460632324219  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342937469482422  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10387849807739258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8584328889846802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8584 | Train 0.3000 | Val 0.1020 | Test 0.1228
loading full batch data spends  0.001500844955444336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352712631225586  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353189468383789  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1000969409942627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8424619436264038
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8425 | Train 0.2643 | Val 0.1300 | Test 0.1335
loading full batch data spends  0.0014667510986328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043501853942871094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043506622314453125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10422134399414062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8363889455795288
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8364 | Train 0.2643 | Val 0.1200 | Test 0.1310
loading full batch data spends  0.0014824867248535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.574920654296875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332447052001953  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332923889160156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10533857345581055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012042045593261719  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.862426519393921
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8624 | Train 0.2929 | Val 0.1220 | Test 0.1310
loading full batch data spends  0.0014367103576660156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346609115600586  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347085952758789  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09674549102783203
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8183711767196655
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8184 | Train 0.2500 | Val 0.1300 | Test 0.1364
loading full batch data spends  0.001628875732421875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043517112731933594  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043521881103515625  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10079336166381836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8285125494003296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8285 | Train 0.2429 | Val 0.1200 | Test 0.1306
loading full batch data spends  0.0014262199401855469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343366622924805  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343843460083008  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1108543872833252
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8397284746170044
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8397 | Train 0.2857 | Val 0.1340 | Test 0.1354
loading full batch data spends  0.0016636848449707031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361104965209961  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361581802368164  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10256171226501465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012093067169189453  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8418879508972168
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8419 | Train 0.2714 | Val 0.1400 | Test 0.1364
loading full batch data spends  0.0014467239379882812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344511032104492  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344987869262695  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10495924949645996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8507369756698608
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8507 | Train 0.2929 | Val 0.1360 | Test 0.1364
loading full batch data spends  0.0016505718231201172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361438751220703  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361915588378906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.11070442199707031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012119770050048828  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8410415649414062
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8410 | Train 0.3000 | Val 0.1360 | Test 0.1397
loading full batch data spends  0.0014615058898925781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335784912109375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336261749267578  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10811376571655273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.836978793144226
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8370 | Train 0.3143 | Val 0.1300 | Test 0.1335
loading full batch data spends  0.001524209976196289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443073272705078  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443550109863281  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10202908515930176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8152390718460083
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8152 | Train 0.3143 | Val 0.1400 | Test 0.1277
loading full batch data spends  0.0014464855194091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341888427734375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342365264892578  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09946465492248535
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8192811012268066
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8193 | Train 0.3214 | Val 0.1420 | Test 0.1233
loading full batch data spends  0.0014843940734863281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449176788330078  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449653625488281  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.1039724349975586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8182196617126465
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8182 | Train 0.3214 | Val 0.1500 | Test 0.1325
loading full batch data spends  0.0015959739685058594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043346405029296875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043351173400878906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09969091415405273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8036915063858032
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8037 | Train 0.3000 | Val 0.1400 | Test 0.1373
loading full batch data spends  0.0016434192657470703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349374771118164  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349851608276367  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10065722465515137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012095451354980469  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8025881052017212
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8026 | Train 0.2929 | Val 0.1380 | Test 0.1330
loading full batch data spends  0.0016090869903564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04326152801513672  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04326629638671875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09957742691040039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8242793083190918
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8243 | Train 0.3357 | Val 0.1420 | Test 0.1412
loading full batch data spends  0.0014905929565429688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04437971115112305  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04438447952270508  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10553312301635742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8057514429092407
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8058 | Train 0.3786 | Val 0.1640 | Test 0.1412
loading full batch data spends  0.0014407634735107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352855682373047  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435333251953125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09838008880615234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7774293422698975
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.7774 | Train 0.3429 | Val 0.1580 | Test 0.1393
loading full batch data spends  0.0017611980438232422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453468322753906  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044539451599121094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10036373138427734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7632627487182617
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.7633 | Train 0.3571 | Val 0.1580 | Test 0.1426
loading full batch data spends  0.0015723705291748047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.124641418457031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340171813964844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340648651123047  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10127472877502441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7994123697280884
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.7994 | Train 0.3571 | Val 0.1640 | Test 0.1407
loading full batch data spends  0.0015835762023925781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043524742126464844  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043529510498046875  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.09777998924255371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.782366394996643
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.7824 | Train 0.3357 | Val 0.1540 | Test 0.1470
loading full batch data spends  0.0015170574188232422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.719329833984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043384552001953125  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043389320373535156  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10977792739868164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7802952527999878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.7803 | Train 0.3143 | Val 0.1600 | Test 0.1325
loading full batch data spends  0.001508474349975586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04465913772583008  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04466390609741211  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

pure train time  0.10601663589477539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057314395904541016  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.808401107788086
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8084 | Train 0.2929 | Val 0.1520 | Test 0.1364
loading full batch data spends  0.0014379024505615234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043456077575683594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043460845947265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09571719169616699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8128129243850708
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.8128 | Train 0.3143 | Val 0.1540 | Test 0.1349
loading full batch data spends  0.0016467571258544922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341268539428711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341745376586914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1056222915649414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7795286178588867
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.7795 | Train 0.3571 | Val 0.1660 | Test 0.1480
loading full batch data spends  0.0014374256134033203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331254959106445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043317317962646484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10007071495056152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7601673603057861
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.7602 | Train 0.3786 | Val 0.1840 | Test 0.1644
loading full batch data spends  0.0016438961029052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043365478515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337024688720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10069775581359863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.823904037475586
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8239 | Train 0.3929 | Val 0.2060 | Test 0.1736
loading full batch data spends  0.0014290809631347656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043505191802978516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350996017456055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09871792793273926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7731921672821045
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.7732 | Train 0.3857 | Val 0.2040 | Test 0.1746
loading full batch data spends  0.0014872550964355469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338264465332031  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043387413024902344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09519743919372559
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012036800384521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7638887166976929
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.7639 | Train 0.4071 | Val 0.1840 | Test 0.1625
loading full batch data spends  0.0014233589172363281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452991485595703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453468322753906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11429405212402344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.776465654373169
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.7765 | Train 0.3643 | Val 0.1720 | Test 0.1567
loading full batch data spends  0.001543283462524414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044522762298583984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044527530670166016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10411906242370605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7649060487747192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.7649 | Train 0.3429 | Val 0.1740 | Test 0.1523
loading full batch data spends  0.0014395713806152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435795783996582  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043584346771240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09979915618896484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7788137197494507
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.7788 | Train 0.3786 | Val 0.1700 | Test 0.1649
loading full batch data spends  0.0015075206756591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043473243713378906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347801208496094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10030221939086914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.733790636062622
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.7338 | Train 0.3929 | Val 0.1760 | Test 0.1678
loading full batch data spends  0.001434326171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.9577484130859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043413639068603516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341840744018555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10008716583251953
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7558337450027466
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.7558 | Train 0.3857 | Val 0.1920 | Test 0.1731
loading full batch data spends  0.0016448497772216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451131820678711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451608657836914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09690284729003906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7628828287124634
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.7629 | Train 0.3857 | Val 0.2040 | Test 0.1842
loading full batch data spends  0.001444101333618164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444941520690918  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449892044067383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09707355499267578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7491060495376587
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.7491 | Train 0.3786 | Val 0.1860 | Test 0.1726
loading full batch data spends  0.0015015602111816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043340206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043344974517822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08502793312072754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7132481336593628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.7132 | Train 0.3786 | Val 0.1840 | Test 0.1668
loading full batch data spends  0.0014352798461914062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342222213745117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434269905090332  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0820615291595459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7297332286834717
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.7297 | Train 0.3786 | Val 0.1800 | Test 0.1726
loading full batch data spends  0.0014348030090332031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043511390686035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351615905761719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08475494384765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7725911140441895
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.7726 | Train 0.3929 | Val 0.1900 | Test 0.1741
loading full batch data spends  0.0014264583587646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452943801879883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453420639038086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0863492488861084
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7286485433578491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.7286 | Train 0.4286 | Val 0.2060 | Test 0.1775
loading full batch data spends  0.0014452934265136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043427467346191406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343223571777344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08828210830688477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7581098079681396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.7581 | Train 0.4286 | Val 0.2000 | Test 0.1838
loading full batch data spends  0.001447916030883789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346179962158203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346656799316406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08969259262084961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7618211507797241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.7618 | Train 0.4071 | Val 0.1860 | Test 0.1833
loading full batch data spends  0.0015151500701904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04441356658935547  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444183349609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08320045471191406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7600042819976807
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.7600 | Train 0.3929 | Val 0.1880 | Test 0.1818
loading full batch data spends  0.0015077590942382812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.00543212890625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342508316040039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342985153198242  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08615708351135254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7341357469558716
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.7341 | Train 0.3714 | Val 0.1860 | Test 0.1721
loading full batch data spends  0.0015032291412353516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452848434448242  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453325271606445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0858156681060791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7205479145050049
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.7205 | Train 0.3429 | Val 0.1940 | Test 0.1625
loading full batch data spends  0.0013861656188964844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433502197265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335498809814453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08468961715698242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7471517324447632
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.7472 | Train 0.3714 | Val 0.2080 | Test 0.1809
loading full batch data spends  0.0014185905456542969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043422698974609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043427467346191406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08566904067993164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7421557903289795
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.7422 | Train 0.3643 | Val 0.1980 | Test 0.1978
loading full batch data spends  0.0014019012451171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347801208496094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348278045654297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08365941047668457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7364739179611206
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.7365 | Train 0.3786 | Val 0.2200 | Test 0.2007
loading full batch data spends  0.0014467239379882812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337739944458008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338216781616211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08638191223144531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.735816240310669
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.7358 | Train 0.3929 | Val 0.2160 | Test 0.2118
loading full batch data spends  0.001405954360961914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5987625122070312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04317522048950195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043179988861083984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08455920219421387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7393665313720703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.7394 | Train 0.3643 | Val 0.2140 | Test 0.1867
loading full batch data spends  0.0014531612396240234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457902908325195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044583797454833984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0810847282409668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7462352514266968
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.7462 | Train 0.3214 | Val 0.1940 | Test 0.1746
loading full batch data spends  0.0013980865478515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044322967529296875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044327735900878906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08324337005615234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.755350112915039
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.7554 | Train 0.3214 | Val 0.1900 | Test 0.1731
loading full batch data spends  0.0014312267303466797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339027404785156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043395042419433594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08211946487426758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.767465591430664
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.7675 | Train 0.3214 | Val 0.1900 | Test 0.1673
loading full batch data spends  0.0013883113861083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340410232543945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043408870697021484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08585667610168457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7528311014175415
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.7528 | Train 0.3500 | Val 0.1940 | Test 0.1736
loading full batch data spends  0.0014395713806152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347848892211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348325729370117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.0861518383026123
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7613481283187866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.7613 | Train 0.3643 | Val 0.2180 | Test 0.1818
loading full batch data spends  0.0013875961303710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449605941772461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450082778930664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08392000198364258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.783336877822876
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.7833 | Train 0.4071 | Val 0.2340 | Test 0.2031
loading full batch data spends  0.0014355182647705078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5987625122070312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454517364501953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454994201660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08106875419616699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7696372270584106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.7696 | Train 0.4429 | Val 0.2180 | Test 0.1978
loading full batch data spends  0.0013799667358398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043570518493652344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043575286865234375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08471059799194336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7460181713104248
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.7460 | Train 0.4286 | Val 0.2160 | Test 0.1934
loading full batch data spends  0.0014317035675048828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433349609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333972930908203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08596491813659668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.712515950202942
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.7125 | Train 0.4286 | Val 0.2100 | Test 0.1900
loading full batch data spends  0.0013875961303710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334068298339844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334545135498047  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08467936515808105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.734155297279358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.7342 | Train 0.4286 | Val 0.2020 | Test 0.1871
loading full batch data spends  0.0014791488647460938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332113265991211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332590103149414  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08505487442016602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.734838843345642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.7348 | Train 0.4143 | Val 0.2020 | Test 0.1707
loading full batch data spends  0.001436471939086914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448127746582031  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044486045837402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08348560333251953
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7510088682174683
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.7510 | Train 0.3929 | Val 0.1920 | Test 0.1659
loading full batch data spends  0.0014414787292480469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043651580810546875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043656349182128906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.07988524436950684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012125015258789062  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7478487491607666
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7478 | Train 0.4071 | Val 0.1880 | Test 0.1707
loading full batch data spends  0.0013952255249023438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043272972106933594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043277740478515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08463025093078613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7878752946853638
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.7879 | Train 0.4214 | Val 0.1940 | Test 0.1828
loading full batch data spends  0.0014142990112304688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455089569091797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445556640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08504319190979004
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7350473403930664
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7350 | Train 0.4286 | Val 0.1880 | Test 0.1929
loading full batch data spends  0.0014083385467529297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043609619140625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361438751220703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08331990242004395
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.718319058418274
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.7183 | Train 0.4214 | Val 0.1880 | Test 0.1939
loading full batch data spends  0.0014524459838867188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5987625122070312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043433189392089844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043437957763671875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08367443084716797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.737828016281128
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.7378 | Train 0.4357 | Val 0.1880 | Test 0.1891
loading full batch data spends  0.0012998580932617188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7179718017578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044592857360839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044597625732421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08518648147583008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7246589660644531
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.7247 | Train 0.4500 | Val 0.2000 | Test 0.1910
loading full batch data spends  0.0014307498931884766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6226043701171875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341697692871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342174530029297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08625459671020508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7507845163345337
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7508 | Train 0.4286 | Val 0.2100 | Test 0.1804
loading full batch data spends  0.001392364501953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459428787231445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044599056243896484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.08497214317321777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.743325114250183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.7433 | Train 0.3714 | Val 0.1980 | Test 0.1789
loading full batch data spends  0.0013489723205566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6941299438476562e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043424129486083984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043428897857666016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09472107887268066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.746736764907837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.7467 | Train 0.4071 | Val 0.2060 | Test 0.1847
loading full batch data spends  0.001974821090698242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043442726135253906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344749450683594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10612678527832031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7484620809555054
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7485 | Train 0.4214 | Val 0.2100 | Test 0.1934
loading full batch data spends  0.002003192901611328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444788932800293  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448366165161133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10881948471069336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7362829446792603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.7363 | Train 0.4286 | Val 0.1980 | Test 0.2031
loading full batch data spends  0.002134084701538086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336214065551758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336690902709961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11101388931274414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7067002058029175
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.7067 | Train 0.3929 | Val 0.2140 | Test 0.2079
loading full batch data spends  0.0020329952239990234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04439973831176758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04440450668334961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1062021255493164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7182378768920898
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.7182 | Train 0.3714 | Val 0.2180 | Test 0.2084
loading full batch data spends  0.001974821090698242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04359579086303711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04360055923461914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10995841026306152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7048993110656738
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.7049 | Train 0.4143 | Val 0.2160 | Test 0.2162
loading full batch data spends  0.0025796890258789062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   7.486343383789062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043430328369140625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043435096740722656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10496163368225098
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7239017486572266
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7239 | Train 0.4357 | Val 0.2160 | Test 0.2016
loading full batch data spends  0.002180337905883789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043265342712402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043270111083984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10417389869689941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6902275085449219
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.6902 | Train 0.3714 | Val 0.2180 | Test 0.1876
loading full batch data spends  0.002117156982421875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.695487976074219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334115982055664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334592819213867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10503101348876953
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7004741430282593
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.7005 | Train 0.3786 | Val 0.2180 | Test 0.1881
loading full batch data spends  0.0020444393157958984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04355812072753906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043562889099121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10315489768981934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7375544309616089
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.7376 | Train 0.4000 | Val 0.2140 | Test 0.1896
loading full batch data spends  0.0020132064819335938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347372055053711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347848892211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11044454574584961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7267425060272217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.7267 | Train 0.4714 | Val 0.2320 | Test 0.2103
loading full batch data spends  0.0019631385803222656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354095458984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354572296142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10365152359008789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.698991298675537
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.6990 | Train 0.4786 | Val 0.2440 | Test 0.2258
loading full batch data spends  0.0021708011627197266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324626922607422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04325103759765625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10383486747741699
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6767103672027588
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.6767 | Train 0.4643 | Val 0.2520 | Test 0.2273
loading full batch data spends  0.0019502639770507812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044550418853759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445551872253418  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10591864585876465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7321834564208984
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.7322 | Train 0.5000 | Val 0.2580 | Test 0.2360
loading full batch data spends  0.0020058155059814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043456077575683594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043460845947265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10405254364013672
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.705520749092102
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.7055 | Train 0.4643 | Val 0.2520 | Test 0.2258
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043604373931884766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0436091423034668  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1026909351348877
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.679356336593628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.6794 | Train 0.4357 | Val 0.2480 | Test 0.2103
loading full batch data spends  0.0019943714141845703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346466064453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346942901611328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11319470405578613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.727463960647583
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.7275 | Train 0.4214 | Val 0.2380 | Test 0.2070
loading full batch data spends  0.0019690990447998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043560028076171875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043564796447753906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10532999038696289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6997809410095215
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.6998 | Train 0.4071 | Val 0.2380 | Test 0.2050
loading full batch data spends  0.0019352436065673828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044464111328125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446887969970703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10395193099975586
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6638386249542236
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.6638 | Train 0.4071 | Val 0.2380 | Test 0.2220
loading full batch data spends  0.0019519329071044922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336881637573242  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337358474731445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10872960090637207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.660555124282837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.6606 | Train 0.4214 | Val 0.2600 | Test 0.2340
loading full batch data spends  0.001987934112548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341459274291992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341936111450195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1043703556060791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.67024827003479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.6702 | Train 0.3929 | Val 0.2260 | Test 0.2152
loading full batch data spends  0.0019652843475341797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043308258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043313026428222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10552668571472168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7508721351623535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.7509 | Train 0.3643 | Val 0.2100 | Test 0.2050
loading full batch data spends  0.002069234848022461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435333251953125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353809356689453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10130882263183594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7183550596237183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.7184 | Train 0.3929 | Val 0.2280 | Test 0.2166
loading full batch data spends  0.0019674301147460938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354286193847656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043547630310058594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10697078704833984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7335163354873657
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7335 | Train 0.4143 | Val 0.2420 | Test 0.2326
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457235336303711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457712173461914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10365676879882812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7208601236343384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.7209 | Train 0.4000 | Val 0.2540 | Test 0.2321
loading full batch data spends  0.001966238021850586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453277587890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453754425048828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10418248176574707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7092188596725464
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.7092 | Train 0.4143 | Val 0.2480 | Test 0.2210
loading full batch data spends  0.0019881725311279297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043419837951660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342460632324219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10512614250183105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7066987752914429
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.7067 | Train 0.3286 | Val 0.2340 | Test 0.2094
loading full batch data spends  0.001969575881958008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344034194946289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344511032104492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10568094253540039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7632886171340942
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.7633 | Train 0.3429 | Val 0.2500 | Test 0.2191
loading full batch data spends  0.002035379409790039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343557357788086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344034194946289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10853409767150879
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.727882981300354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.7279 | Train 0.4000 | Val 0.2660 | Test 0.2398
loading full batch data spends  0.0019643306732177734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043496131896972656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350090026855469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10754251480102539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6959775686264038
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.6960 | Train 0.4286 | Val 0.2500 | Test 0.2321
loading full batch data spends  0.0021638870239257812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340505599975586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340982437133789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10889816284179688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6468923091888428
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.6469 | Train 0.3857 | Val 0.2200 | Test 0.2045
loading full batch data spends  0.0019707679748535156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043257713317871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043262481689453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10697817802429199
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6857330799102783
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.6857 | Train 0.3500 | Val 0.1980 | Test 0.1997
loading full batch data spends  0.002034425735473633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044533729553222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453849792480469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11124515533447266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7647197246551514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.7647 | Train 0.3714 | Val 0.2300 | Test 0.2302
loading full batch data spends  0.0019767284393310547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454517364501953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454994201660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10116291046142578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6657540798187256
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.6658 | Train 0.4000 | Val 0.2440 | Test 0.2418
loading full batch data spends  0.002306222915649414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.267692565917969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04456138610839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04456615447998047  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10623955726623535
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6761759519577026
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.6762 | Train 0.4000 | Val 0.2420 | Test 0.2108
loading full batch data spends  0.001970529556274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448366165161133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448843002319336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10236573219299316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6773854494094849
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.6774 | Train 0.3857 | Val 0.2340 | Test 0.2094
loading full batch data spends  0.001993894577026367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344892501831055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345369338989258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1041116714477539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7358342409133911
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.7358 | Train 0.4071 | Val 0.2460 | Test 0.2273
loading full batch data spends  0.001955747604370117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351043701171875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351520538330078  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11024880409240723
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.671618938446045
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.6716 | Train 0.4929 | Val 0.2240 | Test 0.2234
loading full batch data spends  0.0019979476928710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043262481689453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043267250061035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10863828659057617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6978516578674316
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.6979 | Train 0.4929 | Val 0.2020 | Test 0.2137
loading full batch data spends  0.0019483566284179688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345703125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346179962158203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10166597366333008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6934242248535156
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.6934 | Train 0.4500 | Val 0.2620 | Test 0.2374
loading full batch data spends  0.0020170211791992188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353618621826172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354095458984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10317850112915039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012076854705810547  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6169912815093994
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.6170 | Train 0.4714 | Val 0.2480 | Test 0.2466
loading full batch data spends  0.0021207332611083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452800750732422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453277587890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11034560203552246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.67145574092865
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.6715 | Train 0.4357 | Val 0.2420 | Test 0.2326
loading full batch data spends  0.0020020008087158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04365396499633789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04365873336791992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11986160278320312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012103557586669922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6615115404129028
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.6615 | Train 0.4143 | Val 0.2420 | Test 0.2278
loading full batch data spends  0.001969575881958008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333639144897461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334115982055664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10328531265258789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7064549922943115
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.7065 | Train 0.4143 | Val 0.2380 | Test 0.2224
loading full batch data spends  0.0020051002502441406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341745376586914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342222213745117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11046409606933594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6657357215881348
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.6657 | Train 0.4000 | Val 0.2400 | Test 0.2162
loading full batch data spends  0.002281665802001953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451942443847656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044524192810058594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10338163375854492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.702042579650879
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.7020 | Train 0.4000 | Val 0.2340 | Test 0.2200
loading full batch data spends  0.001972675323486328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343748092651367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434422492980957  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1040499210357666
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7116917371749878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.7117 | Train 0.4000 | Val 0.2320 | Test 0.2234
loading full batch data spends  0.001978635787963867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338693618774414  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339170455932617  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11011981964111328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6765121221542358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.6765 | Train 0.4500 | Val 0.2580 | Test 0.2437
loading full batch data spends  0.0020189285278320312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043413639068603516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341840744018555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1092672348022461
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6588653326034546
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.6589 | Train 0.4071 | Val 0.2620 | Test 0.2398
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435481071472168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04355287551879883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10436129570007324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7169262170791626
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.7169 | Train 0.4000 | Val 0.2640 | Test 0.2408
loading full batch data spends  0.0020058155059814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448413848876953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448890686035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10289907455444336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6799882650375366
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.6800 | Train 0.4571 | Val 0.2720 | Test 0.2485
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451704025268555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452180862426758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11141538619995117
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6562581062316895
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.6563 | Train 0.4500 | Val 0.2680 | Test 0.2519
loading full batch data spends  0.0020020008087158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351615905761719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352092742919922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10227727890014648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.659950852394104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.6600 | Train 0.4571 | Val 0.2660 | Test 0.2539
loading full batch data spends  0.0019762516021728516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331636428833008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332113265991211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10426878929138184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6501657962799072
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.6502 | Train 0.4571 | Val 0.2900 | Test 0.2558
loading full batch data spends  0.001992464065551758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044741153717041016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04474592208862305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10519623756408691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6210910081863403
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.6211 | Train 0.4429 | Val 0.2900 | Test 0.2640
loading full batch data spends  0.002001523971557617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0431818962097168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04318666458129883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10625362396240234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6164900064468384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.6165 | Train 0.4357 | Val 0.2660 | Test 0.2413
loading full batch data spends  0.001992464065551758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044465065002441406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446983337402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10542416572570801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.654529333114624
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.6545 | Train 0.4429 | Val 0.2660 | Test 0.2398
loading full batch data spends  0.0019605159759521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044493675231933594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044498443603515625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10421299934387207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6282986402511597
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00200 | Loss 1.6283 | Train 0.5000 | Val 0.2880 | Test 0.2611
loading full batch data spends  0.0021889209747314453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043343544006347656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334831237792969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10372328758239746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6287530660629272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00201 | Loss 1.6288 | Train 0.4571 | Val 0.2780 | Test 0.2606
loading full batch data spends  0.0019693374633789062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043476104736328125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043480873107910156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10990023612976074
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6527040004730225
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00202 | Loss 1.6527 | Train 0.4571 | Val 0.2720 | Test 0.2592
loading full batch data spends  0.002011537551879883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043557167053222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356193542480469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10274171829223633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012093067169189453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6295305490493774
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00203 | Loss 1.6295 | Train 0.4786 | Val 0.2700 | Test 0.2606
loading full batch data spends  0.002028226852416992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043354034423828125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043358802795410156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11309242248535156
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6173776388168335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00204 | Loss 1.6174 | Train 0.5000 | Val 0.2740 | Test 0.2539
loading full batch data spends  0.0020372867584228516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043406009674072266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434107780456543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1129453182220459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012036800384521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6539067029953003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00205 | Loss 1.6539 | Train 0.5071 | Val 0.2800 | Test 0.2568
loading full batch data spends  0.0020749568939208984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.7670135498046875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352712631225586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353189468383789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10960793495178223
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.627221703529358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00206 | Loss 1.6272 | Train 0.5143 | Val 0.2700 | Test 0.2568
loading full batch data spends  0.002042055130004883
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043309688568115234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043314456939697266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10482192039489746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6019459962844849
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00207 | Loss 1.6019 | Train 0.5214 | Val 0.2740 | Test 0.2510
loading full batch data spends  0.0019538402557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043308258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043313026428222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10406041145324707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5642826557159424
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00208 | Loss 1.5643 | Train 0.5357 | Val 0.2660 | Test 0.2485
loading full batch data spends  0.0021736621856689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044507503509521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044512271881103516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10750031471252441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6189090013504028
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00209 | Loss 1.6189 | Train 0.5286 | Val 0.2700 | Test 0.2510
loading full batch data spends  0.0019414424896240234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04366159439086914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04366636276245117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10284900665283203
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.647078514099121
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00210 | Loss 1.6471 | Train 0.4571 | Val 0.2400 | Test 0.2253
loading full batch data spends  0.002009868621826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332780838012695  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043332576751708984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10344457626342773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6864441633224487
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00211 | Loss 1.6864 | Train 0.5071 | Val 0.2560 | Test 0.2432
loading full batch data spends  0.0019638538360595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445857048034668  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459047317504883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10360574722290039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6030516624450684
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00212 | Loss 1.6031 | Train 0.5286 | Val 0.2640 | Test 0.2447
loading full batch data spends  0.002017498016357422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463052749633789  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463529586791992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10495448112487793
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6245030164718628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00213 | Loss 1.6245 | Train 0.4571 | Val 0.2500 | Test 0.2162
loading full batch data spends  0.0019648075103759766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454469680786133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454946517944336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1038670539855957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6695290803909302
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00214 | Loss 1.6695 | Train 0.4857 | Val 0.2460 | Test 0.2307
loading full batch data spends  0.0019524097442626953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351377487182617  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435185432434082  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10796213150024414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6706557273864746
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00215 | Loss 1.6707 | Train 0.3571 | Val 0.2280 | Test 0.2103
loading full batch data spends  0.0019795894622802734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353666305541992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354143142700195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11688804626464844
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6787832975387573
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00216 | Loss 1.6788 | Train 0.3357 | Val 0.1840 | Test 0.1896
loading full batch data spends  0.002065896987915039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449796676635742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450273513793945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11392331123352051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7242825031280518
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00217 | Loss 1.7243 | Train 0.3500 | Val 0.2420 | Test 0.2205
loading full batch data spends  0.0021393299102783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337787628173828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338264465332031  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10220694541931152
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6833299398422241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00218 | Loss 1.6833 | Train 0.4786 | Val 0.2860 | Test 0.2442
loading full batch data spends  0.002173185348510742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04362010955810547  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0436248779296875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10396695137023926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012125015258789062  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6718089580535889
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00219 | Loss 1.6718 | Train 0.4500 | Val 0.2920 | Test 0.2558
loading full batch data spends  0.0019500255584716797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433344841003418  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333925247192383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1026151180267334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.633155107498169
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00220 | Loss 1.6332 | Train 0.4929 | Val 0.2800 | Test 0.2548
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338359832763672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04338836669921875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10832381248474121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.686543583869934
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00221 | Loss 1.6865 | Train 0.4357 | Val 0.2680 | Test 0.2582
loading full batch data spends  0.001973390579223633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433502197265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335498809814453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1029362678527832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6890922784805298
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00222 | Loss 1.6891 | Train 0.4571 | Val 0.2840 | Test 0.2587
loading full batch data spends  0.002021312713623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343271255493164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343748092651367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.09946155548095703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6368355751037598
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00223 | Loss 1.6368 | Train 0.4786 | Val 0.2900 | Test 0.2558
loading full batch data spends  0.0019626617431640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044499874114990234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044504642486572266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10861921310424805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6148866415023804
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00224 | Loss 1.6149 | Train 0.5214 | Val 0.2900 | Test 0.2519
loading full batch data spends  0.002012014389038086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350709915161133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351186752319336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10757756233215332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5922209024429321
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00225 | Loss 1.5922 | Train 0.4857 | Val 0.2660 | Test 0.2379
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0436091423034668  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04361391067504883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10300707817077637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6661345958709717
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00226 | Loss 1.6661 | Train 0.5000 | Val 0.2620 | Test 0.2326
loading full batch data spends  0.0020253658294677734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353141784667969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353618621826172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1047658920288086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6263444423675537
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00227 | Loss 1.6263 | Train 0.5000 | Val 0.2800 | Test 0.2490
loading full batch data spends  0.001961946487426758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347562789916992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348039627075195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1039421558380127
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.590171217918396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00228 | Loss 1.5902 | Train 0.4929 | Val 0.2940 | Test 0.2684
loading full batch data spends  0.0020041465759277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349517822265625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349994659423828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11007905006408691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6334542036056519
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00229 | Loss 1.6335 | Train 0.4429 | Val 0.2820 | Test 0.2452
loading full batch data spends  0.0019702911376953125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435338020324707  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043538570404052734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10542917251586914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.617094874382019
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00230 | Loss 1.6171 | Train 0.4429 | Val 0.2640 | Test 0.2437
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04440450668334961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04440927505493164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1054527759552002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.605892539024353
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00231 | Loss 1.6059 | Train 0.4357 | Val 0.2660 | Test 0.2403
loading full batch data spends  0.0019626617431640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043346405029296875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043351173400878906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10228943824768066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6508877277374268
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00232 | Loss 1.6509 | Train 0.4214 | Val 0.2680 | Test 0.2360
loading full batch data spends  0.0019829273223876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04458427429199219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04458904266357422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10710835456848145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6810212135314941
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00233 | Loss 1.6810 | Train 0.5000 | Val 0.2920 | Test 0.2573
loading full batch data spends  0.0019598007202148438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044528961181640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044533729553222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11127519607543945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6139726638793945
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00234 | Loss 1.6140 | Train 0.4071 | Val 0.2540 | Test 0.2345
loading full batch data spends  0.0020093917846679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043442726135253906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344749450683594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10315442085266113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.626625657081604
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00235 | Loss 1.6266 | Train 0.4071 | Val 0.2460 | Test 0.2345
loading full batch data spends  0.0019478797912597656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452991485595703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453468322753906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1126718521118164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6219638586044312
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00236 | Loss 1.6220 | Train 0.4714 | Val 0.2540 | Test 0.2413
loading full batch data spends  0.0022461414337158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043489933013916016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349470138549805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11034297943115234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6335762739181519
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00237 | Loss 1.6336 | Train 0.5286 | Val 0.3140 | Test 0.2882
loading full batch data spends  0.0020122528076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.030632019042969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044385433197021484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044390201568603516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10135269165039062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5395839214324951
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00238 | Loss 1.5396 | Train 0.4643 | Val 0.3120 | Test 0.2751
loading full batch data spends  0.0019948482513427734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354238510131836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354715347290039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11509037017822266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012071609497070312  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6612954139709473
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00239 | Loss 1.6613 | Train 0.4429 | Val 0.3240 | Test 0.2805
loading full batch data spends  0.001965761184692383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336833953857422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337310791015625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11085724830627441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.627231478691101
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00240 | Loss 1.6272 | Train 0.4143 | Val 0.3280 | Test 0.2935
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352521896362305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352998733520508  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10299110412597656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7258226871490479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00241 | Loss 1.7258 | Train 0.4786 | Val 0.3280 | Test 0.2897
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349231719970703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349708557128906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11357498168945312
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6128970384597778
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00242 | Loss 1.6129 | Train 0.5000 | Val 0.3060 | Test 0.2790
loading full batch data spends  0.001982450485229492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043416500091552734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043421268463134766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11471390724182129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6801048517227173
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00243 | Loss 1.6801 | Train 0.5357 | Val 0.2800 | Test 0.2737
loading full batch data spends  0.0019676685333251953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043477535247802734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043482303619384766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10631394386291504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6030184030532837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00244 | Loss 1.6030 | Train 0.5643 | Val 0.3260 | Test 0.2868
loading full batch data spends  0.0020203590393066406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043537139892578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043541908264160156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10566830635070801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.571568489074707
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00245 | Loss 1.5716 | Train 0.4857 | Val 0.3160 | Test 0.2689
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351615905761719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352092742919922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11435556411743164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5898311138153076
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00246 | Loss 1.5898 | Train 0.4786 | Val 0.2920 | Test 0.2558
loading full batch data spends  0.0026133060455322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.57763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340219497680664  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340696334838867  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10687422752380371
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5737240314483643
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00247 | Loss 1.5737 | Train 0.4786 | Val 0.2860 | Test 0.2490
loading full batch data spends  0.0020241737365722656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347801208496094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348278045654297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11099386215209961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6048558950424194
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00248 | Loss 1.6049 | Train 0.4643 | Val 0.2900 | Test 0.2500
loading full batch data spends  0.0020346641540527344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340171813964844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340648651123047  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10887455940246582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012095451354980469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5415414571762085
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00249 | Loss 1.5415 | Train 0.4643 | Val 0.2960 | Test 0.2524
loading full batch data spends  0.0021283626556396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334259033203125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334735870361328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10247588157653809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.622535228729248
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00250 | Loss 1.6225 | Train 0.4643 | Val 0.3060 | Test 0.2616
loading full batch data spends  0.001995563507080078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044310569763183594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044315338134765625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10997176170349121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.60248863697052
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00251 | Loss 1.6025 | Train 0.4714 | Val 0.2960 | Test 0.2664
loading full batch data spends  0.001964092254638672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351663589477539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352140426635742  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10636568069458008
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.571189284324646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00252 | Loss 1.5712 | Train 0.4786 | Val 0.3080 | Test 0.2713
loading full batch data spends  0.001996278762817383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450273513793945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044507503509521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10882449150085449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5387959480285645
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00253 | Loss 1.5388 | Train 0.5000 | Val 0.3040 | Test 0.2693
loading full batch data spends  0.0019621849060058594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043283939361572266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0432887077331543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10882258415222168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6394062042236328
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00254 | Loss 1.6394 | Train 0.5214 | Val 0.3160 | Test 0.2877
loading full batch data spends  0.002005338668823242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0446314811706543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04463624954223633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1106264591217041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5956226587295532
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00255 | Loss 1.5956 | Train 0.5571 | Val 0.2780 | Test 0.2722
loading full batch data spends  0.0019519329071044922
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451274871826172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451751708984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10593867301940918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6051292419433594
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00256 | Loss 1.6051 | Train 0.5429 | Val 0.2600 | Test 0.2510
loading full batch data spends  0.002001047134399414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04457283020019531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044577598571777344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10291314125061035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6021537780761719
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00257 | Loss 1.6022 | Train 0.5571 | Val 0.2780 | Test 0.2635
loading full batch data spends  0.001951456069946289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04343986511230469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344463348388672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1078801155090332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6247804164886475
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00258 | Loss 1.6248 | Train 0.5429 | Val 0.3080 | Test 0.2834
loading full batch data spends  0.001993417739868164
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434107780456543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341554641723633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11362957954406738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6379165649414062
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00259 | Loss 1.6379 | Train 0.5214 | Val 0.3280 | Test 0.2800
loading full batch data spends  0.0019297599792480469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452943801879883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453420639038086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10445952415466309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5955145359039307
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00260 | Loss 1.5955 | Train 0.4929 | Val 0.3340 | Test 0.2863
loading full batch data spends  0.002181529998779297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356193542480469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356670379638672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11159396171569824
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5808274745941162
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00261 | Loss 1.5808 | Train 0.4786 | Val 0.3300 | Test 0.2771
loading full batch data spends  0.001968860626220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442310333251953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04442787170410156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11247467994689941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5535131692886353
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00262 | Loss 1.5535 | Train 0.4643 | Val 0.3200 | Test 0.2838
loading full batch data spends  0.0020322799682617188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0432896614074707  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043294429779052734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10478043556213379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5850790739059448
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00263 | Loss 1.5851 | Train 0.4857 | Val 0.3000 | Test 0.2693
loading full batch data spends  0.001973867416381836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445399284362793  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454469680786133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10511016845703125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5627310276031494
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00264 | Loss 1.5627 | Train 0.5000 | Val 0.3180 | Test 0.2877
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0435338020324707  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043538570404052734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10494351387023926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01211690902709961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5937501192092896
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00265 | Loss 1.5938 | Train 0.4929 | Val 0.3360 | Test 0.2805
loading full batch data spends  0.001964092254638672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448843002319336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449319839477539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10813069343566895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6165956258773804
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00266 | Loss 1.6166 | Train 0.5286 | Val 0.3340 | Test 0.2834
loading full batch data spends  0.002060413360595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043480873107910156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348564147949219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10513925552368164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012042045593261719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.602746605873108
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00267 | Loss 1.6027 | Train 0.4500 | Val 0.2880 | Test 0.2544
loading full batch data spends  0.0021317005157470703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04460716247558594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04461193084716797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1071481704711914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5967141389846802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00268 | Loss 1.5967 | Train 0.4714 | Val 0.2920 | Test 0.2645
loading full batch data spends  0.002004861831665039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043591976165771484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043596744537353516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11057543754577637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012125015258789062  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.577489972114563
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00269 | Loss 1.5775 | Train 0.5429 | Val 0.3180 | Test 0.2911
loading full batch data spends  0.001962423324584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044481754302978516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448652267456055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10463786125183105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5528004169464111
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00270 | Loss 1.5528 | Train 0.5286 | Val 0.2960 | Test 0.2756
loading full batch data spends  0.001989603042602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342460632324219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342937469482422  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1055138111114502
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012068748474121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.607940673828125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00271 | Loss 1.6079 | Train 0.5143 | Val 0.2920 | Test 0.2800
loading full batch data spends  0.0021347999572753906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348039627075195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043485164642333984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11528277397155762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5454267263412476
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00272 | Loss 1.5454 | Train 0.5571 | Val 0.3280 | Test 0.3124
loading full batch data spends  0.0020079612731933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044504642486572266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445094108581543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10297036170959473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.570128321647644
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00273 | Loss 1.5701 | Train 0.5500 | Val 0.3220 | Test 0.2964
loading full batch data spends  0.001955270767211914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445713996887207  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044576168060302734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10527348518371582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.579582691192627
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00274 | Loss 1.5796 | Train 0.4929 | Val 0.2980 | Test 0.2761
loading full batch data spends  0.0020318031311035156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339122772216797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339599609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1041569709777832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.58851957321167
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00275 | Loss 1.5885 | Train 0.5643 | Val 0.3340 | Test 0.3158
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04427337646484375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04427814483642578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1030583381652832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5678412914276123
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00276 | Loss 1.5678 | Train 0.5571 | Val 0.3180 | Test 0.2892
loading full batch data spends  0.0020029544830322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04433727264404297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044342041015625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10530662536621094
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5357191562652588
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00277 | Loss 1.5357 | Train 0.5143 | Val 0.2720 | Test 0.2640
loading full batch data spends  0.0019507408142089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043412208557128906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341697692871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10668230056762695
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5937118530273438
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00278 | Loss 1.5937 | Train 0.5071 | Val 0.2840 | Test 0.2655
loading full batch data spends  0.002002716064453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434722900390625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347705841064453  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10586190223693848
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5446373224258423
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00279 | Loss 1.5446 | Train 0.5357 | Val 0.2960 | Test 0.2747
loading full batch data spends  0.001985311508178711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434107780456543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341554641723633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10860943794250488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5858160257339478
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00280 | Loss 1.5858 | Train 0.5857 | Val 0.3160 | Test 0.2950
loading full batch data spends  0.0020198822021484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0434565544128418  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346132278442383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1093144416809082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5232640504837036
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00281 | Loss 1.5233 | Train 0.5786 | Val 0.3300 | Test 0.3133
loading full batch data spends  0.0019731521606445312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044560909271240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044565677642822266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11064291000366211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5572065114974976
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00282 | Loss 1.5572 | Train 0.5643 | Val 0.3320 | Test 0.3124
loading full batch data spends  0.0019903182983398438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353666305541992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354143142700195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10653066635131836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5253218412399292
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00283 | Loss 1.5253 | Train 0.5714 | Val 0.3280 | Test 0.3008
loading full batch data spends  0.001969575881958008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450225830078125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450702667236328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10211372375488281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5572686195373535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00284 | Loss 1.5573 | Train 0.5500 | Val 0.3120 | Test 0.2911
loading full batch data spends  0.0021741390228271484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044528961181640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044533729553222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10306119918823242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5349916219711304
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00285 | Loss 1.5350 | Train 0.5429 | Val 0.3180 | Test 0.2945
loading full batch data spends  0.0021240711212158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347944259643555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348421096801758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11430621147155762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.565861701965332
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00286 | Loss 1.5659 | Train 0.5429 | Val 0.3200 | Test 0.2853
loading full batch data spends  0.0019845962524414062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043508052825927734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043512821197509766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10795402526855469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5019993782043457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00287 | Loss 1.5020 | Train 0.5286 | Val 0.3060 | Test 0.2974
loading full batch data spends  0.001977682113647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044513702392578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044518470764160156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10936617851257324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5104295015335083
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00288 | Loss 1.5104 | Train 0.5286 | Val 0.2860 | Test 0.2713
loading full batch data spends  0.0020051002502441406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044472694396972656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447746276855469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10395073890686035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5563658475875854
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00289 | Loss 1.5564 | Train 0.5071 | Val 0.2700 | Test 0.2766
loading full batch data spends  0.0019497871398925781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347848892211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348325729370117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11149954795837402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5222474336624146
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00290 | Loss 1.5222 | Train 0.5071 | Val 0.2800 | Test 0.2824
loading full batch data spends  0.002043485641479492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043415069580078125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043419837951660156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10386443138122559
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5564600229263306
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00291 | Loss 1.5565 | Train 0.5571 | Val 0.3000 | Test 0.3042
loading full batch data spends  0.0019655227661132812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333925247192383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334402084350586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1056678295135498
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5341132879257202
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00292 | Loss 1.5341 | Train 0.5500 | Val 0.2960 | Test 0.2814
loading full batch data spends  0.0020067691802978516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344654083251953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345130920410156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10276293754577637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5852073431015015
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00293 | Loss 1.5852 | Train 0.3857 | Val 0.2520 | Test 0.2292
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344034194946289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344511032104492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10820126533508301
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5845437049865723
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00294 | Loss 1.5845 | Train 0.4500 | Val 0.2700 | Test 0.2466
loading full batch data spends  0.002000093460083008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353904724121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354381561279297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10387325286865234
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012066364288330078  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5820895433425903
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00295 | Loss 1.5821 | Train 0.4071 | Val 0.2420 | Test 0.2340
loading full batch data spends  0.0019686222076416016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451179504394531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044516563415527344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10024118423461914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6019034385681152
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00296 | Loss 1.6019 | Train 0.3857 | Val 0.2380 | Test 0.2249
loading full batch data spends  0.002009868621826172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043519020080566406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04352378845214844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.109405517578125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.583261251449585
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00297 | Loss 1.5833 | Train 0.4429 | Val 0.2500 | Test 0.2423
loading full batch data spends  0.002138376235961914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.506111145019531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043506622314453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043511390686035156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10376334190368652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5901210308074951
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00298 | Loss 1.5901 | Train 0.4786 | Val 0.2620 | Test 0.2481
loading full batch data spends  0.0019941329956054688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445094108581543  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451417922973633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10462665557861328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.587079644203186
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00299 | Loss 1.5871 | Train 0.4357 | Val 0.2440 | Test 0.2282
loading full batch data spends  0.0019953250885009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349660873413086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350137710571289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10474467277526855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5662052631378174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00300 | Loss 1.5662 | Train 0.4643 | Val 0.2460 | Test 0.2326
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333305358886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333782196044922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10598587989807129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5352414846420288
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00301 | Loss 1.5352 | Train 0.4643 | Val 0.2620 | Test 0.2466
loading full batch data spends  0.0022470951080322266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1961669921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454612731933594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04455089569091797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10514378547668457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6177362203598022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00302 | Loss 1.6177 | Train 0.4714 | Val 0.2700 | Test 0.2655
loading full batch data spends  0.002000093460083008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044577598571777344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044582366943359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10538864135742188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5815777778625488
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00303 | Loss 1.5816 | Train 0.4786 | Val 0.2780 | Test 0.2534
loading full batch data spends  0.0019664764404296875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345893859863281  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043463706970214844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10170841217041016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6251723766326904
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00304 | Loss 1.6252 | Train 0.4857 | Val 0.2740 | Test 0.2524
loading full batch data spends  0.0019843578338623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445505142211914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445981979370117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10381746292114258
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5776342153549194
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00305 | Loss 1.5776 | Train 0.5071 | Val 0.2620 | Test 0.2452
loading full batch data spends  0.0019295215606689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043453216552734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043457984924316406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1052546501159668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.602447509765625
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00306 | Loss 1.6024 | Train 0.4643 | Val 0.2220 | Test 0.2278
loading full batch data spends  0.0020220279693603516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04440927505493164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04441404342651367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10375285148620605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.53075110912323
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00307 | Loss 1.5308 | Train 0.4571 | Val 0.2180 | Test 0.2268
loading full batch data spends  0.0019638538360595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044525146484375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452991485595703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10282635688781738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6430326700210571
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00308 | Loss 1.6430 | Train 0.4500 | Val 0.2220 | Test 0.2263
loading full batch data spends  0.0021719932556152344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452180862426758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452657699584961  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.102325439453125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5931544303894043
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00309 | Loss 1.5932 | Train 0.5143 | Val 0.2540 | Test 0.2553
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044539451599121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044544219970703125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11080360412597656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5481514930725098
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00310 | Loss 1.5482 | Train 0.5286 | Val 0.2720 | Test 0.2693
loading full batch data spends  0.0021665096282958984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043417930603027344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043422698974609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10103845596313477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012073993682861328  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5601228475570679
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00311 | Loss 1.5601 | Train 0.5429 | Val 0.2660 | Test 0.2689
loading full batch data spends  0.0019757747650146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043501853942871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043506622314453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10712814331054688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.585471749305725
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00312 | Loss 1.5855 | Train 0.5000 | Val 0.2700 | Test 0.2689
loading full batch data spends  0.0020208358764648438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04460430145263672  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04460906982421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10975146293640137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013106346130371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5927603244781494
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00313 | Loss 1.5928 | Train 0.5357 | Val 0.2620 | Test 0.2640
loading full batch data spends  0.0019736289978027344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350996017456055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351472854614258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10364270210266113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5701860189437866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00314 | Loss 1.5702 | Train 0.5357 | Val 0.2620 | Test 0.2563
loading full batch data spends  0.0020084381103515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043257713317871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043262481689453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10658597946166992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5169540643692017
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00315 | Loss 1.5170 | Train 0.4929 | Val 0.2320 | Test 0.2408
loading full batch data spends  0.0019483566284179688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044397830963134766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444025993347168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10730791091918945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5588502883911133
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00316 | Loss 1.5589 | Train 0.5286 | Val 0.2440 | Test 0.2461
loading full batch data spends  0.002168416976928711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044442176818847656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444694519042969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11045694351196289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5998116731643677
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00317 | Loss 1.5998 | Train 0.4857 | Val 0.2500 | Test 0.2427
loading full batch data spends  0.0019578933715820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445408821105957  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044545650482177734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10153341293334961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5515140295028687
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00318 | Loss 1.5515 | Train 0.4000 | Val 0.2320 | Test 0.2176
loading full batch data spends  0.0019829273223876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350996017456055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351472854614258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10728907585144043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012079715728759766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5700817108154297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00319 | Loss 1.5701 | Train 0.4286 | Val 0.2460 | Test 0.2307
loading full batch data spends  0.0019779205322265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043459415435791016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346418380737305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11126017570495605
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.516342043876648
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00320 | Loss 1.5163 | Train 0.4786 | Val 0.2460 | Test 0.2263
loading full batch data spends  0.0020508766174316406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043509483337402344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043514251708984375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10845422744750977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5405243635177612
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00321 | Loss 1.5405 | Train 0.4357 | Val 0.2100 | Test 0.2094
loading full batch data spends  0.0019953250885009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344034194946289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04344511032104492  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1056981086730957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.530819058418274
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00322 | Loss 1.5308 | Train 0.4571 | Val 0.2260 | Test 0.2263
loading full batch data spends  0.002179861068725586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349374771118164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349851608276367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10361433029174805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.525002121925354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00323 | Loss 1.5250 | Train 0.4857 | Val 0.2540 | Test 0.2423
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04459571838378906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044600486755371094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11451435089111328
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.554080605506897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00324 | Loss 1.5541 | Train 0.5571 | Val 0.2840 | Test 0.2722
loading full batch data spends  0.001997709274291992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453706741333008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454183578491211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10371685028076172
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5818796157836914
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00325 | Loss 1.5819 | Train 0.5286 | Val 0.2740 | Test 0.2693
loading full batch data spends  0.0019450187683105469
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0445246696472168  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452943801879883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10445594787597656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5856448411941528
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00326 | Loss 1.5856 | Train 0.5143 | Val 0.2620 | Test 0.2558
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333925247192383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334402084350586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10142850875854492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012020587921142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.524354338645935
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00327 | Loss 1.5244 | Train 0.5643 | Val 0.2860 | Test 0.2732
loading full batch data spends  0.0019452571868896484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451608657836914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452085494995117  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10439062118530273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5570032596588135
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00328 | Loss 1.5570 | Train 0.5571 | Val 0.2740 | Test 0.2703
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433955192565918  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340028762817383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10195279121398926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01203155517578125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5137224197387695
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00329 | Loss 1.5137 | Train 0.4857 | Val 0.2720 | Test 0.2515
loading full batch data spends  0.0019681453704833984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043331146240234375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043335914611816406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10326600074768066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5817372798919678
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00330 | Loss 1.5817 | Train 0.4643 | Val 0.2480 | Test 0.2408
loading full batch data spends  0.0020194053649902344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043469905853271484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043474674224853516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10825347900390625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5109044313430786
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00331 | Loss 1.5109 | Train 0.5214 | Val 0.2640 | Test 0.2529
loading full batch data spends  0.0019659996032714844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341840744018555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04342317581176758  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1036067008972168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5762962102890015
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00332 | Loss 1.5763 | Train 0.4286 | Val 0.2380 | Test 0.2191
loading full batch data spends  0.001995086669921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04363203048706055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04363679885864258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10589957237243652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210927963256836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.576460838317871
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00333 | Loss 1.5765 | Train 0.4143 | Val 0.2240 | Test 0.1983
loading full batch data spends  0.001966714859008789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0444798469543457  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044484615325927734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.105255126953125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5588352680206299
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00334 | Loss 1.5588 | Train 0.4071 | Val 0.2180 | Test 0.1910
loading full batch data spends  0.0019915103912353516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04328727722167969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04329204559326172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1059269905090332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012020587921142578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.660681128501892
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00335 | Loss 1.6607 | Train 0.4286 | Val 0.2340 | Test 0.2162
loading full batch data spends  0.001961231231689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043477535247802734  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043482303619384766  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10843896865844727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5250494480133057
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00336 | Loss 1.5250 | Train 0.5071 | Val 0.2820 | Test 0.2563
loading full batch data spends  0.0020241737365722656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453420639038086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453897476196289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10519671440124512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5696136951446533
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00337 | Loss 1.5696 | Train 0.4143 | Val 0.2000 | Test 0.1920
loading full batch data spends  0.0019683837890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444169998168945  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044446468353271484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11054158210754395
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5871320962905884
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00338 | Loss 1.5871 | Train 0.4143 | Val 0.2000 | Test 0.1910
loading full batch data spends  0.0020170211791992188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04339027404785156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043395042419433594  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10666203498840332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6257637739181519
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00339 | Loss 1.6258 | Train 0.4643 | Val 0.2460 | Test 0.2326
loading full batch data spends  0.0019745826721191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351472854614258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351949691772461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11153626441955566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4979020357131958
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00340 | Loss 1.4979 | Train 0.4500 | Val 0.2440 | Test 0.2311
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345369338989258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345846176147461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11146092414855957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5540800094604492
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00341 | Loss 1.5541 | Train 0.3857 | Val 0.2140 | Test 0.2060
loading full batch data spends  0.001978635787963867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043387413024902344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043392181396484375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10354304313659668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5625752210617065
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00342 | Loss 1.5626 | Train 0.3786 | Val 0.1880 | Test 0.1934
loading full batch data spends  0.001988649368286133
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04363203048706055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04363679885864258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10280585289001465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012087821960449219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6051217317581177
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00343 | Loss 1.6051 | Train 0.3857 | Val 0.1920 | Test 0.1905
loading full batch data spends  0.0019583702087402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0432429313659668  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04324769973754883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11765074729919434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5864675045013428
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00344 | Loss 1.5865 | Train 0.4143 | Val 0.2100 | Test 0.2128
loading full batch data spends  0.0020895004272460938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043494224548339844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043498992919921875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1055305004119873
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.578292727470398
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00345 | Loss 1.5783 | Train 0.4857 | Val 0.2380 | Test 0.2379
loading full batch data spends  0.0019800662994384766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346132278442383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346609115600586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1075129508972168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.60759699344635
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00346 | Loss 1.6076 | Train 0.4357 | Val 0.1920 | Test 0.1925
loading full batch data spends  0.0020017623901367188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0443730354309082  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044377803802490234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10479354858398438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5873914957046509
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00347 | Loss 1.5874 | Train 0.4643 | Val 0.2220 | Test 0.2229
loading full batch data spends  0.0021393299102783203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.719329833984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341411590576172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341888427734375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10895538330078125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5605970621109009
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00348 | Loss 1.5606 | Train 0.4786 | Val 0.2680 | Test 0.2660
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447126388549805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447603225708008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11666393280029297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.574730396270752
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00349 | Loss 1.5747 | Train 0.5000 | Val 0.2620 | Test 0.2732
loading full batch data spends  0.0019643306732177734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043422698974609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043427467346191406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11071157455444336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5321513414382935
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00350 | Loss 1.5322 | Train 0.4929 | Val 0.2480 | Test 0.2442
loading full batch data spends  0.001993894577026367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044422149658203125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044426918029785156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10704278945922852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.537320613861084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00351 | Loss 1.5373 | Train 0.4500 | Val 0.2360 | Test 0.2278
loading full batch data spends  0.0019497871398925781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04443645477294922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04444122314453125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10213613510131836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.559289574623108
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00352 | Loss 1.5593 | Train 0.4643 | Val 0.2220 | Test 0.2166
loading full batch data spends  0.0020346641540527344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353523254394531  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043540000915527344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10967206954956055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012122154235839844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6340092420578003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00353 | Loss 1.6340 | Train 0.4643 | Val 0.2340 | Test 0.2379
loading full batch data spends  0.001973390579223633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043404579162597656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340934753417969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10294938087463379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5387918949127197
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00354 | Loss 1.5388 | Train 0.4857 | Val 0.2400 | Test 0.2466
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043218135833740234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043222904205322266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10388398170471191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01205301284790039  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4896408319473267
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00355 | Loss 1.4896 | Train 0.4500 | Val 0.2540 | Test 0.2369
loading full batch data spends  0.0023174285888671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.57763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043412208557128906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04341697692871094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10597920417785645
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5632745027542114
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00356 | Loss 1.5633 | Train 0.4000 | Val 0.2220 | Test 0.2268
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340028762817383  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340505599975586  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10946822166442871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0120849609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6278759241104126
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00357 | Loss 1.6279 | Train 0.3929 | Val 0.2160 | Test 0.2147
loading full batch data spends  0.001955747604370117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445314407348633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04445791244506836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10887670516967773
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5894365310668945
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00358 | Loss 1.5894 | Train 0.4286 | Val 0.2520 | Test 0.2481
loading full batch data spends  0.001939535140991211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334688186645508  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335165023803711  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1083979606628418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012058258056640625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.599095106124878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00359 | Loss 1.5991 | Train 0.4929 | Val 0.2580 | Test 0.2515
loading full batch data spends  0.0020437240600585938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04333019256591797  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433349609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1115419864654541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.527590036392212
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00360 | Loss 1.5276 | Train 0.5071 | Val 0.2560 | Test 0.2684
loading full batch data spends  0.0020122528076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356527328491211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04357004165649414  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10666990280151367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012103557586669922  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5695838928222656
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00361 | Loss 1.5696 | Train 0.5143 | Val 0.2700 | Test 0.2684
loading full batch data spends  0.001967906951904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04325246810913086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04325723648071289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10460329055786133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5233423709869385
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00362 | Loss 1.5233 | Train 0.5143 | Val 0.2700 | Test 0.2790
loading full batch data spends  0.0021970272064208984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350423812866211  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350900650024414  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11142826080322266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012111663818359375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5127825736999512
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00363 | Loss 1.5128 | Train 0.5143 | Val 0.2640 | Test 0.2809
loading full batch data spends  0.0021562576293945312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335784912109375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336261749267578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10395979881286621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5977966785430908
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00364 | Loss 1.5978 | Train 0.4714 | Val 0.2760 | Test 0.2780
loading full batch data spends  0.0021889209747314453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043628692626953125  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043633460998535156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10700011253356934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012119770050048828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4913798570632935
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00365 | Loss 1.4914 | Train 0.4643 | Val 0.2680 | Test 0.2708
loading full batch data spends  0.002142667770385742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345369338989258  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345846176147461  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10828351974487305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.496762990951538
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00366 | Loss 1.4968 | Train 0.4571 | Val 0.2740 | Test 0.2621
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04450798034667969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04451274871826172  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10470199584960938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.532835602760315
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00367 | Loss 1.5328 | Train 0.4571 | Val 0.2560 | Test 0.2611
loading full batch data spends  0.002170085906982422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.719329833984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0433197021484375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04332447052001953  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10356712341308594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.493751883506775
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00368 | Loss 1.4938 | Train 0.4571 | Val 0.2440 | Test 0.2505
loading full batch data spends  0.0021970272064208984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04454231262207031  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044547080993652344  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10406351089477539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4808746576309204
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00369 | Loss 1.4809 | Train 0.4500 | Val 0.2460 | Test 0.2355
loading full batch data spends  0.0021436214447021484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346418380737305  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04346895217895508  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10146641731262207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.584829568862915
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00370 | Loss 1.5848 | Train 0.4357 | Val 0.2440 | Test 0.2311
loading full batch data spends  0.001990795135498047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446601867675781  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044470787048339844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1042025089263916
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.50070059299469
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00371 | Loss 1.5007 | Train 0.4000 | Val 0.2240 | Test 0.2297
loading full batch data spends  0.002148151397705078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337263107299805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337739944458008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10694003105163574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5191688537597656
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00372 | Loss 1.5192 | Train 0.4286 | Val 0.2420 | Test 0.2379
loading full batch data spends  0.0021915435791015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043474674224853516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347944259643555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10279631614685059
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5106778144836426
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00373 | Loss 1.5107 | Train 0.4643 | Val 0.2560 | Test 0.2505
loading full batch data spends  0.002128124237060547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044533729553222656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04453849792480469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10837817192077637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5080883502960205
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00374 | Loss 1.5081 | Train 0.4571 | Val 0.2340 | Test 0.2350
loading full batch data spends  0.0021800994873046875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04355621337890625  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04356098175048828  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11004948616027832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012076854705810547  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5532755851745605
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00375 | Loss 1.5533 | Train 0.4500 | Val 0.2260 | Test 0.2316
loading full batch data spends  0.0021467208862304688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04446840286254883  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447317123413086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10151290893554688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5348873138427734
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00376 | Loss 1.5349 | Train 0.4429 | Val 0.2300 | Test 0.2379
loading full batch data spends  0.0022079944610595703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04350709915161133  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04351186752319336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10564923286437988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012090206146240234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5622433423995972
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00377 | Loss 1.5622 | Train 0.4929 | Val 0.2360 | Test 0.2413
loading full batch data spends  0.0022025108337402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337263107299805  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04337739944458008  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1043710708618164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5207815170288086
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00378 | Loss 1.5208 | Train 0.5071 | Val 0.2520 | Test 0.2563
loading full batch data spends  0.002233743667602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349374771118164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04349851608276367  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10243821144104004
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210641860961914  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5638986825942993
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00379 | Loss 1.5639 | Train 0.5000 | Val 0.2440 | Test 0.2524
loading full batch data spends  0.0023076534271240234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.696846008300781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044481754302978516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448652267456055  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10523223876953125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5266696214675903
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00380 | Loss 1.5267 | Train 0.5000 | Val 0.2420 | Test 0.2476
loading full batch data spends  0.0021944046020507812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335451126098633  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335927963256836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10231876373291016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012095451354980469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5484604835510254
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00381 | Loss 1.5485 | Train 0.5143 | Val 0.2420 | Test 0.2577
loading full batch data spends  0.002136707305908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044472694396972656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04447746276855469  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1007852554321289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4640696048736572
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00382 | Loss 1.4641 | Train 0.5143 | Val 0.2440 | Test 0.2626
loading full batch data spends  0.0021588802337646484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043641090393066406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04364585876464844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10486078262329102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01210927963256836  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.519796371459961
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00383 | Loss 1.5198 | Train 0.5214 | Val 0.2520 | Test 0.2500
loading full batch data spends  0.0019712448120117188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340076446533203  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04340553283691406  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10812783241271973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5071250200271606
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00384 | Loss 1.5071 | Train 0.5071 | Val 0.2680 | Test 0.2597
loading full batch data spends  0.00225830078125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04347562789916992  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04348039627075195  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1056373119354248
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01206350326538086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5016770362854004
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00385 | Loss 1.5017 | Train 0.5071 | Val 0.2540 | Test 0.2471
loading full batch data spends  0.0021317005157470703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044518470764160156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452323913574219  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11054682731628418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5811516046524048
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00386 | Loss 1.5812 | Train 0.4929 | Val 0.2620 | Test 0.2485
loading full batch data spends  0.0021910667419433594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04452705383300781  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044531822204589844  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10590410232543945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5909477472305298
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00387 | Loss 1.5909 | Train 0.4857 | Val 0.2800 | Test 0.2568
loading full batch data spends  0.0019693374633789062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04330778121948242  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04331254959106445  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10774540901184082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4775893688201904
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00388 | Loss 1.4776 | Train 0.5000 | Val 0.2740 | Test 0.2553
loading full batch data spends  0.002186298370361328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04448843002319336  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04449319839477539  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1087799072265625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5507594347000122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00389 | Loss 1.5508 | Train 0.5071 | Val 0.2640 | Test 0.2515
loading full batch data spends  0.002139568328857422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043450355529785156  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04345512390136719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.11037564277648926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5407391786575317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00390 | Loss 1.5407 | Train 0.5214 | Val 0.2640 | Test 0.2510
loading full batch data spends  0.002172708511352539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044507503509521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.044512271881103516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10593056678771973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5112444162368774
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00391 | Loss 1.5112 | Train 0.5071 | Val 0.2720 | Test 0.2485
loading full batch data spends  0.0019714832305908203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043343544006347656  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04334831237792969  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10843992233276367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.499045729637146
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00392 | Loss 1.4990 | Train 0.5071 | Val 0.2760 | Test 0.2476
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04435110092163086  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04435586929321289  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10571026802062988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.013103485107421875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5411202907562256
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00393 | Loss 1.5411 | Train 0.5071 | Val 0.2820 | Test 0.2544
loading full batch data spends  0.001956462860107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043332576751708984  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043337345123291016  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10961794853210449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5448360443115234
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00394 | Loss 1.5448 | Train 0.5071 | Val 0.2820 | Test 0.2597
loading full batch data spends  0.0021893978118896484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043468475341796875  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043473243713378906  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10281920433044434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012100696563720703  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5056819915771484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00395 | Loss 1.5057 | Train 0.5071 | Val 0.2800 | Test 0.2568
loading full batch data spends  0.0021305084228515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335784912109375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04336261749267578  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10977363586425781
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.0121917724609375  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4735114574432373
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00396 | Loss 1.4735 | Train 0.5000 | Val 0.2740 | Test 0.2544
loading full batch data spends  0.002182483673095703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043309688568115234  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043314456939697266  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.1028294563293457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.01199960708618164  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.511183261871338
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00397 | Loss 1.5112 | Train 0.4857 | Val 0.2580 | Test 0.2490
loading full batch data spends  0.0021195411682128906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04353904724121094  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04354381561279297  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10184431076049805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012194633483886719  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5623258352279663
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00398 | Loss 1.5623 | Train 0.4500 | Val 0.2360 | Test 0.2336
loading full batch data spends  0.0021958351135253906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.043352603912353516  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.04335737228393555  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

pure train time  0.10459542274475098
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.012036800384521484  GigaBytes
Max Memory Allocated: 0.057317256927490234  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5275777578353882
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 8, 16])
h.size() torch.Size([2708, 128])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00399 | Loss 1.5276 | Train 0.4857 | Val 0.2440 | Test 0.2471
Total (block generation + training)time/epoch 0.11400232791900634
pure train time/epoch 0.1054267178882252

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368, 1367, 1352, 1359, 1359, 1355, 1349, 1360, 1356, 1341, 1367, 1363, 1356, 1368, 1371, 1369, 1363, 1362, 1367, 1357, 1365, 1337, 1358, 1356, 1358, 1371, 1363, 1359, 1364, 1361, 1365, 1353, 1369, 1352, 1374, 1367, 1359, 1369, 1361, 1369, 1355, 1345, 1362, 1363, 1358, 1358, 1365, 1365, 1352, 1356, 1360, 1356, 1366, 1361, 1371, 1354, 1367, 1372, 1380, 1354, 1357, 1367, 1363, 1368, 1356, 1375, 1364, 1367, 1350, 1368, 1365, 1377, 1355, 1363, 1371, 1368, 1354, 1367, 1374, 1355, 1357, 1357, 1358, 1368, 1362, 1370, 1370, 1362, 1362, 1374, 1368, 1363, 1353, 1340, 1356, 1363, 1354, 1366, 1363, 1361, 1366, 1360, 1356, 1368, 1366, 1358, 1371, 1355, 1368, 1369, 1368, 1368, 1356, 1361, 1367, 1357, 1354, 1366, 1372, 1371, 1357, 1352, 1354, 1358, 1353, 1367, 1372, 1368, 1346, 1370, 1348, 1354, 1362, 1362, 1362, 1368, 1346, 1363, 1366, 1367, 1358, 1364, 1358, 1357, 1358, 1348, 1362, 1361, 1368, 1362, 1375, 1349, 1366, 1370, 1365, 1356, 1352, 1365, 1358, 1378, 1353, 1348, 1361, 1355, 1363, 1363, 1364, 1360, 1371, 1345, 1379, 1359, 1369, 1360, 1362, 1377, 1356, 1374, 1359, 1359, 1362, 1378, 1360, 1368, 1362, 1363, 1354, 1375, 1373, 1352, 1372, 1359, 1367, 1355, 1367, 1356, 1361, 1350, 1342, 1356, 1349]
