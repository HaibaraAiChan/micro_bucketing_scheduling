main start at this time 1696111261.9286418
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.00302886962890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.06546783447265625  GigaBytes
Max Memory Allocated: 0.06849527359008789  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.609375 GB
    Memory Allocated: 0.06547260284423828  GigaBytes
Max Memory Allocated: 0.06849527359008789  GigaBytes

pure train time  0.5102400779724121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.619140625 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.06849527359008789  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9561578035354614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9562 | Train 0.1357 | Val 0.0700 | Test 0.0841
loading full batch data spends  0.0020258426666259766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.650390625 GB
    Memory Allocated: 0.07619619369506836  GigaBytes
Max Memory Allocated: 0.07930278778076172  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.650390625 GB
    Memory Allocated: 0.07620096206665039  GigaBytes
Max Memory Allocated: 0.07930278778076172  GigaBytes

pure train time  0.12083172798156738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.65234375 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.07930278778076172  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9510692358016968
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9511 | Train 0.1571 | Val 0.0920 | Test 0.0880
loading full batch data spends  0.001993894577026367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762028694152832  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07620763778686523  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

pure train time  0.1223607063293457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020743846893310547  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9496796131134033
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9497 | Train 0.1714 | Val 0.0940 | Test 0.0909
loading full batch data spends  0.0019481182098388672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630395889282227  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763087272644043  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

pure train time  0.12507247924804688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.07994365692138672  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9482663869857788
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9483 | Train 0.1714 | Val 0.1040 | Test 0.0899
loading full batch data spends  0.0019774436950683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07652807235717773  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653284072875977  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

pure train time  0.11110663414001465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020733356475830078  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9372929334640503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9373 | Train 0.1857 | Val 0.1160 | Test 0.0928
loading full batch data spends  0.0019981861114501953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638263702392578  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638740539550781  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

pure train time  0.1147007942199707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9367610216140747
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9368 | Train 0.2000 | Val 0.1120 | Test 0.0967
loading full batch data spends  0.00196075439453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654714584350586  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655191421508789  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

pure train time  0.11220955848693848
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020818710327148438  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.936124563217163
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9361 | Train 0.1786 | Val 0.1120 | Test 0.1015
loading full batch data spends  0.0020127296447753906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647180557250977  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764765739440918  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

pure train time  0.11537361145019531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0800013542175293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9405567646026611
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9406 | Train 0.1857 | Val 0.1140 | Test 0.1040
loading full batch data spends  0.0019507408142089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07668590545654297  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.076690673828125  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.1130824089050293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.936484694480896
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9365 | Train 0.1643 | Val 0.1640 | Test 0.1243
loading full batch data spends  0.002008199691772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647371292114258  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647848129272461  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.11255192756652832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02074909210205078  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9405969381332397
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9406 | Train 0.1929 | Val 0.1580 | Test 0.1248
loading full batch data spends  0.001965045928955078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0761728286743164  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07617759704589844  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.10836029052734375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.927344799041748
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9273 | Train 0.2071 | Val 0.1560 | Test 0.1277
loading full batch data spends  0.0020055770874023438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631492614746094  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631969451904297  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.11099457740783691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9266244173049927
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9266 | Train 0.2143 | Val 0.1560 | Test 0.1325
loading full batch data spends  0.0019495487213134766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633829116821289  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07634305953979492  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.11187624931335449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020765304565429688  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.932273268699646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9323 | Train 0.2214 | Val 0.1540 | Test 0.1349
loading full batch data spends  0.001949310302734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07613611221313477  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0761408805847168  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

pure train time  0.11406278610229492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.08011770248413086  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9330025911331177
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9330 | Train 0.2143 | Val 0.1580 | Test 0.1281
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666587829589844  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07667064666748047  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.1130678653717041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020751953125  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.933646321296692
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9336 | Train 0.2071 | Val 0.1400 | Test 0.1170
loading full batch data spends  0.002018451690673828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640314102172852  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640790939331055  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.12358403205871582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9302294254302979
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9302 | Train 0.2143 | Val 0.1460 | Test 0.1296
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07530641555786133  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07531118392944336  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.11035275459289551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9249330759048462
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9249 | Train 0.1786 | Val 0.1480 | Test 0.1349
loading full batch data spends  0.0019805431365966797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631826400756836  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07632303237915039  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.1186513900756836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9232988357543945
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9233 | Train 0.1929 | Val 0.1460 | Test 0.1330
loading full batch data spends  0.0019745826721191406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07535696029663086  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07536172866821289  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.12017393112182617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9405404329299927
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9405 | Train 0.2000 | Val 0.1540 | Test 0.1441
loading full batch data spends  0.002008199691772461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07662630081176758  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07663106918334961  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.12366271018981934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9320683479309082
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9321 | Train 0.2357 | Val 0.1400 | Test 0.1315
loading full batch data spends  0.0019652843475341797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640504837036133  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640981674194336  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.12018871307373047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020722389221191406  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9296461343765259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9296 | Train 0.2500 | Val 0.1260 | Test 0.1219
loading full batch data spends  0.0020151138305664062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07600259780883789  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07600736618041992  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

pure train time  0.12505221366882324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0801234245300293  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9347647428512573
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9348 | Train 0.2214 | Val 0.1180 | Test 0.1141
loading full batch data spends  0.0019583702087402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07674694061279297  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.076751708984375  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.12448310852050781
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9151439666748047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9151 | Train 0.2143 | Val 0.1240 | Test 0.1112
loading full batch data spends  0.0020003318786621094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631874084472656  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763235092163086  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11362600326538086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.906288743019104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.9063 | Train 0.2214 | Val 0.1160 | Test 0.1083
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07507991790771484  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07508468627929688  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11160087585449219
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020647525787353516  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9547553062438965
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9548 | Train 0.2143 | Val 0.1160 | Test 0.1064
loading full batch data spends  0.0020294189453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653999328613281  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654476165771484  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.12463760375976562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.914454698562622
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9145 | Train 0.2214 | Val 0.1140 | Test 0.1078
loading full batch data spends  0.002025127410888672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.8623809814453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07662391662597656  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766286849975586  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.12372422218322754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9307819604873657
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9308 | Train 0.2143 | Val 0.1160 | Test 0.1088
loading full batch data spends  0.0020329952239990234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07626914978027344  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627391815185547  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.12275195121765137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9117968082427979
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.9118 | Train 0.2286 | Val 0.1040 | Test 0.1054
loading full batch data spends  0.001968860626220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.00543212890625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648277282714844  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648754119873047  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.1274428367614746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.919533610343933
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9195 | Train 0.2429 | Val 0.1060 | Test 0.1049
loading full batch data spends  0.0020933151245117188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.7670135498046875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07646036148071289  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07646512985229492  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10966348648071289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020733356475830078  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9222885370254517
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9223 | Train 0.2429 | Val 0.0980 | Test 0.1069
loading full batch data spends  0.001980304718017578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641792297363281  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642269134521484  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11243271827697754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.918448567390442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9184 | Train 0.2357 | Val 0.1060 | Test 0.1049
loading full batch data spends  0.001982450485229492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647466659545898  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647943496704102  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11676359176635742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9032258987426758
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.9032 | Train 0.2429 | Val 0.1140 | Test 0.1074
loading full batch data spends  0.0021300315856933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762796401977539  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628440856933594  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11587691307067871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.916618824005127
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.9166 | Train 0.2643 | Val 0.1200 | Test 0.1088
loading full batch data spends  0.002089977264404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640266418457031  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640743255615234  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11350846290588379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8967788219451904
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.8968 | Train 0.2429 | Val 0.1100 | Test 0.1107
loading full batch data spends  0.001958131790161133
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07660198211669922  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07660675048828125  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10932445526123047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02073049545288086  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.913605809211731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.9136 | Train 0.2143 | Val 0.1120 | Test 0.1049
loading full batch data spends  0.0020008087158203125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648706436157227  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764918327331543  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11287117004394531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8905048370361328
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8905 | Train 0.2429 | Val 0.1080 | Test 0.1030
loading full batch data spends  0.0020525455474853516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07637643814086914  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638120651245117  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11609601974487305
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.881624698638916
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.8816 | Train 0.2286 | Val 0.0900 | Test 0.0962
loading full batch data spends  0.002007007598876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07539558410644531  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07540035247802734  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11043024063110352
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020647525787353516  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9173744916915894
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.9174 | Train 0.2429 | Val 0.0960 | Test 0.1025
loading full batch data spends  0.001962423324584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07501554489135742  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07502031326293945  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10549473762512207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9000468254089355
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.9000 | Train 0.3000 | Val 0.1180 | Test 0.1112
loading full batch data spends  0.0019435882568359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07621002197265625  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07621479034423828  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10549759864807129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8732199668884277
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8732 | Train 0.2643 | Val 0.1280 | Test 0.1112
loading full batch data spends  0.001961946487426758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645463943481445  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645940780639648  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10367083549499512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9112237691879272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.9112 | Train 0.2214 | Val 0.1380 | Test 0.1117
loading full batch data spends  0.0020177364349365234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0761880874633789  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07619285583496094  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11251688003540039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9083914756774902
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.9084 | Train 0.2143 | Val 0.1140 | Test 0.1006
loading full batch data spends  0.0021483898162841797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656526565551758  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657003402709961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10696196556091309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9092401266098022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.9092 | Train 0.2214 | Val 0.1300 | Test 0.0986
loading full batch data spends  0.0020122528076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07661008834838867  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766148567199707  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10541987419128418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8905997276306152
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.8906 | Train 0.2357 | Val 0.0980 | Test 0.1011
loading full batch data spends  0.001970529556274414
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07621049880981445  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07621526718139648  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11124229431152344
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8998953104019165
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8999 | Train 0.2214 | Val 0.0980 | Test 0.1030
loading full batch data spends  0.002003908157348633
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627582550048828  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628059387207031  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11163949966430664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9211013317108154
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.9211 | Train 0.2286 | Val 0.0860 | Test 0.0977
loading full batch data spends  0.001954317092895508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07614612579345703  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07615089416503906  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11878299713134766
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8982975482940674
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.8983 | Train 0.2571 | Val 0.0920 | Test 0.0962
loading full batch data spends  0.0020062923431396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0753941535949707  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07539892196655273  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10230302810668945
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8717503547668457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.8718 | Train 0.2143 | Val 0.1060 | Test 0.0977
loading full batch data spends  0.0019845962524414062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638263702392578  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638740539550781  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10359334945678711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8927433490753174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8927 | Train 0.2429 | Val 0.1180 | Test 0.1117
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766444206237793  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664918899536133  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11125946044921875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02080249786376953  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.885827660560608
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8858 | Train 0.2429 | Val 0.1120 | Test 0.1069
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655525207519531  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656002044677734  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10578370094299316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8826054334640503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8826 | Train 0.2357 | Val 0.1260 | Test 0.1083
loading full batch data spends  0.002021312713623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657718658447266  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07658195495605469  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10750794410705566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020754337310791016  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8851159811019897
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8851 | Train 0.2714 | Val 0.1340 | Test 0.1141
loading full batch data spends  0.002142190933227539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07632732391357422  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633209228515625  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10928845405578613
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8928567171096802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8929 | Train 0.2857 | Val 0.1340 | Test 0.1219
loading full batch data spends  0.0020537376403808594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763101577758789  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631492614746094  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.11099648475646973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8662141561508179
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8662 | Train 0.2929 | Val 0.1480 | Test 0.1281
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07663154602050781  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07663631439208984  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10808396339416504
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02073049545288086  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8497658967971802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8498 | Train 0.2929 | Val 0.1400 | Test 0.1243
loading full batch data spends  0.0023272037506103516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.8623809814453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07643318176269531  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07643795013427734  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

pure train time  0.10696625709533691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802311897277832  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8564836978912354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8565 | Train 0.2929 | Val 0.1480 | Test 0.1233
loading full batch data spends  0.001973867416381836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07677602767944336  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07678079605102539  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10494351387023926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8806623220443726
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8807 | Train 0.2786 | Val 0.1480 | Test 0.1248
loading full batch data spends  0.0020058155059814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655906677246094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656383514404297  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10519671440124512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.868554949760437
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8686 | Train 0.2786 | Val 0.1420 | Test 0.1252
loading full batch data spends  0.002012968063354492
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666349411010742  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666826248168945  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.101165771484375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8873766660690308
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8874 | Train 0.2714 | Val 0.1440 | Test 0.1310
loading full batch data spends  0.001989603042602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07651519775390625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07651996612548828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11182403564453125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020754337310791016  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8816970586776733
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8817 | Train 0.2571 | Val 0.1380 | Test 0.1107
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631349563598633  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631826400756836  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10914778709411621
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.918336272239685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.9183 | Train 0.2714 | Val 0.1460 | Test 0.1252
loading full batch data spends  0.002000570297241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627534866333008  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628011703491211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1004476547241211
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.872068166732788
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8721 | Train 0.2214 | Val 0.1580 | Test 0.1238
loading full batch data spends  0.001954317092895508
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641220092773438  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764169692993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10937070846557617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8841912746429443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8842 | Train 0.2000 | Val 0.1560 | Test 0.1330
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628679275512695  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07629156112670898  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10523104667663574
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9133037328720093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.9133 | Train 0.2000 | Val 0.1500 | Test 0.1349
loading full batch data spends  0.0019752979278564453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07669782638549805  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07670259475708008  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10482311248779297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02076244354248047  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8889155387878418
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8889 | Train 0.2214 | Val 0.1600 | Test 0.1383
loading full batch data spends  0.002022981643676758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631587982177734  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07632064819335938  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11613965034484863
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8979671001434326
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8980 | Train 0.2357 | Val 0.1600 | Test 0.1441
loading full batch data spends  0.0019507408142089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07620000839233398  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07620477676391602  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10614752769470215
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8848563432693481
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8849 | Train 0.2143 | Val 0.1320 | Test 0.1306
loading full batch data spends  0.002051115036010742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664680480957031  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665157318115234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.0993812084197998
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8820232152938843
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8820 | Train 0.1857 | Val 0.1260 | Test 0.1136
loading full batch data spends  0.0019655227661132812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763096809387207  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631444931030273  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10755228996276855
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8783551454544067
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8784 | Train 0.2429 | Val 0.1220 | Test 0.1093
loading full batch data spends  0.0019893646240234375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642936706542969  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07643413543701172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10467648506164551
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02081298828125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8803390264511108
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8803 | Train 0.2214 | Val 0.1200 | Test 0.1035
loading full batch data spends  0.0019690990447998047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07613849639892578  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07614326477050781  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11063575744628906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8907032012939453
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8907 | Train 0.2357 | Val 0.1140 | Test 0.1015
loading full batch data spends  0.0020198822021484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07659530639648438  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766000747680664  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10640144348144531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8464789390563965
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8465 | Train 0.2286 | Val 0.1180 | Test 0.1064
loading full batch data spends  0.001955747604370117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631397247314453  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631874084472656  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10447025299072266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.861594319343567
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8616 | Train 0.2071 | Val 0.1080 | Test 0.1059
loading full batch data spends  0.002000570297241211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07650327682495117  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765080451965332  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10996007919311523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8825507164001465
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8826 | Train 0.2357 | Val 0.1040 | Test 0.1020
loading full batch data spends  0.001974344253540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07646942138671875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07647418975830078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10294508934020996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8912259340286255
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8912 | Train 0.2500 | Val 0.1060 | Test 0.1040
loading full batch data spends  0.002004384994506836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07622098922729492  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07622575759887695  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10505867004394531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8729043006896973
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8729 | Train 0.2500 | Val 0.1240 | Test 0.1112
loading full batch data spends  0.0019812583923339844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07637405395507812  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07637882232666016  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10408401489257812
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896368384361267
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8964 | Train 0.2500 | Val 0.1320 | Test 0.1103
loading full batch data spends  0.0020322799682617188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653141021728516  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653617858886719  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10546708106994629
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8782204389572144
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8782 | Train 0.2429 | Val 0.1340 | Test 0.1093
loading full batch data spends  0.0019876956939697266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07636260986328125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07636737823486328  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10555386543273926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.863316535949707
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8633 | Train 0.2357 | Val 0.1360 | Test 0.1127
loading full batch data spends  0.002045154571533203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07669591903686523  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07670068740844727  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11110186576843262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.863900065422058
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8639 | Train 0.2286 | Val 0.1360 | Test 0.1170
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07636451721191406  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763692855834961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10750341415405273
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8378651142120361
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8379 | Train 0.2643 | Val 0.1100 | Test 0.1141
loading full batch data spends  0.00202178955078125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07667398452758789  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07667875289916992  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10433459281921387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020746707916259766  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8791741132736206
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8792 | Train 0.2429 | Val 0.1040 | Test 0.1146
loading full batch data spends  0.001974344253540039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07624435424804688  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762491226196289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10023975372314453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8630189895629883
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8630 | Train 0.2500 | Val 0.1060 | Test 0.1165
loading full batch data spends  0.0020096302032470703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.076385498046875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07639026641845703  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10495853424072266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8728818893432617
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8729 | Train 0.2357 | Val 0.1120 | Test 0.1228
loading full batch data spends  0.0019822120666503906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630777359008789  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631254196166992  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11761927604675293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020722389221191406  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.883505940437317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8835 | Train 0.2429 | Val 0.1040 | Test 0.1262
loading full batch data spends  0.0021872520446777344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07651138305664062  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07651615142822266  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11427760124206543
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8589651584625244
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.8590 | Train 0.2643 | Val 0.1020 | Test 0.1175
loading full batch data spends  0.0024552345275878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   8.559226989746094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07623291015625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07623767852783203  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11682891845703125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8525882959365845
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.8526 | Train 0.2714 | Val 0.1060 | Test 0.1175
loading full batch data spends  0.002206087112426758
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645797729492188  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764627456665039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11800575256347656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8337815999984741
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.8338 | Train 0.2929 | Val 0.1040 | Test 0.1156
loading full batch data spends  0.0022783279418945312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.9577484130859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07610416412353516  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07610893249511719  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11099076271057129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8731554746627808
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8732 | Train 0.2786 | Val 0.0980 | Test 0.1199
loading full batch data spends  0.002056121826171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.743171691894531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07624959945678711  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07625436782836914  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10377144813537598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.868095874786377
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8681 | Train 0.3000 | Val 0.1200 | Test 0.1243
loading full batch data spends  0.0021703243255615234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765223503112793  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07652711868286133  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10457658767700195
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8502192497253418
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.8502 | Train 0.2571 | Val 0.1400 | Test 0.1296
loading full batch data spends  0.002019643783569336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656049728393555  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656526565551758  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10599040985107422
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8341561555862427
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.8342 | Train 0.2500 | Val 0.1520 | Test 0.1233
loading full batch data spends  0.002146482467651367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762476921081543  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07625246047973633  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10434341430664062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02074909210205078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8441107273101807
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8441 | Train 0.2357 | Val 0.1380 | Test 0.1190
loading full batch data spends  0.0022058486938476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654619216918945  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655096054077148  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10494494438171387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.831272840499878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8313 | Train 0.2429 | Val 0.1400 | Test 0.1175
loading full batch data spends  0.0019910335540771484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07624435424804688  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762491226196289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1061239242553711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8460813760757446
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.8461 | Train 0.2214 | Val 0.1480 | Test 0.1156
loading full batch data spends  0.002242565155029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07680225372314453  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07680702209472656  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10795783996582031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020757198333740234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8302568197250366
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8303 | Train 0.2286 | Val 0.1400 | Test 0.1151
loading full batch data spends  0.0021300315856933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07639741897583008  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640218734741211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10437273979187012
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8262301683425903
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.8262 | Train 0.2500 | Val 0.1320 | Test 0.1233
loading full batch data spends  0.002025127410888672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630109786987305  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630586624145508  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11881780624389648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.837135910987854
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.8371 | Train 0.2786 | Val 0.1320 | Test 0.1190
loading full batch data spends  0.0021517276763916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07524347305297852  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07524824142456055  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1078805923461914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8191090822219849
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.8191 | Train 0.2714 | Val 0.1100 | Test 0.1146
loading full batch data spends  0.0021915435791015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07620477676391602  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07620954513549805  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10825657844543457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8380885124206543
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8381 | Train 0.2571 | Val 0.1060 | Test 0.1141
loading full batch data spends  0.002141714096069336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07652950286865234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653427124023438  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10532832145690918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8275893926620483
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.8276 | Train 0.2714 | Val 0.1080 | Test 0.1170
loading full batch data spends  0.0019526481628417969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07634639739990234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07635116577148438  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1087338924407959
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.825562834739685
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.8256 | Train 0.3071 | Val 0.1180 | Test 0.1223
loading full batch data spends  0.002147674560546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656288146972656  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765676498413086  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10742807388305664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020770549774169922  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8169578313827515
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.8170 | Train 0.3143 | Val 0.1200 | Test 0.1185
loading full batch data spends  0.002199411392211914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654285430908203  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654762268066406  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10586094856262207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7900291681289673
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.7900 | Train 0.3000 | Val 0.1240 | Test 0.1233
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07669591903686523  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07670068740844727  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11014461517333984
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8399863243103027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.8400 | Train 0.2857 | Val 0.1200 | Test 0.1301
loading full batch data spends  0.0019991397857666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641077041625977  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764155387878418  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10677623748779297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8189588785171509
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.8190 | Train 0.3071 | Val 0.1140 | Test 0.1277
loading full batch data spends  0.0021479129791259766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628726959228516  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07629203796386719  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11004185676574707
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020733356475830078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.811560869216919
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.8116 | Train 0.3143 | Val 0.1060 | Test 0.1262
loading full batch data spends  0.0020754337310791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655763626098633  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656240463256836  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10574030876159668
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8098825216293335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.8099 | Train 0.2857 | Val 0.0980 | Test 0.1223
loading full batch data spends  0.00213623046875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648420333862305  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648897171020508  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10835003852844238
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020765304565429688  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8099116086959839
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.8099 | Train 0.3357 | Val 0.1400 | Test 0.1538
loading full batch data spends  0.0021948814392089844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07614469528198242  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07614946365356445  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10640287399291992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7830376625061035
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.7830 | Train 0.3786 | Val 0.1480 | Test 0.1668
loading full batch data spends  0.002192258834838867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.8623809814453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763397216796875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07634449005126953  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10120177268981934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8119580745697021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.8120 | Train 0.3214 | Val 0.1540 | Test 0.1388
loading full batch data spends  0.0021848678588867188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765523910522461  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655715942382812  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11120367050170898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8329664468765259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.8330 | Train 0.3357 | Val 0.1540 | Test 0.1431
loading full batch data spends  0.0021462440490722656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657146453857422  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657623291015625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1092836856842041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02078104019165039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8057661056518555
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.8058 | Train 0.3571 | Val 0.1580 | Test 0.1596
loading full batch data spends  0.002184629440307617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630395889282227  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763087272644043  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.107452392578125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.795021653175354
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.7950 | Train 0.3286 | Val 0.1280 | Test 0.1518
loading full batch data spends  0.0019791126251220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07637596130371094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638072967529297  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11140060424804688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7801392078399658
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.7801 | Train 0.3214 | Val 0.1380 | Test 0.1494
loading full batch data spends  0.002187967300415039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633018493652344  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633495330810547  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.09994673728942871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8037352561950684
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.8037 | Train 0.3429 | Val 0.1360 | Test 0.1567
loading full batch data spends  0.002145528793334961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633495330810547  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763397216796875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.102447509765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7878847122192383
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.7879 | Train 0.3357 | Val 0.1500 | Test 0.1576
loading full batch data spends  0.0021996498107910156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655906677246094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07656383514404297  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11213326454162598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7726958990097046
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.7727 | Train 0.3143 | Val 0.1500 | Test 0.1518
loading full batch data spends  0.0026082992553710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   6.651878356933594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07616472244262695  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07616949081420898  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10222864151000977
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7888790369033813
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.7889 | Train 0.3071 | Val 0.1360 | Test 0.1397
loading full batch data spends  0.002426624298095703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1484832763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633018493652344  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633495330810547  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10956788063049316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8129682540893555
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.8130 | Train 0.3286 | Val 0.1480 | Test 0.1533
loading full batch data spends  0.0025496482849121094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.0067901611328125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641983032226562  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642459869384766  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10359859466552734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.768474817276001
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.7685 | Train 0.3143 | Val 0.1260 | Test 0.1315
loading full batch data spends  0.002209186553955078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07623004913330078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07623481750488281  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10578751564025879
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.80314040184021
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.8031 | Train 0.2714 | Val 0.0960 | Test 0.1049
loading full batch data spends  0.0021517276763916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07501602172851562  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07502079010009766  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10508084297180176
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020605087280273438  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.852530598640442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.8525 | Train 0.3214 | Val 0.1580 | Test 0.1557
loading full batch data spends  0.0021839141845703125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665729522705078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666206359863281  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1080327033996582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02077341079711914  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.792327642440796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.7923 | Train 0.2571 | Val 0.1500 | Test 0.1451
loading full batch data spends  0.002140045166015625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07615280151367188  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0761575698852539  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10390329360961914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8275357484817505
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.8275 | Train 0.2500 | Val 0.1460 | Test 0.1470
loading full batch data spends  0.0020232200622558594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628870010375977  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762934684753418  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1068260669708252
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8060669898986816
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.8061 | Train 0.2500 | Val 0.1480 | Test 0.1581
loading full batch data spends  0.0021398067474365234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633638381958008  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07634115219116211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10707640647888184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.848349928855896
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.8483 | Train 0.3071 | Val 0.1620 | Test 0.1634
loading full batch data spends  0.002200603485107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640981674194336  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641458511352539  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10439872741699219
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.757968544960022
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.7580 | Train 0.3357 | Val 0.1720 | Test 0.1750
loading full batch data spends  0.002135753631591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648229598999023  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648706436157227  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1048123836517334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7498376369476318
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.7498 | Train 0.3571 | Val 0.1900 | Test 0.1905
loading full batch data spends  0.0022094249725341797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657718658447266  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07658195495605469  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10869145393371582
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020754337310791016  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.779272437095642
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.7793 | Train 0.3500 | Val 0.1800 | Test 0.1576
loading full batch data spends  0.0021407604217529297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.552436828613281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766448974609375  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664966583251953  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10000038146972656
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8308329582214355
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.8308 | Train 0.3929 | Val 0.1860 | Test 0.1842
loading full batch data spends  0.002199888229370117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07612800598144531  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07613277435302734  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10853123664855957
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7690465450286865
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.7690 | Train 0.4000 | Val 0.1840 | Test 0.1818
loading full batch data spends  0.0019719600677490234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07616615295410156  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0761709213256836  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10551142692565918
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7413997650146484
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.7414 | Train 0.3643 | Val 0.1700 | Test 0.1804
loading full batch data spends  0.002197742462158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07617998123168945  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07618474960327148  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10697460174560547
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7161979675292969
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.7162 | Train 0.3071 | Val 0.1700 | Test 0.1721
loading full batch data spends  0.0021517276763916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645273208618164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645750045776367  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11092925071716309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02075958251953125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7993935346603394
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.7994 | Train 0.3000 | Val 0.1740 | Test 0.1639
loading full batch data spends  0.0021898746490478516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07674407958984375  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07674884796142578  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11752867698669434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7988426685333252
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7988 | Train 0.2857 | Val 0.1780 | Test 0.1678
loading full batch data spends  0.002142190933227539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0751795768737793  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07518434524536133  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10856246948242188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7640384435653687
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.7640 | Train 0.2929 | Val 0.1820 | Test 0.1692
loading full batch data spends  0.002182483673095703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664680480957031  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665157318115234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11012625694274902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7858399152755737
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7858 | Train 0.3214 | Val 0.1860 | Test 0.1736
loading full batch data spends  0.002120494842529297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.457069396972656e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666444778442383  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07666921615600586  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1060483455657959
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020746707916259766  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7137306928634644
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.7137 | Train 0.3643 | Val 0.1960 | Test 0.1809
loading full batch data spends  0.0021975040435791016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07639408111572266  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07639884948730469  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1078183650970459
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7429792881011963
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.7430 | Train 0.4357 | Val 0.2060 | Test 0.1978
loading full batch data spends  0.002147674560546875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07670021057128906  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0767049789428711  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10938811302185059
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7134259939193726
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.7134 | Train 0.4786 | Val 0.1900 | Test 0.2079
loading full batch data spends  0.0020406246185302734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07629919052124023  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07630395889282227  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10433840751647949
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7512143850326538
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7512 | Train 0.3786 | Val 0.1860 | Test 0.2021
loading full batch data spends  0.0019266605377197266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07671356201171875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07671833038330078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11063647270202637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7556054592132568
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.7556 | Train 0.4571 | Val 0.2020 | Test 0.2220
loading full batch data spends  0.0021736621856689453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631349563598633  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631826400756836  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10476517677307129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7997808456420898
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.7998 | Train 0.4000 | Val 0.1960 | Test 0.1779
loading full batch data spends  0.002132892608642578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645750045776367  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764622688293457  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1093289852142334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7450798749923706
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7451 | Train 0.3214 | Val 0.1740 | Test 0.1683
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07644844055175781  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07645320892333984  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11022806167602539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8150854110717773
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.8151 | Train 0.2929 | Val 0.1740 | Test 0.1634
loading full batch data spends  0.0021376609802246094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1484832763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07625341415405273  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07625818252563477  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10096335411071777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8055367469787598
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.8055 | Train 0.2857 | Val 0.1720 | Test 0.1610
loading full batch data spends  0.0021963119506835938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07632875442504883  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633352279663086  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10634899139404297
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7967873811721802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.7968 | Train 0.3000 | Val 0.1660 | Test 0.1605
loading full batch data spends  0.0021758079528808594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07663202285766602  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07663679122924805  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10376977920532227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020751953125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7786340713500977
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.7786 | Train 0.3143 | Val 0.1620 | Test 0.1586
loading full batch data spends  0.0021820068359375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638835906982422  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07639312744140625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10524415969848633
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7736352682113647
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7736 | Train 0.3571 | Val 0.1620 | Test 0.1601
loading full batch data spends  0.00212860107421875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0751643180847168  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07516908645629883  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1100001335144043
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020637035369873047  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7904844284057617
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.7905 | Train 0.3429 | Val 0.1480 | Test 0.1639
loading full batch data spends  0.0021932125091552734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07614564895629883  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07615041732788086  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10317730903625488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7537692785263062
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.7538 | Train 0.3786 | Val 0.1640 | Test 0.1688
loading full batch data spends  0.002131223678588867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657623291015625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07658100128173828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10221672058105469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7744648456573486
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.7745 | Train 0.3786 | Val 0.1740 | Test 0.1765
loading full batch data spends  0.0020170211791992188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764002799987793  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640504837036133  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10316658020019531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020738601684570312  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6941967010498047
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.6942 | Train 0.4357 | Val 0.1880 | Test 0.1910
loading full batch data spends  0.0019614696502685547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07659626007080078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07660102844238281  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11560988426208496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7766891717910767
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.7767 | Train 0.3857 | Val 0.1700 | Test 0.1765
loading full batch data spends  0.002262115478515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.8623809814453125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07595586776733398  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07596063613891602  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1068716049194336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7368228435516357
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.7368 | Train 0.3786 | Val 0.1640 | Test 0.1755
loading full batch data spends  0.001983642578125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07659292221069336  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07659769058227539  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1115121841430664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02075958251953125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7625478506088257
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.7625 | Train 0.4286 | Val 0.1900 | Test 0.1934
loading full batch data spends  0.0020160675048828125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07641887664794922  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642364501953125  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11630463600158691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7467291355133057
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.7467 | Train 0.4143 | Val 0.1880 | Test 0.1867
loading full batch data spends  0.0019617080688476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665443420410156  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766592025756836  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11103963851928711
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7285938262939453
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.7286 | Train 0.3643 | Val 0.1920 | Test 0.1842
loading full batch data spends  0.001992940902709961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642507553100586  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642984390258789  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1030266284942627
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7198108434677124
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.7198 | Train 0.3357 | Val 0.1780 | Test 0.1750
loading full batch data spends  0.001962423324584961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765690803527832  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657384872436523  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11042237281799316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7793264389038086
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.7793 | Train 0.3214 | Val 0.1800 | Test 0.1707
loading full batch data spends  0.001995086669921875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07643985748291016  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07644462585449219  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10302090644836426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02078104019165039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7307909727096558
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.7308 | Train 0.3357 | Val 0.1840 | Test 0.1697
loading full batch data spends  0.001979351043701172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627725601196289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628202438354492  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11094856262207031
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7535427808761597
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.7535 | Train 0.3286 | Val 0.1840 | Test 0.1692
loading full batch data spends  0.0020182132720947266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627248764038086  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627725601196289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10207676887512207
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7543245553970337
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.7543 | Train 0.3214 | Val 0.1840 | Test 0.1726
loading full batch data spends  0.0019812583923339844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07616472244262695  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07616949081420898  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1036684513092041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7225985527038574
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.7226 | Train 0.3214 | Val 0.1820 | Test 0.1736
loading full batch data spends  0.0019555091857910156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653188705444336  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07653665542602539  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10465407371520996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7386327981948853
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.7386 | Train 0.3500 | Val 0.1800 | Test 0.1741
loading full batch data spends  0.0023148059844970703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   7.700920104980469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665348052978516  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07665824890136719  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10531330108642578
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7606377601623535
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7606 | Train 0.3714 | Val 0.1900 | Test 0.1721
loading full batch data spends  0.0020568370819091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664346694946289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07664823532104492  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11236381530761719
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7885141372680664
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.7885 | Train 0.3857 | Val 0.1800 | Test 0.1692
loading full batch data spends  0.0019922256469726562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765681266784668  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657289505004883  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1013338565826416
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020770549774169922  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.740057110786438
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.7401 | Train 0.4286 | Val 0.1860 | Test 0.1726
loading full batch data spends  0.002103567123413086
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.647804260253906e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631587982177734  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07632064819335938  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11090445518493652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7469217777252197
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.7469 | Train 0.4357 | Val 0.1860 | Test 0.1813
loading full batch data spends  0.0019605159759521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07633543014526367  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0763401985168457  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10623645782470703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7651723623275757
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.7652 | Train 0.4214 | Val 0.1960 | Test 0.1847
loading full batch data spends  0.0019974708557128906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631492614746094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07631969451904297  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11222314834594727
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02074909210205078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7125612497329712
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.7126 | Train 0.4429 | Val 0.1940 | Test 0.1905
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07646322250366211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07646799087524414  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10419011116027832
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7234891653060913
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.7235 | Train 0.4357 | Val 0.2020 | Test 0.1910
loading full batch data spends  0.0020062923431396484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07627534866333008  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07628011703491211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10502362251281738
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7366920709609985
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.7367 | Train 0.4571 | Val 0.1960 | Test 0.1910
loading full batch data spends  0.001966238021850586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07607603073120117  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0760807991027832  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10501575469970703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7233835458755493
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.7234 | Train 0.4143 | Val 0.1800 | Test 0.1818
loading full batch data spends  0.0020105838775634766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8371810913085938e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07661247253417969  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07661724090576172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10503506660461426
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7130239009857178
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.7130 | Train 0.4214 | Val 0.1800 | Test 0.1871
loading full batch data spends  0.0021331310272216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07660388946533203  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07660865783691406  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.1077871322631836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02078104019165039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7417073249816895
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.7417 | Train 0.3929 | Val 0.2100 | Test 0.2055
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07662534713745117  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0766301155090332  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11122345924377441
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.714636206626892
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.7146 | Train 0.4000 | Val 0.2380 | Test 0.2331
loading full batch data spends  0.001963376998901367
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648420333862305  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648897171020508  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10380363464355469
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020786285400390625  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7266697883605957
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.7267 | Train 0.3857 | Val 0.2560 | Test 0.2505
loading full batch data spends  0.001989603042602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638311386108398  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07638788223266602  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10470056533813477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7559003829956055
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.7559 | Train 0.3786 | Val 0.1780 | Test 0.1867
loading full batch data spends  0.001964092254638672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648038864135742  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648515701293945  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10375308990478516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7743775844573975
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.7744 | Train 0.3143 | Val 0.1560 | Test 0.1659
loading full batch data spends  0.0019986629486083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07605409622192383  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07605886459350586  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10343098640441895
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7199511528015137
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.7200 | Train 0.2857 | Val 0.1560 | Test 0.1639
loading full batch data spends  0.001979827880859375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07648611068725586  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07649087905883789  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11255478858947754
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7743504047393799
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.7744 | Train 0.3071 | Val 0.1760 | Test 0.1731
loading full batch data spends  0.0019958019256591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657718658447266  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07658195495605469  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10614466667175293
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072000503540039  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7950206995010376
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.7950 | Train 0.3071 | Val 0.1820 | Test 0.1789
loading full batch data spends  0.0019526481628417969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07658529281616211  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07659006118774414  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.09877562522888184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7613333463668823
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.7613 | Train 0.3500 | Val 0.1840 | Test 0.1765
loading full batch data spends  0.001987457275390625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07677078247070312  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07677555084228516  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.11003828048706055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02073049545288086  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7917850017547607
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.7918 | Train 0.3643 | Val 0.1840 | Test 0.1813
loading full batch data spends  0.001953601837158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07622289657592773  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07622766494750977  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10544919967651367
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.787179946899414
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.7872 | Train 0.4143 | Val 0.1980 | Test 0.1857
loading full batch data spends  0.0020380020141601562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07635211944580078  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07635688781738281  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10369205474853516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7321579456329346
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.7322 | Train 0.4071 | Val 0.1840 | Test 0.1900
loading full batch data spends  0.0019571781158447266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657861709594727  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765833854675293  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10422730445861816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020807743072509766  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.800133228302002
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.8001 | Train 0.4286 | Val 0.1820 | Test 0.1939
loading full batch data spends  0.0019979476928710938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640266418457031  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07640743255615234  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10272455215454102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.763340950012207
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.7633 | Train 0.4071 | Val 0.1760 | Test 0.1925
loading full batch data spends  0.0019600391387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0762491226196289  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07625389099121094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10743999481201172
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7637237310409546
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.7637 | Train 0.3929 | Val 0.1700 | Test 0.1707
loading full batch data spends  0.002058267593383789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07635498046875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07635974884033203  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10613870620727539
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7388886213302612
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.7389 | Train 0.4000 | Val 0.1660 | Test 0.1649
loading full batch data spends  0.0019636154174804688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07657909393310547  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0765838623046875  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10283517837524414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.021641254425048828  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7481932640075684
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.7482 | Train 0.4000 | Val 0.1680 | Test 0.1663
loading full batch data spends  0.002013683319091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764913558959961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07649612426757812  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10407495498657227
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020792007446289062  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7675931453704834
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.7676 | Train 0.3857 | Val 0.1720 | Test 0.1741
loading full batch data spends  0.001967191696166992
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07654714584350586  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07655191421508789  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10488343238830566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7261182069778442
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.7261 | Train 0.3786 | Val 0.1960 | Test 0.1891
loading full batch data spends  0.002177715301513672
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07649803161621094  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07650279998779297  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10305023193359375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02072763442993164  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7548784017562866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.7549 | Train 0.4000 | Val 0.2100 | Test 0.2065
loading full batch data spends  0.0019583702087402344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0760812759399414  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07608604431152344  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

pure train time  0.10389828681945801
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.0802602767944336  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6907724142074585
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.6908 | Train 0.4286 | Val 0.2220 | Test 0.2287
loading full batch data spends  0.0019922256469726562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.600120544433594e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07698678970336914  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07699155807495117  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

pure train time  0.10954618453979492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020778656005859375  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7327508926391602
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.7328 | Train 0.4286 | Val 0.2460 | Test 0.2389
loading full batch data spends  0.0019457340240478516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07501888275146484  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07502365112304688  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

pure train time  0.10415387153625488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.020717144012451172  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6761813163757324
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.6762 | Train 0.4143 | Val 0.2440 | Test 0.2389
loading full batch data spends  0.0019884109497070312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.07642650604248047  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.0764312744140625  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

pure train time  0.11652541160583496
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.671875 GB
    Memory Allocated: 0.02163839340209961  GigaBytes
Max Memory Allocated: 0.08041667938232422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7241129875183105
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 16, 16])
h.size() torch.Size([2708, 256])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.7241 | Train 0.4143 | Val 0.2300 | Test 0.2336
Total (block generation + training)time/epoch 0.11984805464744568
pure train time/epoch 0.10861197661380378

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368]
