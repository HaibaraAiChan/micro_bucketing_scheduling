main start at this time 1696111711.7800953
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
140
500
2068
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

in feats:  1433
self._in_src_feats,  1433
self._in_dst_feats 1433
loading full batch data spends  0.0031554698944091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.814697265625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.027371883392333984  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.552734375 GB
    Memory Allocated: 0.027376651763916016  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

pure train time  0.5000417232513428
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.560546875 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.027743816375732422  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9514591693878174
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00000 | Loss 1.9515 | Train 0.1429 | Val 0.3000 | Test 0.3017
loading full batch data spends  0.0015397071838378906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02871084213256836  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02871561050415039  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

pure train time  0.11280035972595215
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9480329751968384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00001 | Loss 1.9480 | Train 0.1357 | Val 0.2220 | Test 0.2355
loading full batch data spends  0.0015292167663574219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028789043426513672  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028793811798095703  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

pure train time  0.11661815643310547
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04641866683959961  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9489327669143677
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00002 | Loss 1.9489 | Train 0.1357 | Val 0.1320 | Test 0.1281
loading full batch data spends  0.0014927387237548828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9087066650390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028819561004638672  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028824329376220703  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.114166259765625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9449717998504639
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00003 | Loss 1.9450 | Train 0.1786 | Val 0.1160 | Test 0.0957
loading full batch data spends  0.001493692398071289
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.504753112792969e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886056900024414  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.11428689956665039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9445648193359375
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00004 | Loss 1.9446 | Train 0.1929 | Val 0.1200 | Test 0.0977
loading full batch data spends  0.0014908313751220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809547424316406  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028814315795898438  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.1149892807006836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9444730281829834
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00005 | Loss 1.9445 | Train 0.1857 | Val 0.1140 | Test 0.0977
loading full batch data spends  0.0014290809631347656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029912948608398438  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991771697998047  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

pure train time  0.11750531196594238
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046445369720458984  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9456636905670166
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00006 | Loss 1.9457 | Train 0.1929 | Val 0.1140 | Test 0.0957
loading full batch data spends  0.001528024673461914
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028905868530273438  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891063690185547  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

pure train time  0.11817526817321777
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9424830675125122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00007 | Loss 1.9425 | Train 0.2000 | Val 0.1140 | Test 0.0953
loading full batch data spends  0.0014290809631347656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02996826171875  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02997303009033203  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

pure train time  0.11746430397033691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.046520233154296875  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.936394214630127
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00008 | Loss 1.9364 | Train 0.1857 | Val 0.1140 | Test 0.0933
loading full batch data spends  0.00152587890625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922557830810547  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028927326202392578  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11460709571838379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9391765594482422
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00009 | Loss 1.9392 | Train 0.1714 | Val 0.1020 | Test 0.0899
loading full batch data spends  0.0014433860778808594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028779983520507812  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028784751892089844  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11729764938354492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9359418153762817
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00010 | Loss 1.9359 | Train 0.1714 | Val 0.1020 | Test 0.0885
loading full batch data spends  0.0015568733215332031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888965606689453  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894424438476562  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11600852012634277
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.935853123664856
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00011 | Loss 1.9359 | Train 0.1714 | Val 0.1040 | Test 0.0895
loading full batch data spends  0.0015079975128173828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029868125915527344  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029872894287109375  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11675119400024414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.933876633644104
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00012 | Loss 1.9339 | Train 0.1714 | Val 0.1100 | Test 0.0914
loading full batch data spends  0.0015375614166259766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028825759887695312  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028830528259277344  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.11438465118408203
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9243148565292358
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00013 | Loss 1.9243 | Train 0.1714 | Val 0.1200 | Test 0.0972
loading full batch data spends  0.0014259815216064453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898406982421875  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898883819580078  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.1171257495880127
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9270368814468384
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00014 | Loss 1.9270 | Train 0.1857 | Val 0.1200 | Test 0.1035
loading full batch data spends  0.0014805793762207031
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989339828491211  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02989816665649414  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

pure train time  0.1137542724609375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.046523094177246094  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9258078336715698
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00015 | Loss 1.9258 | Train 0.1929 | Val 0.1140 | Test 0.1098
loading full batch data spends  0.0014259815216064453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028802871704101562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10080385208129883
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9355741739273071
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00016 | Loss 1.9356 | Train 0.1714 | Val 0.1140 | Test 0.1170
loading full batch data spends  0.0014965534210205078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028886795043945312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028891563415527344  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10108613967895508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.919065237045288
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00017 | Loss 1.9191 | Train 0.1714 | Val 0.1180 | Test 0.1190
loading full batch data spends  0.0014295578002929688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878284454345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028787612915039062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10312223434448242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.932547688484192
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00018 | Loss 1.9325 | Train 0.1786 | Val 0.1200 | Test 0.1199
loading full batch data spends  0.0015606880187988281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994394302368164  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029948711395263672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10831427574157715
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9048391580581665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00019 | Loss 1.9048 | Train 0.1857 | Val 0.1180 | Test 0.1267
loading full batch data spends  0.0014846324920654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028892040252685547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1029520034790039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9328999519348145
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00020 | Loss 1.9329 | Train 0.2071 | Val 0.1240 | Test 0.1190
loading full batch data spends  0.0015316009521484375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028765201568603516  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028769969940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10717582702636719
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9063441753387451
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00021 | Loss 1.9063 | Train 0.2286 | Val 0.1280 | Test 0.1165
loading full batch data spends  0.0015075206756591797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.315376281738281e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02897930145263672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898406982421875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11927986145019531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9077383279800415
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00022 | Loss 1.9077 | Train 0.2071 | Val 0.1300 | Test 0.1146
loading full batch data spends  0.0014736652374267578
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028852462768554688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11258792877197266
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8988981246948242
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00023 | Loss 1.8989 | Train 0.2000 | Val 0.1300 | Test 0.1141
loading full batch data spends  0.0014371871948242188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028725624084472656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028730392456054688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10683178901672363
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.90152108669281
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00024 | Loss 1.9015 | Train 0.1929 | Val 0.1320 | Test 0.1132
loading full batch data spends  0.0015282630920410156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891063690185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0289154052734375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1121370792388916
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9118000268936157
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00025 | Loss 1.9118 | Train 0.1929 | Val 0.1320 | Test 0.1103
loading full batch data spends  0.0014874935150146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029935359954833984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029940128326416016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10407280921936035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9062052965164185
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00026 | Loss 1.9062 | Train 0.2071 | Val 0.1260 | Test 0.1127
loading full batch data spends  0.0015454292297363281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028842926025390625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847694396972656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10886693000793457
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8948966264724731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00027 | Loss 1.8949 | Train 0.1929 | Val 0.1260 | Test 0.1107
loading full batch data spends  0.0014874935150146484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892446517944336  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892923355102539  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11518359184265137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.900148630142212
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00028 | Loss 1.9001 | Train 0.1929 | Val 0.1260 | Test 0.1103
loading full batch data spends  0.001565694808959961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892303466796875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.12062263488769531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9009928703308105
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00029 | Loss 1.9010 | Train 0.1929 | Val 0.1300 | Test 0.1161
loading full batch data spends  0.001577138900756836
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.790855407714844e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888774871826172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889251708984375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10867857933044434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.908573865890503
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00030 | Loss 1.9086 | Train 0.1929 | Val 0.1300 | Test 0.1161
loading full batch data spends  0.0014722347259521484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029902935028076172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029907703399658203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10806679725646973
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.898650884628296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00031 | Loss 1.8987 | Train 0.2071 | Val 0.1260 | Test 0.1175
loading full batch data spends  0.0014271736145019531
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028878211975097656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.11874127388000488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8837350606918335
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00032 | Loss 1.8837 | Train 0.2286 | Val 0.1220 | Test 0.1107
loading full batch data spends  0.0016627311706542969
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   5.412101745605469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028908252716064453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913021087646484  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1201167106628418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896426796913147
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00033 | Loss 1.8964 | Train 0.2143 | Val 0.1260 | Test 0.1059
loading full batch data spends  0.001520395278930664
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895498275756836  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895975112915039  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09732961654663086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8869833946228027
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00034 | Loss 1.8870 | Train 0.2143 | Val 0.1300 | Test 0.1069
loading full batch data spends  0.0015549659729003906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891683578491211  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892160415649414  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0991964340209961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8623912334442139
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00035 | Loss 1.8624 | Train 0.2429 | Val 0.1300 | Test 0.1151
loading full batch data spends  0.0014843940734863281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867721557617188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09651756286621094
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9135994911193848
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00036 | Loss 1.9136 | Train 0.2500 | Val 0.1320 | Test 0.1190
loading full batch data spends  0.0014834403991699219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881622314453125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882099151611328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10180878639221191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00917673110961914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8942557573318481
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00037 | Loss 1.8943 | Train 0.2643 | Val 0.1400 | Test 0.1214
loading full batch data spends  0.0015070438385009766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028727054595947266  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028731822967529297  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10029125213623047
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.870881199836731
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00038 | Loss 1.8709 | Train 0.2429 | Val 0.1380 | Test 0.1257
loading full batch data spends  0.0014827251434326172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028783798217773438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02878856658935547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10190534591674805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8889648914337158
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00039 | Loss 1.8890 | Train 0.2643 | Val 0.1300 | Test 0.1248
loading full batch data spends  0.001758575439453125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.100799560546875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0961601734161377
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8907817602157593
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00040 | Loss 1.8908 | Train 0.2714 | Val 0.1260 | Test 0.1228
loading full batch data spends  0.0015292167663574219
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886056900024414  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865337371826172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09553790092468262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8914387226104736
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00041 | Loss 1.8914 | Train 0.2643 | Val 0.1300 | Test 0.1257
loading full batch data spends  0.0014286041259765625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891397476196289  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028918743133544922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10284972190856934
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9038604497909546
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00042 | Loss 1.9039 | Train 0.2357 | Val 0.1300 | Test 0.1238
loading full batch data spends  0.0015494823455810547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994251251220703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029947280883789062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10270118713378906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8966022729873657
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00043 | Loss 1.8966 | Train 0.2357 | Val 0.1360 | Test 0.1136
loading full batch data spends  0.0014481544494628906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028860092163085938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886486053466797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10394477844238281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8778352737426758
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00044 | Loss 1.8778 | Train 0.2357 | Val 0.1320 | Test 0.1136
loading full batch data spends  0.0015175342559814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09636259078979492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9013887643814087
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00045 | Loss 1.9014 | Train 0.2643 | Val 0.1480 | Test 0.1161
loading full batch data spends  0.0014929771423339844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884817123413086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885293960571289  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10459518432617188
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9159632921218872
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00046 | Loss 1.9160 | Train 0.2429 | Val 0.1440 | Test 0.1262
loading full batch data spends  0.0014846324920654297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028809070587158203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028813838958740234  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10257554054260254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009171485900878906  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.903079867362976
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00047 | Loss 1.9031 | Train 0.2357 | Val 0.1440 | Test 0.1267
loading full batch data spends  0.0014507770538330078
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8848648071289062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873443603515625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09659028053283691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8865282535552979
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00048 | Loss 1.8865 | Train 0.2357 | Val 0.1440 | Test 0.1301
loading full batch data spends  0.0014867782592773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029945850372314453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029950618743896484  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09718680381774902
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8627192974090576
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00049 | Loss 1.8627 | Train 0.2571 | Val 0.1460 | Test 0.1281
loading full batch data spends  0.0015211105346679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.886222839355469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028932571411132812  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028937339782714844  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10421967506408691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8875468969345093
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00050 | Loss 1.8875 | Train 0.2500 | Val 0.1360 | Test 0.1199
loading full batch data spends  0.0015106201171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9325485229492188e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029940128326416016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029944896697998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09670853614807129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8977844715118408
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00051 | Loss 1.8978 | Train 0.2429 | Val 0.1340 | Test 0.1161
loading full batch data spends  0.0015108585357666016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851032257080078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10046720504760742
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8711282014846802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00052 | Loss 1.8711 | Train 0.2500 | Val 0.1340 | Test 0.1214
loading full batch data spends  0.0014920234680175781
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889108657836914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028895854949951172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0988459587097168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.887689471244812
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00053 | Loss 1.8877 | Train 0.2643 | Val 0.1300 | Test 0.1267
loading full batch data spends  0.00144195556640625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028954029083251953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028958797454833984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10410928726196289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8891710042953491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00054 | Loss 1.8892 | Train 0.2714 | Val 0.1320 | Test 0.1272
loading full batch data spends  0.001550436019897461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028871536254882812  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876304626464844  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09281206130981445
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8961329460144043
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00055 | Loss 1.8961 | Train 0.2714 | Val 0.1300 | Test 0.1252
loading full batch data spends  0.0014405250549316406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028994083404541016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028998851776123047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08483624458312988
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8792835474014282
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00056 | Loss 1.8793 | Train 0.2714 | Val 0.1260 | Test 0.1209
loading full batch data spends  0.0013623237609863281
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5033950805664062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02894735336303711  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895212173461914  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.083160400390625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.895725965499878
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00057 | Loss 1.8957 | Train 0.2500 | Val 0.1240 | Test 0.1219
loading full batch data spends  0.0013153553009033203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.86102294921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029947280883789062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029952049255371094  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08170509338378906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8696086406707764
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00058 | Loss 1.8696 | Train 0.2643 | Val 0.1160 | Test 0.1252
loading full batch data spends  0.001356363296508789
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5033950805664062e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915809631347656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029920578002929688  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0808572769165039
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8736249208450317
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00059 | Loss 1.8736 | Train 0.2786 | Val 0.1040 | Test 0.1175
loading full batch data spends  0.0013294219970703125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.8133392333984375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028828144073486328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883291244506836  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08679556846618652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.875145435333252
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00060 | Loss 1.8751 | Train 0.2786 | Val 0.1000 | Test 0.1088
loading full batch data spends  0.0013418197631835938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028840065002441406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028844833374023438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.07882428169250488
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8887895345687866
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00061 | Loss 1.8888 | Train 0.3000 | Val 0.1040 | Test 0.1127
loading full batch data spends  0.0013232231140136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028863906860351562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08043265342712402
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.896431565284729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00062 | Loss 1.8964 | Train 0.3000 | Val 0.1260 | Test 0.1281
loading full batch data spends  0.0013742446899414062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.5272369384765625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028862476348876953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867244720458984  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08405709266662598
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.849827766418457
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00063 | Loss 1.8498 | Train 0.2643 | Val 0.1280 | Test 0.1359
loading full batch data spends  0.0013077259063720703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7894973754882812e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029963016510009766  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029967784881591797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.07919788360595703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.875855565071106
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00064 | Loss 1.8759 | Train 0.2714 | Val 0.1360 | Test 0.1310
loading full batch data spends  0.0013670921325683594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.4318695068359375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028832435607910156  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028837203979492188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08103394508361816
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8692387342453003
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00065 | Loss 1.8692 | Train 0.2714 | Val 0.1360 | Test 0.1320
loading full batch data spends  0.0013320446014404297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6464462280273438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028808116912841797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812885284423828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08537697792053223
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8600877523422241
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00066 | Loss 1.8601 | Train 0.2857 | Val 0.1360 | Test 0.1315
loading full batch data spends  0.0013554096221923828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.6702880859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928756713867188  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893352508544922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08274269104003906
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00922250747680664  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8739726543426514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00067 | Loss 1.8740 | Train 0.2929 | Val 0.1360 | Test 0.1315
loading full batch data spends  0.0013232231140136719
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.765655517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827190399169922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028831958770751953  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08078265190124512
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8617926836013794
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00068 | Loss 1.8618 | Train 0.2786 | Val 0.1320 | Test 0.1344
loading full batch data spends  0.0013723373413085938
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.7418136596679688e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029882431030273438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02988719940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.08339953422546387
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8749784231185913
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00069 | Loss 1.8750 | Train 0.2714 | Val 0.1280 | Test 0.1354
loading full batch data spends  0.0019783973693847656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.6716461181640625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028802871704101562  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10685491561889648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8693560361862183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00070 | Loss 1.8694 | Train 0.2857 | Val 0.1340 | Test 0.1339
loading full batch data spends  0.0020339488983154297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028900146484375  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890491485595703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10268926620483398
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0092010498046875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8729987144470215
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00071 | Loss 1.8730 | Train 0.2643 | Val 0.1300 | Test 0.1291
loading full batch data spends  0.001959562301635742
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09820127487182617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8468230962753296
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00072 | Loss 1.8468 | Train 0.2643 | Val 0.1440 | Test 0.1310
loading full batch data spends  0.0019927024841308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893543243408203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028940200805664062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10231494903564453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8492405414581299
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00073 | Loss 1.8492 | Train 0.2714 | Val 0.1400 | Test 0.1301
loading full batch data spends  0.001967906951904297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890634536743164  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028911113739013672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10175347328186035
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8257081508636475
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00074 | Loss 1.8257 | Train 0.2786 | Val 0.1300 | Test 0.1252
loading full batch data spends  0.0020148754119873047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028795242309570312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028800010681152344  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10595273971557617
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009198188781738281  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8692816495895386
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00075 | Loss 1.8693 | Train 0.2786 | Val 0.1300 | Test 0.1306
loading full batch data spends  0.001969575881958008
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.790855407714844e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028907299041748047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028912067413330078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10182523727416992
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.835121989250183
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00076 | Loss 1.8351 | Train 0.2929 | Val 0.1260 | Test 0.1330
loading full batch data spends  0.0019483566284179688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028917312622070312  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09949684143066406
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8423446416854858
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00077 | Loss 1.8423 | Train 0.3000 | Val 0.1260 | Test 0.1325
loading full batch data spends  0.0019669532775878906
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886962890625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887439727783203  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.1062157154083252
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.845557689666748
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00078 | Loss 1.8456 | Train 0.2929 | Val 0.1320 | Test 0.1354
loading full batch data spends  0.002010345458984375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028968334197998047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028973102569580078  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09779620170593262
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009249210357666016  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8229293823242188
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00079 | Loss 1.8229 | Train 0.3000 | Val 0.1220 | Test 0.1349
loading full batch data spends  0.0019540786743164062
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028883934020996094  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09863686561584473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8424129486083984
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00080 | Loss 1.8424 | Train 0.2929 | Val 0.1180 | Test 0.1397
loading full batch data spends  0.0020084381103515625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028982162475585938  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02898693084716797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09636688232421875
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00927591323852539  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.829278826713562
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00081 | Loss 1.8293 | Train 0.3071 | Val 0.1300 | Test 0.1335
loading full batch data spends  0.001965761184692383
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881908416748047  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.0288238525390625  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10658740997314453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8185945749282837
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00082 | Loss 1.8186 | Train 0.3214 | Val 0.1220 | Test 0.1407
loading full batch data spends  0.002266407012939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   0.00010633468627929688
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029882431030273438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02988719940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10297822952270508
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8039209842681885
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00083 | Loss 1.8039 | Train 0.3286 | Val 0.1460 | Test 0.1335
loading full batch data spends  0.002127408981323242
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.2438507080078125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028873920440673828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10570192337036133
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8338418006896973
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00084 | Loss 1.8338 | Train 0.3429 | Val 0.1420 | Test 0.1335
loading full batch data spends  0.0020236968994140625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029910564422607422  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915332794189453  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09996747970581055
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.78671133518219
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00085 | Loss 1.7867 | Train 0.3500 | Val 0.1560 | Test 0.1422
loading full batch data spends  0.0019769668579101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.528594970703125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02881622314453125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882099151611328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10104990005493164
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7971242666244507
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00086 | Loss 1.7971 | Train 0.3286 | Val 0.1660 | Test 0.1557
loading full batch data spends  0.0020165443420410156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028904438018798828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890920639038086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10229253768920898
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009251594543457031  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7957136631011963
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00087 | Loss 1.7957 | Train 0.3357 | Val 0.1660 | Test 0.1547
loading full batch data spends  0.0019762516021728516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028765201568603516  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028769969940185547  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09924197196960449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.803650975227356
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00088 | Loss 1.8037 | Train 0.3214 | Val 0.1500 | Test 0.1494
loading full batch data spends  0.0020041465759277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02984905242919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02985382080078125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10201382637023926
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8177106380462646
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00089 | Loss 1.8177 | Train 0.3500 | Val 0.1740 | Test 0.1485
loading full batch data spends  0.002178192138671875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1961669921875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028930187225341797  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028934955596923828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10369205474853516
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7735748291015625
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00090 | Loss 1.7736 | Train 0.3500 | Val 0.1580 | Test 0.1417
loading full batch data spends  0.00200653076171875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029938697814941406  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029943466186523438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10567378997802734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7737176418304443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00091 | Loss 1.7737 | Train 0.3571 | Val 0.1480 | Test 0.1397
loading full batch data spends  0.001956462860107422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887725830078125  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.09726119041442871
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8096063137054443
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00092 | Loss 1.8096 | Train 0.3500 | Val 0.1560 | Test 0.1436
loading full batch data spends  0.0019927024841308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892303466796875  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.0988609790802002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8054730892181396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00093 | Loss 1.8055 | Train 0.3214 | Val 0.1620 | Test 0.1460
loading full batch data spends  0.001943826675415039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10628581047058105
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.758834719657898
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00094 | Loss 1.7588 | Train 0.2714 | Val 0.1460 | Test 0.1388
loading full batch data spends  0.0020112991333007812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030003070831298828  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.03000783920288086  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

pure train time  0.10384607315063477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743194580078125  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8338433504104614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00095 | Loss 1.8338 | Train 0.3429 | Val 0.1620 | Test 0.1470
loading full batch data spends  0.001972675323486328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882980346679688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02888774871826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10763931274414062
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7677662372589111
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00096 | Loss 1.7678 | Train 0.3786 | Val 0.1700 | Test 0.1426
loading full batch data spends  0.002003192901611328
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886819839477539  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028872966766357422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10269451141357422
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7588863372802734
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00097 | Loss 1.7589 | Train 0.3429 | Val 0.1620 | Test 0.1402
loading full batch data spends  0.001958608627319336
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877664566040039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028781414031982422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10215163230895996
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7280462980270386
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00098 | Loss 1.7280 | Train 0.3500 | Val 0.1560 | Test 0.1378
loading full batch data spends  0.002007007598876953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884387969970703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028848648071289062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10928010940551758
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8390474319458008
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00099 | Loss 1.8390 | Train 0.3643 | Val 0.1620 | Test 0.1494
loading full batch data spends  0.002022266387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.790855407714844e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890609741210938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889537811279297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0999908447265625
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7702662944793701
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00100 | Loss 1.7703 | Train 0.3857 | Val 0.1820 | Test 0.1557
loading full batch data spends  0.002004861831665039
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028815746307373047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028820514678955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10146045684814453
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009192943572998047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7594424486160278
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00101 | Loss 1.7594 | Train 0.3857 | Val 0.1740 | Test 0.1673
loading full batch data spends  0.0019905567169189453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992868423461914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029933452606201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10026407241821289
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7744519710540771
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00102 | Loss 1.7745 | Train 0.3714 | Val 0.1800 | Test 0.1659
loading full batch data spends  0.001989603042602539
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992534637451172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993011474609375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10171985626220703
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.764054775238037
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00103 | Loss 1.7641 | Train 0.3429 | Val 0.1820 | Test 0.1634
loading full batch data spends  0.0019440650939941406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028942108154296875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028946876525878906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10152816772460938
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7511576414108276
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00104 | Loss 1.7512 | Train 0.3786 | Val 0.1820 | Test 0.1683
loading full batch data spends  0.0020079612731933594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028892040252685547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1013345718383789
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7480570077896118
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00105 | Loss 1.7481 | Train 0.3429 | Val 0.1820 | Test 0.1746
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887582778930664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028880596160888672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10386037826538086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7507737874984741
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00106 | Loss 1.7508 | Train 0.3357 | Val 0.1700 | Test 0.1678
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991962432861328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924392700195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10055685043334961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7703828811645508
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00107 | Loss 1.7704 | Train 0.3357 | Val 0.1680 | Test 0.1659
loading full batch data spends  0.0019576549530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029910564422607422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029915332794189453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1096031665802002
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7449413537979126
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00108 | Loss 1.7449 | Train 0.3143 | Val 0.1700 | Test 0.1659
loading full batch data spends  0.0019881725311279297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028835773468017578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02884054183959961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0990760326385498
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7384856939315796
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00109 | Loss 1.7385 | Train 0.3714 | Val 0.1800 | Test 0.1721
loading full batch data spends  0.0019550323486328125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028874874114990234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0970149040222168
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.737897515296936
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00110 | Loss 1.7379 | Train 0.3857 | Val 0.2120 | Test 0.1876
loading full batch data spends  0.0019936561584472656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028902530670166016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028907299041748047  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09994840621948242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7346775531768799
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00111 | Loss 1.7347 | Train 0.3929 | Val 0.2120 | Test 0.1939
loading full batch data spends  0.001979351043701172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029930591583251953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029935359954833984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09732198715209961
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7005236148834229
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00112 | Loss 1.7005 | Train 0.3500 | Val 0.1900 | Test 0.1779
loading full batch data spends  0.002017498016357422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879642486572266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028884410858154297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10009264945983887
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009273052215576172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7807666063308716
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00113 | Loss 1.7808 | Train 0.4000 | Val 0.2060 | Test 0.1978
loading full batch data spends  0.0019807815551757812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028894424438476562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028899192810058594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11122751235961914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7616593837738037
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00114 | Loss 1.7617 | Train 0.4071 | Val 0.2160 | Test 0.2045
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029868125915527344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029872894287109375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1009218692779541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7562979459762573
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00115 | Loss 1.7563 | Train 0.3857 | Val 0.1940 | Test 0.1828
loading full batch data spends  0.0019495487213134766
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.361701965332031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10055112838745117
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.720542311668396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00116 | Loss 1.7205 | Train 0.4000 | Val 0.1960 | Test 0.1712
loading full batch data spends  0.001992940902709961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029924392700195312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029929161071777344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1004493236541748
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6968084573745728
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00117 | Loss 1.6968 | Train 0.3786 | Val 0.1820 | Test 0.1625
loading full batch data spends  0.0019919872283935547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028825759887695312  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028830528259277344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10153532028198242
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7750931978225708
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00118 | Loss 1.7751 | Train 0.3786 | Val 0.1920 | Test 0.1721
loading full batch data spends  0.002017498016357422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10759186744689941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7690761089324951
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00119 | Loss 1.7691 | Train 0.3929 | Val 0.2120 | Test 0.1838
loading full batch data spends  0.001968860626220703
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028896808624267578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890157699584961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10071945190429688
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7016878128051758
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00120 | Loss 1.7017 | Train 0.4071 | Val 0.2120 | Test 0.2026
loading full batch data spends  0.0020143985748291016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028847217559814453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851985931396484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10555291175842285
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7019176483154297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00121 | Loss 1.7019 | Train 0.3643 | Val 0.2020 | Test 0.1886
loading full batch data spends  0.0019698143005371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028698444366455078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02870321273803711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10516238212585449
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.760772705078125
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00122 | Loss 1.7608 | Train 0.2571 | Val 0.1560 | Test 0.1494
loading full batch data spends  0.001992940902709961
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02995920181274414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029963970184326172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10225677490234375
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8208166360855103
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00123 | Loss 1.8208 | Train 0.3571 | Val 0.2000 | Test 0.1915
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029821395874023438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02982616424560547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10100483894348145
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7509000301361084
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00124 | Loss 1.7509 | Train 0.4000 | Val 0.2080 | Test 0.2074
loading full batch data spends  0.0020449161529541016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.814697265625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028829097747802734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028833866119384766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10035204887390137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.695979118347168
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00125 | Loss 1.6960 | Train 0.3786 | Val 0.2180 | Test 0.1809
loading full batch data spends  0.0020999908447265625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   9.226799011230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028850078582763672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10475349426269531
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7144641876220703
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00126 | Loss 1.7145 | Train 0.3571 | Val 0.1960 | Test 0.1789
loading full batch data spends  0.0020596981048583984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890157699584961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890634536743164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10992836952209473
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7546312808990479
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00127 | Loss 1.7546 | Train 0.3500 | Val 0.2000 | Test 0.1765
loading full batch data spends  0.0019927024841308594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1076047420501709
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7845039367675781
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00128 | Loss 1.7845 | Train 0.3571 | Val 0.2000 | Test 0.1750
loading full batch data spends  0.002021312713623047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10080981254577637
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7630610466003418
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00129 | Loss 1.7631 | Train 0.3929 | Val 0.2080 | Test 0.1857
loading full batch data spends  0.0019769668579101562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028931140899658203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028935909271240234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10014581680297852
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7630914449691772
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00130 | Loss 1.7631 | Train 0.4071 | Val 0.2260 | Test 0.1929
loading full batch data spends  0.001998424530029297
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028822898864746094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827667236328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09853529930114746
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009262561798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7130213975906372
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00131 | Loss 1.7130 | Train 0.4214 | Val 0.2260 | Test 0.2162
loading full batch data spends  0.0019567012786865234
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028827667236328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028832435607910156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10429644584655762
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7251245975494385
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00132 | Loss 1.7251 | Train 0.4143 | Val 0.2300 | Test 0.2220
loading full batch data spends  0.002047300338745117
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879476547241211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879953384399414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1061861515045166
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009214401245117188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7271524667739868
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00133 | Loss 1.7272 | Train 0.4000 | Val 0.2100 | Test 0.2089
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991008758544922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02991485595703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10236263275146484
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7263083457946777
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00134 | Loss 1.7263 | Train 0.3571 | Val 0.2000 | Test 0.1871
loading full batch data spends  0.0019986629486083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899789810180664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029002666473388672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11120796203613281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009281158447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7447614669799805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00135 | Loss 1.7448 | Train 0.3786 | Val 0.1980 | Test 0.2060
loading full batch data spends  0.0020627975463867188
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.57763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028742313385009766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028747081756591797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0972597599029541
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7336971759796143
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00136 | Loss 1.7337 | Train 0.3857 | Val 0.2300 | Test 0.2045
loading full batch data spends  0.0020704269409179688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.886222839355469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994060516357422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994537353515625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10028266906738281
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7163194417953491
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00137 | Loss 1.7163 | Train 0.4071 | Val 0.2160 | Test 0.1881
loading full batch data spends  0.0019652843475341797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4809112548828125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028975486755371094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028980255126953125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10323977470397949
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6960757970809937
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00138 | Loss 1.6961 | Train 0.3786 | Val 0.1960 | Test 0.1852
loading full batch data spends  0.002013683319091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028855323791503906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028860092163085938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11403894424438477
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7665326595306396
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00139 | Loss 1.7665 | Train 0.3929 | Val 0.2080 | Test 0.1886
loading full batch data spends  0.0019593238830566406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029955387115478516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029960155487060547  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10977292060852051
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.738595962524414
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00140 | Loss 1.7386 | Train 0.3929 | Val 0.2060 | Test 0.1862
loading full batch data spends  0.002013683319091797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028864383697509766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028869152069091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10750675201416016
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7709498405456543
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00141 | Loss 1.7709 | Train 0.4071 | Val 0.2140 | Test 0.1929
loading full batch data spends  0.0019698143005371094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029962539672851562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029967308044433594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0984194278717041
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7539085149765015
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00142 | Loss 1.7539 | Train 0.4286 | Val 0.2180 | Test 0.2026
loading full batch data spends  0.0020110607147216797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028874874114990234  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10868668556213379
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7112160921096802
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00143 | Loss 1.7112 | Train 0.4000 | Val 0.2160 | Test 0.2132
loading full batch data spends  0.0019462108612060547
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028853416442871094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028858184814453125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.0963449478149414
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7380053997039795
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00144 | Loss 1.7380 | Train 0.4071 | Val 0.2220 | Test 0.2249
loading full batch data spends  0.0020041465759277344
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029900550842285156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029905319213867188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10578727722167969
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7378672361373901
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00145 | Loss 1.7379 | Train 0.4071 | Val 0.2220 | Test 0.2253
loading full batch data spends  0.0019562244415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02882671356201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883148193359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10772085189819336
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7486690282821655
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00146 | Loss 1.7487 | Train 0.4286 | Val 0.2280 | Test 0.2278
loading full batch data spends  0.0020093917846679688
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0040740966796875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029860496520996094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029865264892578125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10081815719604492
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6813805103302002
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00147 | Loss 1.6814 | Train 0.4214 | Val 0.2240 | Test 0.2210
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.24249267578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028975963592529297  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028980731964111328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10955214500427246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6902856826782227
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00148 | Loss 1.6903 | Train 0.4357 | Val 0.2220 | Test 0.2137
loading full batch data spends  0.0020232200622558594
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028859615325927734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10473847389221191
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7015326023101807
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00149 | Loss 1.7015 | Train 0.4571 | Val 0.2280 | Test 0.2162
loading full batch data spends  0.0019562244415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.337860107421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02875518798828125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02875995635986328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10427308082580566
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.696135401725769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00150 | Loss 1.6961 | Train 0.4500 | Val 0.2280 | Test 0.2089
loading full batch data spends  0.001988649368286133
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883148193359375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02883625030517578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10824823379516602
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7089391946792603
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00151 | Loss 1.7089 | Train 0.4500 | Val 0.2280 | Test 0.2074
loading full batch data spends  0.0019750595092773438
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02895212173461914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028956890106201172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1084604263305664
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7219061851501465
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00152 | Loss 1.7219 | Train 0.4571 | Val 0.2120 | Test 0.2079
loading full batch data spends  0.0020058155059814453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02890777587890625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891254425048828  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10788679122924805
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009267807006835938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7414371967315674
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00153 | Loss 1.7414 | Train 0.4714 | Val 0.2280 | Test 0.2195
loading full batch data spends  0.0022830963134765625
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.124641418457031e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891683578491211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02892160415649414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10287237167358398
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7083059549331665
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00154 | Loss 1.7083 | Train 0.5071 | Val 0.2460 | Test 0.2345
loading full batch data spends  0.002027273178100586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.695487976074219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028793811798095703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028798580169677734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09960722923278809
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.690354585647583
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00155 | Loss 1.6904 | Train 0.5286 | Val 0.2660 | Test 0.2505
loading full batch data spends  0.0019538402557373047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.458427429199219e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993917465209961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02994394302368164  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10193896293640137
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7024753093719482
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00156 | Loss 1.7025 | Train 0.5429 | Val 0.2600 | Test 0.2553
loading full batch data spends  0.002017498016357422
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028867721557617188  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02887248992919922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09788274765014648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923013687133789  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7265666723251343
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00157 | Loss 1.7266 | Train 0.5071 | Val 0.2580 | Test 0.2548
loading full batch data spends  0.001984119415283203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028972625732421875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028977394104003906  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09938597679138184
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7139997482299805
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00158 | Loss 1.7140 | Train 0.4786 | Val 0.2480 | Test 0.2539
loading full batch data spends  0.001996755599975586
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.075599670410156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028879165649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028883934020996094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10423851013183594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009235858917236328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6994026899337769
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00159 | Loss 1.6994 | Train 0.4643 | Val 0.2300 | Test 0.2447
loading full batch data spends  0.0019652843475341797
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028954029083251953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028958797454833984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10190129280090332
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.656182050704956
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00160 | Loss 1.6562 | Train 0.4429 | Val 0.2200 | Test 0.2379
loading full batch data spends  0.002015829086303711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029897689819335938  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990245819091797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09952306747436523
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6399480104446411
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00161 | Loss 1.6399 | Train 0.4286 | Val 0.2500 | Test 0.2481
loading full batch data spends  0.0019578933715820312
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.3855438232421875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028824329376220703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028829097747802734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10158514976501465
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6872050762176514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00162 | Loss 1.6872 | Train 0.4071 | Val 0.2740 | Test 0.2669
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028877735137939453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028882503509521484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10096192359924316
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6776256561279297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00163 | Loss 1.6776 | Train 0.4071 | Val 0.2640 | Test 0.2650
loading full batch data spends  0.001954793930053711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879476547241211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02879953384399414  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1063239574432373
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6801539659500122
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00164 | Loss 1.6802 | Train 0.4429 | Val 0.2800 | Test 0.2737
loading full batch data spends  0.0020296573638916016
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028923511505126953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10775542259216309
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6404833793640137
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00165 | Loss 1.6405 | Train 0.5000 | Val 0.2860 | Test 0.2853
loading full batch data spends  0.001970052719116211
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.4332275390625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028897762298583984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028902530670166016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10441374778747559
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7010693550109863
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00166 | Loss 1.7011 | Train 0.4857 | Val 0.2880 | Test 0.2800
loading full batch data spends  0.0020465850830078125
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.7670135498046875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029956817626953125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029961585998535156  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11596202850341797
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6688976287841797
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00167 | Loss 1.6689 | Train 0.4643 | Val 0.2820 | Test 0.2708
loading full batch data spends  0.0020837783813476562
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   4.1484832763671875e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932498931884766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029937267303466797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10266613960266113
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.674656629562378
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00168 | Loss 1.6747 | Train 0.4643 | Val 0.3040 | Test 0.2742
loading full batch data spends  0.0020503997802734375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.5762786865234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028863906860351562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028868675231933594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10153031349182129
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009246349334716797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6646161079406738
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00169 | Loss 1.6646 | Train 0.4500 | Val 0.2840 | Test 0.2747
loading full batch data spends  0.0019600391387939453
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028885364532470703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028890132904052734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10338759422302246
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6263314485549927
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00170 | Loss 1.6263 | Train 0.4357 | Val 0.2800 | Test 0.2640
loading full batch data spends  0.0019965171813964844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.956390380859375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028889179229736328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02889394760131836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10101556777954102
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009278297424316406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6540660858154297
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00171 | Loss 1.6541 | Train 0.4500 | Val 0.2800 | Test 0.2568
loading full batch data spends  0.001979351043701172
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028913497924804688  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02891826629638672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10558724403381348
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6061370372772217
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00172 | Loss 1.6061 | Train 0.4500 | Val 0.2800 | Test 0.2606
loading full batch data spends  0.0020012855529785156
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865814208984375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870582580566406  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09794378280639648
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6491283178329468
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00173 | Loss 1.6491 | Train 0.4857 | Val 0.2840 | Test 0.2597
loading full batch data spends  0.001954793930053711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876424789428711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02876901626586914  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10658574104309082
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6801989078521729
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00174 | Loss 1.6802 | Train 0.4571 | Val 0.2840 | Test 0.2660
loading full batch data spends  0.002007722854614258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.123283386230469e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029936790466308594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029941558837890625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09702444076538086
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6670364141464233
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00175 | Loss 1.6670 | Train 0.4571 | Val 0.2960 | Test 0.2718
loading full batch data spends  0.001971006393432617
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993488311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029939651489257812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10133051872253418
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6456063985824585
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00176 | Loss 1.6456 | Train 0.4857 | Val 0.2820 | Test 0.2858
loading full batch data spends  0.0019910335540771484
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0279159545898438e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029950618743896484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029955387115478516  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10407590866088867
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6595045328140259
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00177 | Loss 1.6595 | Train 0.4643 | Val 0.3080 | Test 0.2945
loading full batch data spends  0.001953601837158203
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029901981353759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029906749725341797  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10893821716308594
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6254693269729614
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00178 | Loss 1.6255 | Train 0.5000 | Val 0.3040 | Test 0.3017
loading full batch data spends  0.0019898414611816406
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028871536254882812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028876304626464844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10075545310974121
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009241104125976562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6455408334732056
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00179 | Loss 1.6455 | Train 0.5000 | Val 0.2860 | Test 0.2959
loading full batch data spends  0.0019876956939697266
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.62396240234375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028928279876708984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028933048248291016  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10150504112243652
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.645879864692688
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00180 | Loss 1.6459 | Train 0.5071 | Val 0.3020 | Test 0.3095
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.218650817871094e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028774738311767578  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02877950668334961  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09923338890075684
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6381101608276367
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00181 | Loss 1.6381 | Train 0.4214 | Val 0.2700 | Test 0.2911
loading full batch data spends  0.001957416534423828
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028858661651611328  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886343002319336  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10819029808044434
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6277326345443726
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00182 | Loss 1.6277 | Train 0.4214 | Val 0.2600 | Test 0.2829
loading full batch data spends  0.0020067691802978516
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028922080993652344  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028926849365234375  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09999990463256836
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.00923299789428711  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5927530527114868
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00183 | Loss 1.5928 | Train 0.3714 | Val 0.2520 | Test 0.2403
loading full batch data spends  0.001977682113647461
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.1948089599609375e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02992725372314453  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029932022094726562  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1044924259185791
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6762608289718628
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00184 | Loss 1.6763 | Train 0.3714 | Val 0.2560 | Test 0.2427
loading full batch data spends  0.0020134449005126953
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899456024169922  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02899932861328125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10374665260314941
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009259700775146484  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6616902351379395
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00185 | Loss 1.6617 | Train 0.4286 | Val 0.2900 | Test 0.2945
loading full batch data spends  0.001964569091796875
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028807640075683594  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028812408447265625  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09720063209533691
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6499933004379272
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00186 | Loss 1.6500 | Train 0.4214 | Val 0.2780 | Test 0.2703
loading full batch data spends  0.0020089149475097656
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0994415283203125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028854846954345703  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028859615325927734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1008751392364502
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009224891662597656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6770025491714478
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00187 | Loss 1.6770 | Train 0.4143 | Val 0.2820 | Test 0.2689
loading full batch data spends  0.0019490718841552734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.266334533691406e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.11194610595703125
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6733310222625732
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00188 | Loss 1.6733 | Train 0.4786 | Val 0.2940 | Test 0.3022
loading full batch data spends  0.002046346664428711
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.314018249511719e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028865337371826172  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028870105743408203  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09642672538757324
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6725414991378784
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00189 | Loss 1.6725 | Train 0.4571 | Val 0.2980 | Test 0.3017
loading full batch data spends  0.0019741058349609375
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886199951171875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02886676788330078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10064315795898438
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.613718032836914
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00190 | Loss 1.6137 | Train 0.4643 | Val 0.2500 | Test 0.2742
loading full batch data spends  0.002009153366088867
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.147125244140625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028851032257080078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02885580062866211  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.1013944149017334
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009219646453857422  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.663222074508667
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00191 | Loss 1.6632 | Train 0.4286 | Val 0.2560 | Test 0.2679
loading full batch data spends  0.0019545555114746094
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02893686294555664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028941631317138672  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09792184829711914
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009350776672363281  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6590068340301514
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00192 | Loss 1.6590 | Train 0.4214 | Val 0.2560 | Test 0.2679
loading full batch data spends  0.0019986629486083984
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.743171691894531e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02990436553955078  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029909133911132812  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10477280616760254
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.614711880683899
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00193 | Loss 1.6147 | Train 0.4500 | Val 0.2860 | Test 0.2993
loading full batch data spends  0.0019655227661132812
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.409385681152344e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993154525756836  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02993631362915039  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10352087020874023
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6390023231506348
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00194 | Loss 1.6390 | Train 0.4571 | Val 0.3020 | Test 0.2945
loading full batch data spends  0.0020101070404052734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   2.9802322387695312e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028920650482177734  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028925418853759766  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10107803344726562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009256839752197266  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6373786926269531
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00195 | Loss 1.6374 | Train 0.4214 | Val 0.2620 | Test 0.2664
loading full batch data spends  0.001990795135498047
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.838539123535156e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028831958770751953  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.028836727142333984  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09997010231018066
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.617598056793213
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00196 | Loss 1.6176 | Train 0.4000 | Val 0.2500 | Test 0.2529
loading full batch data spends  0.002007722854614258
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.170967102050781e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030035972595214844  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.030040740966796875  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10168838500976562
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010262489318847656  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6277471780776978
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00197 | Loss 1.6277 | Train 0.4071 | Val 0.2700 | Test 0.2548
loading full batch data spends  0.0019490718841552734
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.2901763916015625e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02868795394897461  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.02869272232055664  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.09954094886779785
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.009347915649414062  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.604048252105713
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00198 | Loss 1.6040 | Train 0.4429 | Val 0.2900 | Test 0.2882
loading full batch data spends  0.0020117759704589844
connection checking time:  0
block generation total time  0
generate_dataloader_block spend   3.0517578125e-05
step  0
----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029891014099121094  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.029895782470703125  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

pure train time  0.10280513763427734
-----------------------------------------after optimizer zero grad
 Nvidia-smi: 1.59375 GB
    Memory Allocated: 0.010259628295898438  GigaBytes
Max Memory Allocated: 0.04743480682373047  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5915499925613403
layer  0
shape of h = x[input_nodes].to(device)  torch.Size([2708, 1433])
shape of h =layer(block, h)  torch.Size([2708, 4, 16])
h.size() torch.Size([2708, 64])
layer  1
shape of h.view(h.shape[0]*h.shape[1], h.shape[2])  torch.Size([2708, 7])
shape of y[output_nodes]  torch.Size([2708, 7])

Run 00 | Epoch 00199 | Loss 1.5915 | Train 0.4429 | Val 0.3280 | Test 0.2916
Total (block generation + training)time/epoch 0.1134228241443634
pure train time/epoch 0.1025221578928889

num_input_list  [1359, 1355, 1364, 1364, 1362, 1361, 1378, 1359, 1369, 1365, 1351, 1365, 1368, 1361, 1365, 1369, 1348, 1361, 1343, 1370, 1360, 1355, 1361, 1357, 1346, 1356, 1375, 1355, 1364, 1362, 1359, 1368, 1363, 1365, 1361, 1363, 1356, 1346, 1347, 1349, 1356, 1364, 1357, 1368, 1363, 1358, 1365, 1345, 1357, 1375, 1361, 1366, 1356, 1365, 1361, 1356, 1361, 1363, 1370, 1366, 1352, 1357, 1353, 1359, 1367, 1353, 1354, 1354, 1353, 1377, 1355, 1350, 1361, 1364, 1361, 1350, 1365, 1357, 1357, 1359, 1359, 1364, 1354, 1373, 1360, 1374, 1353, 1360, 1349, 1367, 1361, 1367, 1365, 1357, 1359, 1366, 1358, 1359, 1347, 1359, 1355, 1349, 1369, 1368, 1353, 1361, 1362, 1375, 1368, 1361, 1357, 1354, 1371, 1364, 1363, 1369, 1358, 1369, 1361, 1358, 1361, 1359, 1338, 1369, 1370, 1356, 1354, 1363, 1367, 1366, 1356, 1362, 1358, 1353, 1367, 1365, 1344, 1377, 1364, 1354, 1372, 1361, 1374, 1361, 1350, 1367, 1354, 1374, 1365, 1354, 1344, 1361, 1362, 1363, 1355, 1361, 1367, 1356, 1364, 1357, 1364, 1371, 1353, 1365, 1352, 1361, 1350, 1369, 1369, 1359, 1363, 1365, 1360, 1361, 1351, 1379, 1371, 1369, 1372, 1358, 1362, 1354, 1350, 1356, 1374, 1361, 1352, 1355, 1376, 1354, 1359, 1354, 1358, 1373, 1371, 1361, 1364, 1370, 1340, 1368]
